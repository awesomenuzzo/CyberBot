i An update to this article is included at the end

Medical Image Analysis 93 (2024) 103100

Contents lists available at ScienceDirect

Medical Image Analysis
journal homepage: www.elsevier.com/locate/media

Survey paper

GAN-based generation of realistic 3D volumetric data: A systematic review
and taxonomy
AndrÃ© Ferreira a,b,c,h,i ,âˆ—, Jianning Li b,c,d , Kelsey L. Pomykala c , Jens Kleesiek c,d,e,g , Victor Alves a ,
Jan Egger b,c,d,f
a

Center Algoritmi/LASI, University of Minho, Braga, 4710-057, Portugal

b Computer Algorithms for Medicine Laboratory, Graz, Austria
c Institute for AI in Medicine (IKIM), University Medicine Essen, GirardetstraÃŸe 2, Essen, 45131, Germany
d Cancer Research Center Cologne Essen (CCCE), University Medicine Essen, HufelandstraÃŸe 55, Essen, 45147, Germany
e

German Cancer Consortium (DKTK), Partner Site Essen, HufelandstraÃŸe 55, Essen, 45147, Germany
Institute of Computer Graphics and Vision, Graz University of Technology, Inffeldgasse 16, Graz, 801, Austria
TU Dortmund University, Department of Physics, Otto-Hahn-StraÃŸe 4, 44227 Dortmund, Germany
h
Department of Oral and Maxillofacial Surgery, University Hospital RWTH Aachen, 52074 Aachen, Germany
i
Institute of Medical Informatics, University Hospital RWTH Aachen, 52074 Aachen, Germany
f

g

ARTICLE

INFO

MSC:
41A05
41A10
65D05
65D17
Keywords:
Synthetic volumetric data
Generative adversarial network
Systematic review
Volumetric GANs taxonomy

ABSTRACT
With the massive proliferation of data-driven algorithms, such as deep learning-based approaches, the
availability of high-quality data is of great interest. Volumetric data is very important in medicine, as it ranges
from disease diagnoses to therapy monitoring. When the dataset is sufficient, models can be trained to help
doctors with these tasks. Unfortunately, there are scenarios where large amounts of data is unavailable. For
example, rare diseases and privacy issues can lead to restricted data availability. In non-medical fields, the
high cost of obtaining enough high-quality data can also be a concern. A solution to these problems can be the
generation of realistic synthetic data using Generative Adversarial Networks (GANs). The existence of these
mechanisms is a good asset, especially in healthcare, as the data must be of good quality, realistic, and without
privacy issues. Therefore, most of the publications on volumetric GANs are within the medical domain. In this
review, we provide a summary of works that generate realistic volumetric synthetic data using GANs. We
therefore outline GAN-based methods in these areas with common architectures, loss functions and evaluation
metrics, including their advantages and disadvantages. We present a novel taxonomy, evaluations, challenges,
and research opportunities to provide a holistic overview of the current state of volumetric GANs.

1. Introduction
In this systematic review, we survey works that generate realistic
synthetic 3D volumetric data with Generative Adversarial Networks
(GANs) (Goodfellow et al., 2014). With the massive increase of datadriven algorithms, such as deep learning-based approaches, during
the last years (Egger et al., 2021, 2022), data is of great interest. In
this context, high-quality training, validation and testing datasets are
required. Unfortunately, there are scenarios and applications where
large amounts of these data are unavailable. Examples can come from
the medical domain, with rare diseases, leading to an insufficient
amount of initial training data. Moreover, additionally in the medical
field, when dealing with real patient data, privacy issues can also limit
the amount of available data. This problem does not only affect the

medical field, as the cost of obtaining high-quality labelled data is
very high in many other fields, such as object recognition and the
study of porous media (Mosser et al., 2017; Muzahid et al., 2021).
A solution to this problem can be the generation of synthetic data to
perform data augmentation, along with additional novel data augmentation mechanisms (Shorten and Khoshgoftaar, 2019). Therefore, we
outline GAN-based methods in this area with common architectures,
loss functions and evaluation metrics, pros, cons, challenges, research
opportunities for a holistic overview of the state-of-the-art, and we also
present a novel taxonomy. Throughout this review, the term â€˜â€˜realisticâ€™â€™
will be used very frequently. This term means that it looks like real
data, i.e. that it is capable of fooling experts in the field, and that it
looks realistic enough to be used as real data. This type of data is in high

âˆ— Corresponding author at: Center Algoritmi/LASI, University of Minho, Braga, 4710-057, Portugal.

E-mail addresses: id10656@alunos.uminho.pt (A. Ferreira), Jianning.Li@uk-essen.de (J. Li), Kelsey.Herrmann@uk-essen.de (K.L. Pomykala),
Jens.Kleesiek@uk-essen.de (J. Kleesiek), valves@di.uminho.pt (V. Alves), Jan.Egger@uk-essen.de (J. Egger).
https://doi.org/10.1016/j.media.2024.103100
Received 5 December 2022; Received in revised form 20 November 2023; Accepted 30 January 2024
Available online 2 February 2024
1361-8415/Â© 2024 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

demand, as data that mimics reality is needed, especially in the medical
field. For brevity, â€˜â€˜3Dâ€™â€™ is also used as a substitute for â€˜â€˜volumetricâ€™â€™
when not otherwise stated.
The amount of volumetric data has been increasing, as this type of
data is important to represent volumetric object in an accurate way not
achievable by 2D image representation. In medical imaging analysis,
3D data has shown to be essential for patient motoring, disease detection and treatment, drug research, and many more. Radiologists usually
analyse these data slice by slice, as it is believed that experienced
radiologists are able to mentally visualise the volume. However, this
might bring inconsistencies among radiologist interpretations. On the
other hand, the visualisation of the whole volume at once provides
an overview which makes it easier to understand unfamiliar shapes.
The 2D slice inspection might be more beneficial for disease detection, however, physicians and other professionals that do not have
the same expertise as radiologists highly benefit from the volumetric
visualisation of such data for treatment planning and better spatial
understanding (Preim and Bartz, 2007).
With enough volumetric data, it might be possible to, for example,
train models for augmented realities to real visualisation of patientâ€™s
lesions in the correct space, or to see the placement of simulated objects
in the real world in the most organised way possible for organisational
efficiency, or for construction purposes.
Fig. 1. PRISMA diagram. *Removed using an automation tool (python script). **
Excluded because they did not involve the generation of volumetric data.

1.1. Manuscript outline
We present a systematic review on the use of GANs for the generation of volumetric data, which includes general methods such as denoising, reconstruction, segmentation, classification, and image translation,
as well as specific applications such as nuclei counting. This section
presents the manuscript outline, the search strategy, the research questions, an insight of the background on volumetric data generation, and
the types of 3D data representation. The rest of this review is organised
as follows.
Section 2 provides a brief explanation of GAN architecture, main
purposes, advantages, and disadvantages. This section is intended to
provide an easy-to-understand insight into GANs, how they work, and
also to describe possible applications.
Section 3 provides an overview of the works done on generating
realistic 3D data with GANs and a description of the main information
and statistics related to modalities, application and metrics used. In
Sections 3.1 and 3.2, summary and insights are provided about each
group of loss functions and evaluation metrics used in the papers,
with more detailed explanations in Appendices A and B, with relevant
references if more in-depth knowledge is required.
Section 3.4 presents the works that are considered relevant, i.e. when
the output of the networks are synthetic volumetric data generated by
GANs or when the generation of volumetric data is crucial for the downstream task. All these papers are summarised in Tables 1â€“3 with respect
to Modality, Medical, Dataset, Network, Loss function, Evaluation Metric,
and Comments. Wherever possible, these tables contain references to
the datasets used as well as references to lesser known concepts and
architectures. All acronyms and abbreviations, if used more than once,
appear in Section 1.1.3. If used only in a particular table, the acronyms
appear before that table.
Section 3.5 contains a closer look at relevant work from various
fields that the reader should explore in more detail. Section 4 provides
a general discussion of the current state of use of GANs, the main problems and possible solutions, tendencies in volumetric data generation
with GANs, conclusions that emerge from the review, and research
opportunities that researchers could and/or should take. In Appendix C
we discuss the applications in the referenced papers, divided into
medical and non-medical applications (Tables C.1 and C.2). The material in the Appendix is also important for a deeper understanding
of the review, especially for less experienced readers. It is therefore
recommended to access it while reading the main manuscript.

To clarify, it should be noted that this is an overview focusing
mainly on the use of GANs to generate volumetric data. The reason
for this choice is the will to improve 3D data generation with GANs, an
underdeveloped topic with great potential but which still needs further
development. The target audience of this review is researchers who
want to enter the field of volumetric synthetic data generation, with
or without much experience with GANs.
1.1.1. Search strategy
We performed a search in the IEEE Xplore Digital Library, Scopus,
PubMed, and Web of Science with the search query â€˜((â€˜â€˜Generative
Adversarial Networkâ€™â€™ OR â€˜â€˜Generative Adversarial Networksâ€™â€™ OR gan OR
gans) AND (generation OR generative) AND (3d OR three-dimensional OR
volumetric) AND data AND synthetic)â€™ to find specific papers on the use
of GANs for volumetric data generation. Since GANs were presented
in 2014 by Goodfellow et al. (2014), all papers prior to 2014 were
excluded.
During the search we found 317 non-unique records, of which 161
were duplicated and 1 was published before 2014 shown by PubMed in
relation to three-dimensional multicellular tumour spheroids, leaving
155 remaining papers. Based on the titles and abstracts, we excluded
82 records that did not mention volumetric generation with GANs. We
assessed the resulting 73 different papers and excluded 1 of them, which
was a review paper on deep learning in pore imaging and modelling.
After further reading, it turned out that 14 articles did not actually
use volumetric data. This resulted in a total of 58 core papers about
generation of volumetric data using GANs, which will be covered in
depth in our review. To the best of our knowledge, this is the first
review that provides a detailed analysis of the published papers on
the use of GANs for the generation of volumetric data. The PRISMA
diagram in Fig. 1 provides a summary overview of our screening.
Note that we include all published research, not just medical applications, which is beneficial for readers from all fields who want an
overview of the potential applications of volumetric data generation
using GANs. We distinguish between medical and non-medical applications, which makes it easier for the reader to focus on the desired
field.
2

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

expensive to obtain further data or when the real data is protected by
data protection regulations.
Machine and deep learning solutions have gained prominence over
the last decade. These technologies are state-of-the-art in a variety of
image processing tasks, ranging from medical to non-medical applications. However, large datasets are needed to achieve good performance.
In the case of volumetric data, the costs associated with their acquisition and processing lead to an intense search for ways to generate them
synthetically.
Computer-aided design (CAD) has been used to create synthetic
volumes as it allows the simulation of a wide range of real volumetric
objects, e.g. to simulate material properties, to train segmentation and
classification models, to be used in virtual or augmented realities and
much more. This type of synthetic data can be created completely in
a virtual mode, e.g. the creation of objects, virtual scenes and virtual
worlds, which can be done manually when no other strategy is possible,
or based on real-world data, i.e. point clouds or voxel grids (Man and
Chahl, 2022). Kohtala and Steinert (2021) trains an object recognition
model using synthetic objects created with CAD. Wong et al. (2019)
create a computer vision system to recognise supermarket products in
a warehouse environment using synthetic data from CAD. Marcu et al.
(2018) use a synthetic 3D aerial dataset created from 3D meshes to
train a model to estimate depth and safe landing areas for unmanned
aerial vehicles. These techniques can be used to capture multiple 2D
images from different angles of objects or the entire 3D object.
Although there are not many works that use statistical shape models
(SSM) or principal component analysis (PCA) to generate synthetic
volumetric data, they may also be an option. Li et al. (2022) use SSM for
synthetic correction of skull defects, and Heimann and Meinzer (2009)
give an overview of papers using SSM in medical image segmentation. Blanz and Vetter (1999) develop 3D morphable models based on
SSM to generate facial shapes. Yu et al. (2021) apply PCA to model the
shape of healthy human skulls and to synthetically correct defective
skulls.
Autoencoders and variational autoencoders are one of the first deep
learning architectures that have had a real impact on the generation
of synthetic volumetric data in both medical and non-medical applications. Autoencoders and variational autoencoders have been used
for various tasks, such as denoising (Kascenas et al., 2022), feature
extraction (Huang et al., 2022) and image compression (Tudosiu et al.,
2020), and also in synthetic volume generation, e.g., Zhang et al.
(2019b) use a variational autoencoder approach for 3D shape synthesis,
more specifically for aircraft models, and Saha et al. (2020) generate
3D car shapes from point clouds.
GANs are an improvement over existing deep learning architectures
for generating synthetic data, as this technique can achieve greater realism and diversity compared to the other approaches. Recently, diffusion
models have been able to outperform GANs in image synthesis (Dhariwal and Nichol, 2021). They are GANsâ€™ biggest competitor in image
generation, and their research has grown exponentially (Croitoru et al.,
2022). However, GANs are distinct enough from diffusion models, and
none of them can completely replace the other. Therefore, a systematic
review of the existing methodologies for generating synthetic volumes
using GANs is presented. It is important to note that some of the papers
presented in this review use some of the above methods to obtain
the dataset for training the GAN, e.g., Greminger (2020), Kniaz et al.
(2020), Yang et al. (2017), as the use of one method does not preclude
the other.
Although synthetic volumetric data have many applications, their
generation presents some challenges. Synthetic data aim to overcome
several problems previously mentioned, such as unbalanced datasets,
the time and cost of obtaining real data, or even the impossibility of
obtaining such data. However, some synthetic data must be created
manually because it cannot be done otherwise, which requires time,
effort and skilled labour, and is therefore expensive and inefficient.
Rendering volumetric data is also very computationally intensive, and

1.1.2. Research questions
The overall aim of this systematic review is to analyse works published between 2014 and January 2022 on the generation of volumetric
data with GANs. In this regard, we defined the following main research
questions for our study: (1) What are the different applications of
GANs in the generation of volumetric data? (2) What are the methods
most frequently or successfully employed by GANs in the generation
of volumetric data? (3) What are the strengths and limitations of these
methods? (4) What improvements are sought through the use of this
technology?
1.1.3. Acronyms and abbreviations
The following list shows the abbreviations that are used more than
once throughout the review. Other abbreviations are defined directly
before each table, when used only once.
â€¢ Acc â€” Accuracy;
â€¢ ADNI â€” Alzheimerâ€™s Disease Neuroimaging Initiative;
â€¢ Adv â€” Adversarial loss;
â€¢ AUC â€” Area Under the Curve;
â€¢ CAD â€” Computer-Aided Design;
â€¢ CBCT â€” Cone-Beam Computed Tomography;
â€¢ CE â€” Cross-Entropy;
â€¢ cGAN â€” conditional GAN;
â€¢ CT â€” Computed Tomography;
â€¢ DCGAN â€” Deep Convolutional Generative Adversarial Networks;
â€¢ DSC â€” Dice Similarity Coefficient;
â€¢ ED-GAN â€” Encoder-Decoder GAN;
â€¢ FID â€” FrÃ©chet Inception Distance;
â€¢ HD â€” Hausdorff Distance;
â€¢ HU â€” Hounsfield Unit;
â€¢ IoU â€” Intersection-over-Union;
â€¢ KL â€” Kullbackâ€“Leibler;
â€¢ LiDAR â€” Light Detection And Ranging;
â€¢ LIDC â€” Lung Image Database Consortium;
â€¢ LSGAN â€” Least Squares GAN;
â€¢ MAE â€” Mean Absolute Error;
â€¢ Minkowski functional â€” Porosity, specific surface area, average
width, Euler number, Permeability;
â€¢ MRI â€” Magnetic Resonance Imaging;
â€¢ MSE â€” Mean Squared Error;
â€¢ NCC â€” Normalised Correlation Coefficient;
â€¢ NMSE â€” Normalised Mean Squared Error;
â€¢ PC â€” Point Cloud;
â€¢ PET â€” Positron Emission Tomography;
â€¢ PGGAN â€” Progressive Growing GANs;
â€¢ Pre â€” Precision;
â€¢ PSNR â€” Peak Signal-to-Noise Ratio;
â€¢ RGB-D â€” Red, Green, Blue image with Depth;
â€¢ SEM â€” Scanning Electron Microscope;
â€¢ Sen â€” Sensitivity;
â€¢ Spe â€” Specificity;
â€¢ SSIM â€” Structural Similarity Index Measure;
â€¢ VTT â€” Visual Turing Test;
â€¢ WGAN â€” Wasserstein GAN;
â€¢ WGAN-GP â€” Wasserstein GAN with Gradient Penalty;
1.2. Background on volumetric data generation
Synthetic data are artificially generated by applying a sampling
procedure to real data or by simulation. They must be realistic enough,
but still different from the real data, i.e. they do not come directly from
the real world. Synthetic data are created when the amount of available
real data is insufficient for the task at hand, or they are unbalanced or
incomplete. In particular, they are used when it is impossible or too
3

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

processing this type of data requires significant memory and computing
power. In addition, the generation of volumetric data is still very underresearched compared to the generation of 2D images, resulting in a
lack of conventional metrics for assessing the quality of synthetic data
and a complete pipeline for generating this type of data. The lack
of large datasets such as ImageNet (Russakovsky et al., 2015) also
delays research and development of approaches to generate volumetric
synthetic data. Therefore, this review aims to provide an overview of
works on the generation of synthetic volumetric data using a promising
technology, GANs. A further discussion of the main problems and
solutions can be found in 4.

in 2D. 3D datasets are more complex, which leads to higher algorithm complexity, more instability and higher computational capacity
requirements. Therefore, traditional tasks such as object recognition
and segmentation are more challenging when using 3D data. Some
works try to circumvent the volumetric aspect of the data by using
multiple 2D views of the object as input (Su et al., 2015). However,
important information is lost, such as depth information, and multiple
frames of the same object are not able to truly represent the object,
because the network processes them as individual pieces of information
rather than collective information. Also, using the depth information as
an additional channel (in RGB-D images) does not represent the entire
object under investigation, limiting the amount of information that can
be fed to the algorithm.
The use of voxel grids can bridge this gap between 2D and 3D vision
and enables the adaptation of some 2D image processing concepts to
3D. Maturana and Scherer (2015) was one of the first works to use
deep learning on voxel grids constructed from point clouds. With such
a representation, the use of 3D convolutions is possible, and it can be
processed more easily and efficiently than point clouds. Voxel grids are
richer in information compared to point clouds in some situations. The
representation of voxel grids is valuable for the detection of high level
features such as shapes and whole objects, which is more difficult to
achieve with point clouds.
Learning directly from point clouds requires the use of specialised
networks, such as PointNet (Qi et al., 2017). Since point clouds are
simply an individual set of points represented in a 3D space, the
order of input should be irrelevant to the network as it does not
affect the geometry of the object. Therefore, multi-layer perceptron
is used instead of convolutions. However, such a representation does
not necessarily ensure that the network learns dependencies between
points in the neighbourhood, which is easily captured by convolutions. Due to the complexity of using point clouds directly as input
to deep learning algorithms, this is still an under-researched topic,
making it easier for researchers to convert point clouds into voxel
grids and use better developed computer vision mechanisms (GutiÃ©rrezBecker et al., 2021). New research areas are being developed to address the complexity and problems of existing point cloud processing
algorithms (Mihir Garimella, 2018).
In GANs, point clouds are usually converted into voxel grids before
being fed into the GAN. The convolution architecture requires regular
inputs to function properly. Since meshes and point clouds are not
equivalent to regular voxel grids, most researchers choose to convert
the data into 3D grid-like structures or a collection of multiple views
(2D images), e.g. Li et al. (2019). As mentioned earlier, using a 2D view
of 3D data is suboptimal and memory inefficient. Conversion to 3D
voxel grids is also memory inefficient and imposes spatial constraints
on each point in the point cloud. However, such a constraint can be
beneficial for some tasks, e.g. brain tumour segmentation, where the
connectivity between voxels in the neighbourhood is essential for accurate segmentation as it is very likely that a tumour cell is adjacent to
another tumour cell. Volumetric information and higher level features,
such as whole objects detection, are also beneficial for some tasks, such
as classification. In this review, several works were found that convert
meshes and point clouds into voxels, e.g. Yang et al. (2017), Kniaz et al.
(2020), Nozawa et al. (2021).
However, some proposals have been made to use point clouds as
input to the network. Point cloud GAN (PC-GAN (Li et al., 2018))
is a GAN architecture capable of processing point clouds without the
need for voxelisation. The point cloud of an object (ğœƒ) is a set of ğ‘›
dimensional vectors ğ‘‹ = {ğ‘¥1 , ğ‘¥2 , â€¦ , ğ‘¥ğ‘› } with ğ‘¥ğ‘– âˆˆ Rğ‘‘ where ğ‘‘ = 3 and
ğ‘› âˆˆ Z+ . The corresponding point clouds of ğ‘€ objects is then ğ‘‹ (ğ‘€) . The
generative model is defined as ğ‘(ğ‘‹), which must be able to generate
new sets ğ‘‹ and generate new points for a giving set, i.e., ğ‘¥ âˆ¼ ğ‘(ğ‘¥|ğ‘‹).
Therefore, joint likelihood can be expressed by Eq. (1).

1.3. Types of 3D data representation
3D geometry data are usually divided into three main groups: Point
clouds, meshes and voxel grids.
Meshes are representations of 3D objects using polygons, e.g. triangles or quadrilaterals, that form a mesh of faces in a 3D space (X, Y
and Z axes). Each polygon consists of vertices, edges and an orientation
vector connected to its immediate neighbour (without overlap) to form
objects. This type of representation allows for fast processing as simple
shapes are used to represent complex objects. Usually, meshes are
obtained from point clouds after they have been processed by computer
software, or they can be created manually using CAD software, but this
is tedious and sometimes ineffective for some applications involving
complex objects. Meshes can also be created using voxel grids through
the use of appropriate software.
Point clouds are data points in a three-dimensional space, i.e. measurement points in the X, Y and Z axes. Each individual point represents
a spatial measurement on the surface of the object. To represent an
object, multiple points are acquired. Point clouds are permutation invariant, unlike voxel grids. They may or may not contain RGB, if so they
also contain information about the colour of the object. They can also
contain intensity information representing the strength of the reflection
of the laser pulse. These points are created with special tools, namely
laser scanners. The best known laser scanner is the Light Detection and
Ranging (LiDAR) sensor, which uses rapid laser pulses to measure multiple distances between the sensor and surfaces. These sensors provide
an accurate representation of real world space, surfaces and objects,
making this data suitable for examining objects in the real world. The
denser the points, the more detailed the representation, allowing the
study of textures or other smaller features. This technology can be used
to represent small objects such as chairs or manufacturing parts, or
larger objects such as historical monuments or entire representations
of urban environments. It can also be used for autonomous vehicles
by collecting multiple distances/point clouds that serve as input for
machine and deep learning algorithms, allowing the vehicle to make
fast decisions.
However, such data cannot be used as input for convolutional
networks because they are irregular graph data. An example of regular
data are voxel grids. Voxel grids are 3D grids organised in layers, rows
and columns. Each intersection between a layer, row and column is
called a voxel, which is assigned an intensity value. Voxel grids can be
thought of as fixed-size point clouds, where each voxel has a fixed size
and discrete coordinates, but point clouds can have an infinite number
of points for each space. Voxels are often used to represent medical
imaging, such as MRI, CT and other modalities.
Point clouds can be converted into 3D CAD models through a
process called surface reconstruction (Berger et al., 2017), or even used
to create meshes or voxel grids. Point clouds can be used to represent
volumetric data, such as in medical imaging, for multi-sampling and
data compression, as point clouds are more memory efficient than voxel
grids (Sitek et al., 2006). Any data format can be converted to another,
but information is always lost when converting point clouds to another
format, as usually not all points are represented during the conversion,
e.g. when converting a point cloud to a voxel grid.
Building computer vision pipelines for 3D data is not a mere
extension of traditional deep learning techniques that work perfectly

ğ‘›
ğ‘(ğ‘‹, ğœƒ) = ğ‘(ğœƒ)ğ›±ğ‘–=1
ğ‘(ğ‘¥ğ‘– |ğœƒ)

(1)

ğ‘› ğ‘(ğ‘¥ |ğœƒ) is the points for the object.
where ğ‘(ğœƒ) is the object and ğ›±ğ‘–=1
ğ‘–

4

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Existent generative models such as GANs only work with datasets
that are a set of fixed dimensional instances, whereas point clouds are a
set of sets. Using traditional GANs to learn a marginal distribution ğ‘(ğ‘¥),
where ğ‘‹ is point clouds, is unrealistic since the marginal distribution
is uninformative for such algorithms.
Li et al. (2018) purpose the use of a generator that takes as input
the noise vector ğ‘§ and a descriptor encoding ğœ“ of the distribution of ğœƒ,
and defines the objective function of the GAN by Eq. (2).
Eğœƒâˆ¼ğ‘(ğœƒ) [ğ‘šğ‘–ğ‘›ğºğ‘¥ ,ğ‘„ ğ·(Pâˆ¥G)]

(2)

where P is ğ‘(ğ‘‹|ğœƒ), G is ğºğ‘¥ (ğ‘§, ğ‘„(ğ‘‹)), where ğ‘„(ğ‘‹) is an inference network
to learn the informative description ğœ“ about the distribution ğ‘(ğœƒ|ğ‘‹).
However, the size of ğ‘‹ and the permutation of the points is variable,
which increases the complexity of this problem. Furthermore, the PCGAN also does not take into account the shape of the objects or the
relationship between points in the same neighbourhood, which makes
such a solution suboptimal compared to the use of voxel grids.
Sulakhe et al. (2022) is an example of a successful attempt to use
point clouds as input for GANs. They have developed a solution for
reconstructing skulls with GANs and point clouds instead of using 3D
meshes and CAD software. In their experiments, for each surface (each
3D mesh of the ROI of the skull part to be reconstructed), m points
are sampled, resulting in a point cloud of m points, where m=1024.
Specifying a certain number of points per point cloud allows for easier
use of conventional GAN approaches. The proposed CranGAN is an
autoencoder based architecture conditioned by the defective skull ğ‘ƒğ‘–ğ‘› ,
where the goal is to train a generator: ğ‘ƒğ‘œğ‘¢ğ‘¡ = ğº(ğ‘ƒğ‘–ğ‘› ). The encoder is
based on the PointNet (Qi et al., 2017), where the classifier head is
replaced by a 256-dimensional embedding and the decoder consists
of fully connected layers. The discriminator is adapted from the PCGAN (Li et al., 2018), i.e. it consists of fully connected layers that
classify the data as real or fake. The objective function is similar
to Eq. (3), where ğ‘§ is replaced by ğ‘ƒğ‘–ğ‘› . In contrast to PC-GAN, the
vanilla GAN objective as well as LSGAN and WGAN-GP were tested.
The results show that using the vanilla GAN objective leads to better
results compared to the other approaches.
GutiÃ©rrez-Becker et al. (2021) develops another framework that
works directly with point clouds for anatomical shape analysis. They
point out that using point clouds is more lightweight and simple than
using meshes. Both the generator and discriminator are adopted from
PointNet (Qi et al., 2017) to encode the point clouds. The generative
network is a conditional encoderâ€“decoder architecture. Their approach
takes into account that a shape represented by a point cloud must be
invariant to transformations such as scaling, translation and rotation.
To this end, the data is preprocessed to be centred by its centre of mass
and a network is trained to align the point cloud (ğ‘ƒğ‘Ÿğ‘ğ‘¤ ) by rotation:
ğ‘ƒğ‘Ÿğ‘ğ‘¤ â†’ ğœƒ that ğ‘ƒ = ğ‘‡ (ğœƒ)ğ‘ƒğ‘Ÿğ‘ğ‘¤ . The framework is then composed of:
(1) a rotation network; (2) a conditional generator conditioned by the
rotated point cloud; (3) a discriminator that distinguishes between the
generated and the real data. With their approach, multiple structures
can be used as input. To test the framework, the ADNI dataset (Jack
et al., 2008) was used by converting the MRI scans into meshes and
then uniformly sampling points to generate point clouds. The framework is capable of generating annotated point cloud data for various
tasks such as classification, regression, among others.
Ben-Hamu et al. (2018) developed a GAN that works with meshes
and generates human body and tooth meshes. The proposed architecture is based on Karras et al. (2017), with the number of channels
adapted to the mesh data, i.e. ğ‘˜ Ã— ğ‘˜ Ã— 3|ğ¹ | where ğ‘˜ is the grid size, 3 the
number of coordinates (ğ‘¥, ğ‘¦, ğ‘§) and ğ¹ the face of each triangulation
of the mesh, and the convolutions are replaced by periodic convolutions, producing spherical surfaces. With this model it is possible to
automatically generate massively plausible random models.
In summary, the use of point clouds and meshes as direct input
to deep learning algorithms is still an under-researched topic, leading
researchers to convert such irregular data into a regular format, i.e. 3D

Fig. 2. Illustration of the vanilla generative adversarial network (GAN). The solid lines
are the data transfer and the dashed lines are the feedback/losses.

voxel grids, and use the better-studied GANs, although this type of
conversion adds unnecessary volume to the representation and makes it
less memory-efficient (Qi et al., 2017). It was found that when working
with raw point clouds, the PointNet network is usually used as an
encoder to capture the features of this data. A more specific network
was expected for meshes, without using convolutions. However, as can
be seen in the work of Ben-Hamu et al. (2018), the use of convolutions
could be appropriate for the generation of meshes. It is expected that
more networks will emerge that can handle such data, as they contain
a different type of information that is relevant to other areas such as
autonomous driving.
2. Generative adversarial networks
GANs were first introduced by Goodfellow et al. (2014). They proposed a new framework in which two networks are trained to compete
and overcome each other: the generator and the discriminator. The
generator is trained to learn the real data distribution, i.e. it learns the
distribution of the dataset and generates new synthetic data. The discriminator is trained to discriminate between real and synthetic data.
The latter can be trained with one of two main objectives: calculate the
probability of the data being real or fake, or calculate the realness or
falseness of the given data (Arjovsky et al., 2017a).
Fig. 2 illustrates how a vanilla GAN works. The Generator receives
a random vector (ğ‘ğ‘Ÿ ) as input and generates fake data. Then the
Discriminator receives fake and real data and returns a probability. This
value is the feedback/loss that is passed on to the generator and the
discriminator itself. If the discriminator always outputs values close to
0.5, this means that it is not able to distinguish between true and fake
samples, so convergence has been archived.
In the original work, random noise was used as input to the generator, but it can be extended to a variety of input types by using other
GAN architectures, e.g. conditional GAN (cGAN) (Mirza and Osindero,
2014). This allows the discriminator to receive auxiliary information,
e.g. labels, in addition to synthetic and real data.
GAN training, also called â€˜â€˜minmax gameâ€™â€™, aims to satisfy the objective Function (3), where ğ‘¥ denotes the real data, ğ‘ğ‘§ (ğ‘§) denotes a
prior input noise, ğº(ğ‘§) denotes the generated image, ğ·(ğº(ğ‘§)) denotes
the probability of the fake image to be true, and ğ·(ğ‘¥) the probability
of a real image to be true. For the cGAN, the objective function is very
similar with the original one, but ğ·(ğ‘¥) â†’ ğ·(ğ‘¥ âˆ£ ğ‘¦), and ğ·(ğº(ğ‘§)) â†’
ğ·(ğº(ğ‘§ âˆ£ ğ‘¦)), where ğ‘¦ is the condition.
ğ‘šğ‘–ğ‘›ğº ğ‘šğ‘ğ‘¥ğ· ğ‘‰ (ğ·, ğº) = Eğ‘¥âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘¥) [ğ‘™ğ‘œğ‘”(ğ·(ğ‘¥))]
+ Eğ‘§âˆ¼ğ‘ğ‘§ (ğ‘§) [ğ‘™ğ‘œğ‘”(1 âˆ’ ğ·(ğº(ğ‘§)))]

(3)

However, this equation has the problem of saturation in minimising
the loss of the generator ğ‘™ğ‘œğ‘”(1 âˆ’ ğ·(ğº(ğ‘§))). To solve this problem, Goodfellow et al. (2014) propose to maximise ğ‘™ğ‘œğ‘”(ğ·(ğº(ğ‘§))) instead. This
technique is also known as a non-saturating GAN. For more detail about
the vanilla GAN and cGAN, refer to Goodfellow et al. (2014), Mirza and
Osindero (2014).
5

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

point at which the discriminator is no longer able to distinguish
between real and fake data and only gives random guesses. When
this point is passed, the generator is fed with poor feedback,
causing the results to deteriorate;
â€¢ Diminished gradient: On the other hand, the feedback becomes
meaningless to the generator if the discriminator performs too
well. If the generator cannot improve as fast as the discriminator,
or in the case of an optimal discriminator, this problem can occur;
â€¢ Overfitting: When the amount of data is very limited and no precautions are taken, the discriminator may overfit on the training
data, i.e. the discriminatorâ€™s output distribution for the real and
the fake samples do not overlap, making the feedback meaningless;
â€¢ Imperception: no loss function or evaluation metric is able
to mimic human judgement, which makes comparison between
models very challenging without human intervention. The largescale applications, e.g., denoising, reconstruction, synthetic data
generation and segmentation, bring a lot of heterogeneity, which
makes it harder to define how we can evaluate them.

2.1. Main purposes of GANs
Goodfellow et al. (2014) used GANs to improve the realism of generated images beyond what was achieved with autoencoders, variational
autoencoders and other generative models. In the last few years, GANs
have been further developed with works such as Karras et al. (2018,
2019), which are capable of creating realistic human faces. The rapid
progress of GANs has raised some concerns, such as the expansion
and introduction of new threats and attacks, according to Brundage
et al. (2018). The report examines common artificial intelligence architectures and addresses several concerns about the security threats,
for example phishing attacks or speech synthesis, which enables easier
attacks on a larger scale.
Apart from these concerns, GANs enabled the development of great
tools such as image-to-image translation (Isola et al., 2017; Zhu et al.,
2017), text-to-image translation (Zhang et al., 2017; Dash et al., 2017),
super-resolution (Ledig et al., 2017), 3D object generation (Wang et al.,
2017), semantic translation to image (Wang et al., 2018), removal of
noise and image correction (Zhang et al., 2019a; Tran et al., 2020),
disentanglement using GANs (Wu et al., 2021), among many other
applications.
GANs have also improved algorithms in the medical field, mainly
through their ability to generate synthetic images to train other deep
learning algorithms. Jeong et al. (2022) provide a systematic review
of medical image classification and regression and demonstrates the
results that can be achieved by using these architectures. Yi et al.
(2019) presents a more comprehensive overview of GANs in medical
image analysis, highlighting the modalities and tasks.
On the other hand, our systematic review examines the use of
GANs to generate volumetric data, including medical and non-medical
data but properly separated for clarity. These papers report on the use
of GANs for denoising, nuclei counting, reconstruction, segmentation,
classification, image translation or simply general applications.

These challenges are not limited to the generation of volumetric
data, but are even more difficult to overcome due to the additional
dimensionality. More dimensions mean more data complexity and more
computational effort for processing, which makes the experiments more
difficult and time-consuming. As a result of these challenges, the generation of volumetric data with GANs is still under-researched compared
to 2D imaging.
2.3. Further reading
As supplementary reading, we present the following list of papers:
â€¢ Goodfellow et al. (2014) â€” Generative adversarial nets;
â€¢ Gui et al. (2020) â€” A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications;
â€¢ Kazeminia et al. (2020) â€” GANs for medical image analysis;
â€¢ Lucic et al. (2018) Are GANs Created Equal? A Large-Scale Study.
â€¢ Zhu et al. (2017) â€” Unpaired Image-to-Image Translation using
Cycle-Consistent Adversarial Networks;

2.2. Advantages and disadvantages
The main advantages of GANs are mentioned above in Section 2.1.
They can be used to generate realistic data which might be used for
data augmentation for other purposes, with a higher realism than other
approaches. As demonstrated in Ferreira et al. (2022b), the use of GANs
for data augmentation can outperform the use of conventional data
augmentation.
One of the main concerns with this generative technology is the
ability to produce deep fakes (Shen et al., 2018; Ponomarev, 2019) and
trick detection algorithms or even humans, which can lead to misunderstandings or fraud. As evidenced by Brundage et al. (2018), this raises
several cybersecurity concerns, as images or even fake videos can be
created that are so realistic that they can fool anyone with a simple
application, e.g. Deep Fakes (accessed [06-06-2022]). However, it can
also be used to improve cybersecurity and combat deep fakes (Navidan
et al., 2021; Arora and Shantanu, 2020). Although this technology
permits harmful effects, the benefits that can be derived from it are
immense.
In addition to the concerns mentioned above, GANs also have
challenges that are more technical (Chen, 2021):

These papers present how GANs work, the different ways they can
be used, and the general advantages and disadvantages. They provide
the reader with a deeper, but also broader, understanding of GANs,
although reading them is not mandatory to understand the purpose of
this review.
2.4. Applications
2.4.1. Image translation
Image translation converts an input image into another synthetic
version of that input image, e.g. a photo taken in the morning into
an evening photo. Only works dealing with the translation of medical
images were found in this review. In the medical context, this is usually
the translation between modalities, i.e., CBCT â‡‹ CT, CT â‡‹ PET, CT
â‡‹ MRI, and MRI â‡‹ PET. When the dataset contains both images,
a Pix2Pix-based cGAN architecture can be used. However, in cases
where paired data is not available, a CycleGAN-based architecture is
generally used which takes advantage of the cycle consistency loss
(Appendix A.1.5). Taking as example Fig. 3, ğ‘ ğ‘‡ 2 = ğºğ‘‡ 1â†’ğ‘‡ 2 (ğ‘‡ 1), ğ‘ ğ‘‡ 1 =
ğ¹ğ‘‡ 2â†’ğ‘‡ 1 (ğ‘‡ 2).
This application is particularly appealing in situations where the
presence of paired data is important, e.g. in brain tumour segmentation,
or in situations where the patientâ€™s exposure to a particular modality
should be reduced, e.g. in creating synthetic CT scans from MRI scans
so that no radiation is used. With the CycleGAN architecture, it is even
possible to create paired data without having a previous paired dataset.

â€¢ Mode collapse: when the generator is not able to produce a large
number of outputs, but always produces a small set of outputs
or even the same one. This can happen when the discriminator is stuck in a local minimum and the generator learns that
it is possible to produce the same set of outputs to fool the
discriminator;
â€¢ Non-convergence: as the generator produces more and more
realistic data and the discriminator cannot follow this evolution,
the discriminatorâ€™s feedback gradually becomes meaningless. The
convergence point of the network can be characterised by the
6

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 5. Basic illustrations of a classifier training using synthetic data.

defined as ğ‘‰ğ‘‘ğ‘’ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ğ‘‘ = ğºğ‘‘ğ‘’ğ‘›ğ‘œğ‘–ğ‘ ğ‘–ğ‘›ğ‘” (ğ‘‰ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ ). Normally, noise from a known
distribution, such as a Gaussian distribution, is used to create pairs of
noisy and denoised volumes, but in real-world scenarios the noise does
not follow any particular distribution. To solve this problem, Li et al.
(2020) has developed a GAN to learn the distribution of real-world fast
optical coherence Doppler tomography (ODT) noise. Then, this noise
is added to the noise-free ODT and a GAN-based denoiser is trained.
Such approaches can lead to reducing the acquisition time of ODT scans
without compromising the quality of the volumes.

Fig. 3. Basic illustration of the CycleGAN architecture.

2.4.3. Classification
Classification involves training a model that predicts the correct
label/class of the input data, i.e. instead of outputting a volume, the
network only outputs the label. The vanilla GAN consists of a generator
and a classification network, i.e. the discriminator. In some cases, the
discriminator is trained not only to classify the input data as real or
fake, but also to predict the class of the object, e.g. in Muzahid et al.
(2021). The classification task is not directly related to the generator, but can be used to improve it, e.g. a conditional generator (ğ‘ğº)
must be able to produce synthetic data with features corresponding
to the class used as a condition, as illustrated in Fig. 5. This is formally defined as ğ‘ƒ ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› = ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ ğ‘–ğ‘’ğ‘Ÿ(ğ‘ ğ‘€ğ‘…ğ¼ğ‘ ğ‘–ğ‘ğ‘˜ ) where ğ‘ ğ‘€ğ‘…ğ¼ğ‘ ğ‘–ğ‘ğ‘˜ =
ğ‘ğº(ğ¿ğ‘ğ‘ğ‘’ğ¿ğ‘ ğ‘–ğ‘ğ‘˜ ), and the loss can be calculated with e.g. CE, ğ¿ğ‘œğ‘ ğ‘  =
ğ¿ğ¶ğ¸ (ğ‘ƒ ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ¿ğ‘ğ‘ğ‘’ğ¿ğ‘ ğ‘–ğ‘ğ‘˜ ). If this synthetic data is classified correctly,
it means that the generator is producing good data. A classifier can be
used to classify synthetic data directly from the conditional generator.
This approach is very useful for situations with limited annotated data,
as the synthetic data contains the appropriate labels that were used
as input to the generator, e.g., Jung et al. (2020) trained a conditional
generator to create synthetic brains with different stages of Alzheimerâ€™s
disease. This task is also useful for evaluating the quality of the generated data and comparing models, e.g. when the goal of generating
synthetic data is to improve a classification model. Accuracy, AUC,
and CE (Sections B.2.1, B.2.2 and A.3.3) are often used to evaluate the
performance of classifiers.

Fig. 4. Basic illustrations of reconstruction architectures: First row â€” From low to
high resolution; Second row â€” Inpainting.

2.4.2. Reconstruction and denoising
In computer vision, reconstruction is the process of capturing the
shape and appearance of real objects in order to complete, i.e. reconstruct, incomplete objects. The basic pipelines of two reconstruction
tasks (low-to-high and inpainting) are shown in Fig. 4, where the high
resolution volume (ğ‘‰â„ğ‘–ğ‘”â„ ) is given by ğ‘‰â„ğ‘–ğ‘”â„ = ğºğ‘™ğ‘œğ‘¤â†’â„ğ‘–ğ‘”â„ (ğ‘‰ğ‘™ğ‘œğ‘¤ ), and the
complete volume (ğ‘‰ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¡ğ‘’ ) is given by ğ‘‰ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¡ğ‘’ = ğºğ‘–ğ‘›ğ‘ğ‘ğ‘–ğ‘›ğ‘¡ (ğ‘‰ğ‘–ğ‘›ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¡ğ‘’ ). The
former is also known as â€˜â€˜super-resolutionâ€™â€™, where a GAN is trained to
increase the resolution of low resolution samples. Using GANs for this
could be interesting for some tasks, however, unrealistic information
could be added that could harm the downstream task, e.g. artefacts in
super-resolution of low-dose CT scans.
In this review, most reconstruction tasks were from 2D to 3D, i.e. reconstruction of volumes from two-dimensional images. Other works
seek to generate complete from corrupted 3D scans, e.g., Wang et al.
(2017), who uses a generative network to complete damaged 3D objects
(this task can also be called inpainting). The remaining ones deal
with the generation of high-resolution volumes from low-resolution,
e.g., Halpert (2019), who improve the resolution of 3D seismic images. Reconstruction can also be applied in medicine, e.g. Moghari
et al. (2019) has developed a GAN to generate high-resolution CT
from low-dose CT scans. This task can bring several advantages, such
as shortening the acquisition time or reducing the radiation needed.
Commonly, PSNR (Appendix B.1.6) is used to assess the quality of these
reconstructions, as a volume with low resolution can be considered a
volume with noise, i.e. with a low PSNR. MS-SSIM, MSE and other
voxel-wise metrics (Section 3.2) can also be used to evaluate the
reconstruction task.
Denoising is the process of removing noise (i.e. grainy appearance,
artefacts and random information) from the volume to restore the
true appearance of the volume. This process is very delicate because
removing noise can result in removing important features and details
of the volume. This task is very similar to reconstruction, but it is
more specifically about removing artefacts from the volume rather
than increasing resolution or inpainting. Formally, this task can be

2.4.4. Segmentation and nuclei counting
Segmentation is the classification of voxels to identify whole objects
that belong to a class. The volumes are then divided into distinguishable and meaningful regions for object recognition. This task
has been extensively researched by the computer vision community
as it allows for the automation of various tasks such as counting the
number of people in an image, detecting and measuring the volume of
a tumour for treatment planning, organ segmentation and much more.
The papers found in this review that use GANs for segmentation are
all related to the medical field, but can also be used for non-medical
tasks. For example, Zhang et al. (2021) uses two GAN architectures for
semantic segmentation of 3D pelvis CT scans, using a generator (ğº)
to produce unannotated synthetic scans. Then, the synthetic data and
the annotated real data are used to train another GAN consisting of a
segmentation network (ğ‘†) and a discriminator to distinguish between
real masks and masks generated by the segmentation network. With
this approach, it is possible to use both labelled and unlabelled data.
This is formally defined as ğ‘¦Ì‚ = ğ‘†(ğ‘¥)
Ì‚ with ğ‘¥Ì‚ = ğº(ğ‘§) where ğ‘§ is a random
7

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 6. Number of volumetric data papers per modality.

vector and ğ‘¦Ì‚ the segmentation. Ho et al. (2019) use a CycleGAN-based
architecture to generate synthetic scans with the corresponding segmentation labels. The goal is similar to Fig. 3, but instead of translating
between modalities, the translation is performed between the synthetic
microscopy 3D volumes and the segmentation mask.
Han et al. (2019b) is the only work that investigates GANs for nuclei
counting. This task is very similar to Ho et al. (2019) segmentation explained earlier, but the real downstream task is not segmentation. Han
et al. (2019b) train a 3D GAN to produce synthetic distance maps,
which are then used for nuclei counting by performing threshold and
connected component analysis.

Fig. 7. Number of volumetric data papers per non-medical application.

2.4.5. General
In this overview, general task means generating synthetic data
without applying it to a specific downstream task. This task aims
to improve the visual aspect of the generated volumes. Rusak et al.
(2020) used a GAN to improve the borders between tissues of synthetic
brain MRI scans, Danu et al. (2019) generated synthetic blood vessels
indistinguishable from the real ones, and Liu et al. (2019) generates
synthetic Berea sandstone and Estaillades corbonate to increase the
amount of data available for other tasks. Usually, the visual assessment
is the chosen approach to evaluate such task, however, it is intriguing
why so few works used the visual Turing test B.1.8.

Fig. 8. Number of medical papers per organ.

Reconstruction is the main application of GANs in the non-medical
context and image translation in the medical context, as can be seen in
Fig. 7. It is worth noting that most papers in the field of medicine are
related to humans (Fig. D.2). The most commonly studied organ in the
medical field is the brain, as shown in Fig. 8. This popularity results
from the increasing use of MRI to study the brain and the need for
pipelines to process these images and automate processes. MRI scans of
the brain are also easier to process/analyse compared to the rest of the
body because there is less movement and variation, resulting in fewer
artefacts and faster acquisitions.
In Fig. D.3 it is possible to find the number of publications per
year in relation to the different modalities. CT and MRI are present
almost every year, which corresponds to the reality of volumetric data
acquisition. Especially in the medical field, MRI and CT are widely
used. In the non-medical field, CT and X-rays are mainly used, but
CAD is also intensively studied due to development of sensors such as
RGB-D (ZollhÃ¶fer et al., 2018) and LiDAR2 (Collis, 1970).
Figs. 9 and 10 contain the number of papers that use a particular
loss function or evaluation metric directly in the generation task (and
not in a secondary task). Most of them are easy to explain and widely
known, but some require some context to be understood, so it is
necessary to read Sections 3.1 and 3.2, as well as Appendices A and
B for better understanding.
Fig. 9 shows that mean absolute error (MAE) is the most commonly
used loss function because it helps to stabilise the training of the
generator and avoid mode collapse (Thanh-Tung and Tran, 2020),
especially in the first steps of the training (cross entropy (CE) and
mean square error (MSE) do the same). Cycle consistency is also

3. Generating realistic synthetic 3D data: a review of works
The use of GANs to generate synthetic data has increased significantly in recent years. These deep learning networks are becoming
very popular, as they can generate more realistic and sharper synthetic
images than other traditional generative approaches. GANs are implicit
models as they do not use explicit density functions, in contrast to
variational autoenconders, which are explicit models (Mohamed and
Lakshminarayanan, 2016). Several review articles have been submitted
on the use of GANs for different purposes, ranging from more general
review articles, e.g. Alqahtani et al. (2021), to more specific articles,
e.g. Apostolopoulos et al. (2022).
This section provides statistical information on the papers considered relevant. The number of papers on volumetric imaging has
increased in recent years, and from 2014 to 2016 no papers were
published (Fig. D.1). MRI and CT are the two most popular modalities
(Fig. 6), with more than half of the papers considered applied in the
medical field (Fig. D.2).
The modalities used in the reviewed papers are: MRI (McRobbie
et al., 2017), CT (Scarfe et al., 2006), PET (Townsend, 2008), optical coherence tomography (OCT) (Bezerra et al., 2009), microscopy1 (Fadero
et al., 2018), CAD (Shivegowda et al., 2022), red, green, blue depth
sensors (RGB-D) (Zhou et al., 2021), seismic reflection data (Dumay
and Fournier, 1988), focused ion beam scanning electron microscopy
(FIB-SEM) (Fischer et al., 2020), and kelvin probe force microscopy
(KPFM) (Melitz et al., 2011).

1

2

What is Electron Microscopy? (accessed [06-06-2022])
8

What is LiDAR? (accessed [06-06-2022])

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 9. Number of papers per loss function/functions.

one of the most commonly used loss functions due to its ability to
perform image translations, and then WGAN/WGAN-GP (Wasserstein
GAN/Wasserstein GAN with gradient penalty) for its ability to stabilise
and prevent vanishing gradient (Gulrajani et al., 2017).
As can be seen in Fig. 10, the peak signal-to-noise ratio (PSNR)
is the most common evaluation metric as it is able to evaluate reconstructions, which can be considered one of the main applications
of volumetric GANs, although GANs are explored for numerous other
purposes. Structural similarity index measure (SSIM) is the second
most used metric because it can better assess the quality of the data,
taking into account human perception. MAE is often used because it
is very easy to calculate and helps stabilise GAN training by making
the generator produce data that is realistic rather than just trying to
trick the discriminator (Isola et al., 2017; Pathak et al., 2016). Usually,
FrÃ©chet inception distance (FID) is considered the most reliable metric.
However, it is not easy to adapt to volumetric data and even the existing adaptations are not well accepted by researchers. Accuracy is not
really used to assess the quality of the images in terms of perception,
but to assess the quality of the data to perform data augmentation,
and to assess the improvements of other tasks when synthetic data are
used, e.g. segmentation and classification tasks. For that reason, dice
similarity coefficient (DSC), sensitivity and F1-score are also commonly
used.
The proposed taxonomy for medical and non-medical GANs is
shown in Figs. 12 and 13 which can guide researchers to relevant
publications they are interested in. The taxonomy has been divided
into two different figures due to its size. First, a distinction is made
between medical and non-medical applications, then by application,
architecture, and the respective papers. For example: A user wants to
perform a modality translation of volumetric brain images, following
â€˜â€˜Medicalâ€™â€™ â†’ â€˜â€˜Image Translationâ€™â€™ â†’ â€˜â€˜Brainâ€™â€™ will find the architectures
used and the corresponding papers; Another user wants to reconstruct
non-medical 3D volumes from 2D images, following â€˜â€˜Non-Medicalâ€™â€™ â†’
â€˜â€˜Reconstruction (2D to 3D)â€™â€™ will find all available architectures and
papers for 2D to 3D reconstruction.
Equivalent to Fragemann et al. (2022), we group the works by task
and body part. We have explicitly decided against a technical grouping
at the first level of the hierarchical level, as the focus of the review is on

Fig. 10. Number of papers per evaluation metric/metrics. The incomplete loss function
description in the figure is â€˜â€˜Volume fractions, triple phase boundary (TPB), double
phase boundary (DPB), densities, relative surface area, relative diffusivity, surface area,
two-point correlation function (TPCF)â€™â€™.

the data aspect. So, from the readersâ€™ point of view, with a certain type
of data in mind, they can find appropriate techniques to utilise. Furthermore, our taxonomy shows that bins/leafs are still under-researched
and need more attention from the research community.
Fig. 11 shows each different architecture used in each application
for medical and non-medical purposes, showing that cGAN-based and
CycleGAN-based architectures are used for several different applications.
3.1. Loss functions
This section provides a brief explanation of each loss function used
in the reviewed papers, along with any references needed and each
paper in which each metric was used. It is important to note that some
loss functions were developed for a specific problem and only make
sense in that context, or they are an adaptation of existing metrics, such
as shape consistency and identity.
In Tables 1â€“3, the loss functions between the parentheses were used
in the downstream/secondary task and not directly in the generation
process.
By definition, the adversarial loss is mandatory when using GANs,
as this is the breakthrough for these architectures, so this loss function
is always used, although with some variations, e.g., JS, WGAN, WGANGP, LSGAN, Hinge. When the authors do not mention which strategy
was used, the vanilla GAN loss function is usually applied, i.e. JS, so
that â€˜â€˜Advâ€™â€™ appears in the tables when the vanilla GAN is used, or it is
not mentioned otherwise.
9

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

its complexity, the complexity of the architecture and the optimisation
method. Sulakhe et al. (2022) tested the use of Vanilla GAN, LSGAN
and WGAN-GP and achieved better results with the first function, which
also shows that no function is generally better than the other, it depends
on the task at hand. Therefore, tuning the hyperparameters in each of
the cost functions used can lead to better performance and thus a better
return on investment. As can be seen in this overview, most works stick
to the use of the vanilla GAN or the WGAN/WGAN-GP, which should
be the cost functions to experiment with first.
Some recommendations can be extracted from this and other papers
published until now:
â€¢ Starting with the non-saturating version of the vanilla GAN cost
function. If it still collapses after the following recommendations,
experiment with the WGAN-GP.
â€¢ Using the Adam optimiser (and it is recommended to use this
optimiser except for WGAN which should use RMSProp), ğ›½1 = 0.5.
â€¢ Using a learning rate of 0.0002 or less can result in more stable
training. Tuning the learning rate is very important if the training
collapses.
â€¢ If the discriminator or the generator is trained more often than
the other, mode collapse and training instability can be reduced.
This can be deduced from the loss plots of the generator and discriminator. If one of the two clearly dominates, such an approach
should be tested.
â€¢ Choosing a lower learning rate for the generator than for the
discriminator can also contribute to stability by forcing the generator to make minor changes to fool the discriminator, rather than
sudden changes.
â€¢ For the WGAN-GP, the weighting of the GP is also important. 10
is recommended, but other values can also provide better results,
e.g., Gupta et al. (2021) set 1.
â€¢ Do not use batch normalisation in the discriminator for WGANGP training. Instance normalisation should be used for every
approach.
â€¢ The use of spectral normalisation is also appropriate to stabilise
the training of the discriminator (Ferreira et al., 2022b).

Fig. 11. Architectures used in every (A) medical application (B) non-medical
application.

If a cycle GAN-based architecture is chosen, cycle consistency is a
good loss function (as are feature consistency, identity, shape consistency and spatial consistency, if possible), as it allows image translation
with more stability and realism.
The use of a pixel-wise loss function (e.g. CE, MSE, MAE, Lp ) in the
generator is also important to stabilise it, especially in the first steps
of the training. The WGAN-GP loss function is also a good choice to
stabilise training and avoid mode collapse, as explained below.
The loss function in the frequency domain seems to be very promising to avoid blurring, however, it has only been used in one paper (Ma
et al., 2020), but the ablation studies show that the use of this loss
function has improved their results.
The loss functions are grouped in 4 groups: â€˜â€˜Adversarial lossâ€™â€™, â€˜â€˜Loss
functions to explore the intermediate layersâ€™â€™, â€˜â€˜Pixel/Voxel-wise loss
functionsâ€™â€™, and â€˜â€˜Other loss functionsâ€™â€™. In Sections 3.1.1, 3.1.2, 3.1.3,
and 3.1.4 a summary and insight about each group is given. All the
metrics are explained in detail in Appendices A.1, A.2, A.3, and A.4
respectively. For readers interested in various loss functions, we refer
to an excellent review of them (Taha and Hanbury, 2015).

In environments with limited computing power, starting with a
subset of the original dataset can speed up hyperparameter tuning and
detect bugs:
â€¢ First make sure that the generator is able to learn by freezing the
discriminator and vice versa.
â€¢ Second, test with only one sample to confirm that both networks
are capable of overfitting. If everything works, this means that
the networks should also be suitable for larger datasets.
â€¢ Third, use a larger subset and review the loss plots to check for
training instability and mode collapse (looking at some generated
samples can also help identify mode collapse). If one of the
networks outperforms the other, the above tactics should be used.
â€¢ Finally, test with the entire dataset.

3.1.1. Summary of the adversarial loss functions
Lucic et al. (2018) conducted a study between different GAN objective functions, including the vanilla GAN, LSGAN, WGAN and WGANGP (functions used to generate volumetric data, as seen in this review).
After experimenting with different datasets, the authors concluded that
no method consistently outperformed the non-saturating version of
the vanilla GAN (Section 2). The authors point out the limitations of
the conducted study and mention that more complex datasets with
higher resolution might require more layers in the neural network
architectures. This is especially true for volumetric data, as a volume
with a resolution of only 128 Ã— 128 Ã— 128 has more voxels than an image
of 1024 Ã— 1024 (2097152voxels > 1 048 576 pixels), and the presence
of contextual and spatial information in volumes which adds more
complexity to volumetric datasets even if they contain fewer samples. A
pattern can be discerned: WGAN and WGAN-GP seem to perform better
on more complex datasets, inferring that such cost functions provide
better and more stable training. However, the authors recommend
investing more time in optimising the hyperparameters than in the cost
functions. None of the functions stand out from the others in terms of
stability and generation quality, as this depends heavily on the dataset,

For image translation, e.g. between MRI and CT, CycleGAN with cycle consistency loss should be used, whenever possible with shape consistency, spatial consistency, identity consistency or feature consistency
(which are explained below).
In our personal experiments with the BraTS2021 dataset to generate
synthetic tumours, using WGAN-GP led to more instability and worse
results than using the non-saturating vanilla GAN loss. We also found
that training the generator twice as much as the discriminator per
iteration led to more stable training and thus better results.
3.1.2. Summary of the loss functions to explore the intermediate layers
All of this loss functions (except content and style) assume that
a pre-trained model is capable of extract the features that correctly
describe the ground truth. In a certain level, these approaches can be
10

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 12. Proposed taxonomy of medical volumetric GANs. *These works performed tests in more than one structure.

Fig. 13. Proposed taxonomy of non-medical volumetric GANs.
11

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

compared with knowledge distillation, but here the â€˜â€˜teacherâ€™â€™ (the pretrained model) and the â€˜â€˜studentâ€™â€™ (the model to train) have distinct
goals and the â€˜â€˜studentâ€™â€™ might not be smaller than the â€˜â€˜teacherâ€™â€™. However, as discussed in Appendix A.2.4, this assumption may be wrong
and the pre-trained model may not be trained correctly (e.g. biased by
the data used) or the dataset used for training and the new dataset may
be very different, leading to an incorrect extraction of high and low
level features.
In any case, if these functions are intended to be used, the latent vector or the feature-consistent should be chosen, avoiding the perceptual
loss. The feature-consistent loss is particularly interesting, as it does not
require ground truth to work well with the CycleGAN architecture. If
ground truth is available, it is recommended to use the identity loss
(Appendix A.3.5) as it is more intuitive to use, less computationally
intensive and not potentially biased.
The content and style loss functions are a mixture of the feature and
vanilla adversarial loss, where importance is also given to the encoded
vector of each layer of the discriminator to compare high and low level
features, rather than just comparing one value of reality. This idea is
interesting, especially if one of the features is to be emphasised more.
In other cases, however, it could be impractical.

3.2. Evaluation metrics
This section presents a brief explanation of each evaluation metric
used in the papers studied, along with the required references and each
paper in which the particular metric was used. It is important to note
that some metrics are only used for specific purposes and are not wellknown, or are sometimes just an adaptation of existing metrics for the
problem in question but with a different name, such as fraction of
unmachinable voxels and shape-score.
The evaluation metrics are divided into â€˜â€˜Generation Taskâ€™â€™ and
â€˜â€˜Downstream Taskâ€™â€™, as some of them are not used to directly evaluate
the quality of the generated data, i.e. the metrics in the â€˜â€˜Generation
Taskâ€™â€™ are used to directly evaluate the quality of the synthetic data,
while the metrics in the â€˜â€˜Downstream Taskâ€™â€™ are used to evaluate a
specific task that uses synthetic data. In Tables 1â€“3, the metrics in
parentheses are those used in the â€˜â€˜Downstream Taskâ€™â€™. The metrics
for assessing the downstream task are not discussed in detail, as these
metrics are not used to directly assess the quality of the synthetic data
and are highly dependent on each specific task. For more information
on each individual evaluation metric for the downstream task, see
Appendix B.2.
It was expected that the visual Turing test would be used more
often, as human specialists are the most reliable method to determine
the realism of the generated data. However, this is comprehensible as
it is very time-consuming and requires specialists. FID comes closest to
human judgement, but is mainly used for the evaluation of 2D data, as
the adaptations to 3D lack in volumetric context. When ground truth
is available, e.g. in a denoising task, metrics such as PSNR, SSIM and
MS-SSIM are the most appropriate to use.

3.1.3. Summary of the pixel/voxel-wise loss functions
Adding a pixel-wise metric, such as CE, BCE, MAE, MSE or ğ¿ğ‘ ,
makes the training more stable, especially at the beginning, and helps
the generator learn more quickly what the expected values of each
voxel are, rather than just trying to trick the discriminator. Using the
adversarial loss alone could lead to unrealistic results, as the generator
would only have the task of fooling the discriminator. However, using a
voxel-wise metric alone would produce blurred averaged images, which
is why using the adversarial loss is important for sharper results.
BCE is suitable for classification problems where only two options
are possible, e.g. real and fake, CE can be used when more than two
classes need to be compared, e.g. when classifying normal control (NC),
mild cognitive impairment (MCI), Alzheimerâ€™s disease (AD).
Projection, orientation and depth losses ensure good 3D generation
from 2D data, i.e. these metrics ensure that the visible part used as
input is perfectly reconstructed, but they are not able to control the
reconstruction of the non-visible part. For the non-visible part, several
outputs are possible, but using Laplacian loss or MAE/MSE to compare
the volumetric ground truth with the reconstruction could lead to more
stable training and better results.
It was expected that more work would be found in the frequency
domain, as several features of an image, e.g. texture, edges, among
others, can be captured by frequency. However, it is argued that such
translation to the frequency domain is unnecessary as 2D and 3D
convolutions are capable of capturing the same features.
For shape consistency and spatial losses, it is assumed that the
trained segmentation networks are able to correctly segment both
modalities. However, as mentioned in Section 3.1.2, this assumption
may lead to incorrect feedback and thus to poorer training of the GAN.
GMD is the loss function that focuses more on the edges. Therefore,
if expressive and non-blurred edges are important for the realism of the
generated volumes, this loss function is recommended.

3.2.1. Summary of the evaluation metrics for the generation task
The generation task evaluation metrics are divided by voxel-wise
metrics (Appendices B.1.1, B.1.2, B.1.3, B.1.4, B.1.5, B.1.6, and
B.1.7), visualisation (Appendices B.1.8 and B.1.9), middle layers (Appendices B.1.10 and B.1.11), and problem specific metrics (Appendices B.1.12, B.1.13, B.1.14, B.1.15, and B.1.16).
For GANs evaluation, every metric that is capable of measuring the
distance between the distribution of real world data and the distribution of the generator model can serve as quantifier of performance for
generative models.
PSNR is preferred over MSE because it more accurately assesses the
visual quality of the data produced. MSE is insensitive to blur, giving
good results even with blurred scans. However, PSNR has been shown
to be unreliable with human assessment. Slight changes in brightness
and contrast do not alter the visual perception of such images, but
drastically change the PSNR. MS-SSIM and SSIM are preferred over
PSNR and MSE for quality assessment, as they are able to detect structural changes, e.g., distortions, but are almost insensitive to changes in
hue (e.g., colour change). Therefore, if the colour is important for the
realism of the generated data, both metrics (PSNR and MS-SSIM/SSIM)
should be used for the evaluation. However, this will rarely be the case
for volumetric data.
Metrics such as CE, MSE, NCC to compare voxel values are better
used for training the generator or classifier, not so much for comparing
models, as this metric is too general and not able to capture small details that are important for the realism of the generated data. For each
specific case, there are the best metrics, e.g. the Minkowski functional
for synthetic generation of porous media, reconstruction resolution,
fraction of unmachinable voxels, and other attributes that are specific
to the object under study and should be retained in the synthetically
generated volumes. These attributes can be compared using graphs or
using metrics, e.g., MSE or MAE. When comparing models trained with
different datasets, or when the datasets are too heterogeneous, MAPE
between absolute values is also highly recommended, although MAE is
more commonly used.

3.1.4. Summary of other loss functions
Many other loss functions can be used as long as they are able to
capture the essential features for the realism of the data. Boundary and
volume loss functions are used by Momeni et al. (2021) to ensure that
the generated cerebral microbleed has the same volume as the input
and that the generated portions can merge with the surrounding tissue.
The Minkowski functional is used to measure the distance between geometric features of volumes, i.e. the volume, surface area, average width
and Eulerâ€™s number. This has been used to create synthetic porous
media, but can also be used for other structures, such as biomedical
images (Depeursinge et al., 2014).
12

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

semi-supervised GANs: This architecture is based on the semisupervised learning technique, which uses both labelled and unlabelled
data to train the network (Liu et al., 2020a). Here the discriminator is
also a classifier and the loss function of the discriminator also contains
the supervised component, i.e., if the discriminator classifies the input
data correctly.
DCGAN (deep convolutional generative adversarial networks):
It is based on vanilla GAN, but consists of convolutional layers and
transposition convolutional layers instead of fully connected layers. For
the generation of images and volumes, GANs consisting of convolutional layers are usually used, but fully connected layers may also be
used, as in the vanilla GAN.

In summary, the best approach for comparing models and accessing
the realism of the generated data strongly depends on the task, as
different tasks have different objects with different characteristics that
should be analysed in the evaluation or even in the training phase.
However, the use of MS-SSIM, PSNR and clustering t-SNE is strongly
recommended for an objective comparison and the visual Turing test
for a subjective comparison. The use of approaches that use pretrained models should be avoided unless the pre-trained model has
been trained on the dataset used for GAN training, but even then
it is difficult to ensure that the metric is accurate, as explained in
Appendices B.1.10 and B.1.11. Therefore, SIS and S-score are preferable
to IS and FID.
Naturally, using synthetic data for the downstream task is the best
way to assess whether the generated data is capable of improving the
task, but it does not guarantee realism.

3.3.2. 3D from 2D
This section reviews some architectures that are capable of generating volumes from 2D data. MP-GAN, GAN2Dto3D and SliceGAN
are able to generate volumetric data from a dataset consisting only of
2D images by using a 3D generator and a 2D discriminator. Z-GAN
and SPGAN are able to generate volumes from 2D images, but their
training dataset also contains the corresponding 3D data. For this task,
a generator consisting of a 2D encoder and a 3D decoder is used.
MP-GAN (Multi-Projection GAN): This architecture aims to solve
the problem of generating volumes when only 2D images are available (Li et al., 2019). The generator is given a random vector and
produces volumes that are projected into 2D images with multiple
viewpoints. Multiple discriminators are used to evaluate the realism of
such projections. The number of discriminators is equal to the number
of viewpoints, so each discriminator only needs to learn the distribution
of the particular view. Therefore, the discriminator is a 2D network and
the generator is a 3D network.
GAN2Dto3D: This architecture is very similar to the MP-GAN architecture, but instead of projecting the output of the generator into
multiple views, the generated volume is sliced into three coordinates,
i.e. X-Y, X-Z and Y-Z, and only one discriminator is used. It is used
by Sciazko et al. (2021) to generate synthetic 3D microstructures with
only 2D real data available for training.
SliceGAN: This architecture is used for the generation of a 3D
volume from a simple 2D image. Kench and Cooper (2021) presents
this architecture for generating 3D microstructures from 2D slices of
isotropic material. This architecture is very similar to GAN2Dto3D in
that the generator creates a volume from a random vector, decomposes
the volume in three axes and feeds the discriminator to compare
with slices from large 2D isotropic images. Kench and Cooper (2021)
adds that this approach is only possible for isotropic, nondirectional
material. Anisotropic materials require more than one view of 2D slices,
similar to MP-GAN.
Z-GAN: It is employed for image-to-voxel translation using a conditional generator consisting of an encoder that encodes the 2D image
input and a decoder that generates the volume in a UNet-based format Kniaz et al. (2020). Skip connections between the 2D and 3D
convolutions are made by â€˜â€˜copy inflatingâ€™â€™ the 2D sensors to match
the dimensions of the 3D sensors. The discriminator tries to distinguish
between synthetic and real volumes by using a 3D PatchGAN. Therefore, Z-GAN is based on the Pix2Pix architecture, with the decoder and
discriminator adapted for volumetric data.
SPGAN (Slice to Pores GAN): This architecture is used in Krutko
et al. (2019) to generate 3D volumes of porous media from 2D images
of thin sections. For this, the 2D image is encoded and concatenated
with a random noise vector, which is decoded by a 3D decoder that
generates the volume, i.e. the conditional generator. The discriminator
receives both synthetic and real volumes. This architecture is identical
to the Z-GAN, but no skip connections are used between the encoder
and the decoder, and noise is added to the encoded 2D slice before
feeding the decoder.

3.3. Architectures
3.3.1. Based on vanilla GAN
HingeGAN/LSGAN / WGAN/WGAN-GP: These are objective functions rather than architectures. The HingeGAN uses the hinge loss
to measure the distance between the discriminatorâ€™s output and the
label, i.e. true or false (Appendix A.1.3). The vanilla GAN tries to
solve Eq. (3) using JS divergence (Appendix A.1.1), for this it uses the
BCE (Appendix A.3.3), but the LSGAN replaces it with least squares
loss (Appendix A.1.2). WGAN uses Wasserstein distance instead of JS
divergence, also called earth moverâ€™s distance (Appendix A.1.4). In
the WGAN architecture, the discriminator is called critic because the
output is not a probability of reality, but how real it is, i.e. it is such
as the regular discriminator, but without the sigmoid function, so it
outputs scalar values. The WGAN-GP replaces the weight clipping used
by the WGAN with gradient penalty, as explained in Appendix A.1.4.
Several researchers prefer using the WGAN or WGAN-GP loss function
over the vanilla GAN function, so this is discriminated in Tables 1â€“
3. As explained in Section 3.1.1, no objective function is superior to
the other in every case, but depends mostly on a case-by-case basis.
It should be noted, however, that some papers that have used WGANGP have also tried the vanilla GAN and obtained better results with
the former. However, others mention that no difference in results was
found when using one or the other. Therefore, it is recommended to
read Section 3.1.1 for further recommendations on the best approaches
to generate synthetic volumetric data with GANs.
PGGAN (Progressive Growing GANs): This network attempts to
stabilise the training and increase the realism of the generated data
by gradually increasing the resolution of the output volumes. In a first
step, the generator produces 4 Ã— 4 Ã— 4 volumes from random noise
and the discriminator is required to distinguish between the synthetic
data and the downscaled real data. If the results are satisfactory with
this resolution, the networks are increased to generate and discriminate
8 Ã— 8 Ã— 8 volumes. This process is repeated until the desired resolution
is achieved, which in the case of Zhang et al. (2021) is 64 Ã— 128 Ã— 128.
CryoGAN: This architecture is very similar to the vanilla GAN,
but the generator is a cryo-EM physics simulator instead of a regular
network (Gupta et al., 2021), and the random vector is replaced by a
3D density map from the cryo-EM physics simulator. This approach can
be adapted for different areas where a simulator can replace a regular
generator.
MSG-GAN (Multi-Scale Gradient GAN): This architecture can be
understood as a vanilla GAN with skip connections between the generator and the discriminator, with a convolution layer between these
skip connections. The generator produces multiple scaled samples that
are fed to the discriminator, i.e. the full resolution volume produced
by the last block of the generator is the input of the first block of the
discriminator, then this output is concatenated with the output of the
penultimate block of the generator, and so on. Greminger (2020) claims
that this architecture leads to stable training.
13

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

between real and fake data, and outputs a prediction of the class,
which is also part of the feedback to the generator. The volumes are
refined by a refiner, which consists of four residual blocks after they
have been generated by the generator. As with PGGAN, the number
of layers of the discriminator and generator increases, increasing the
resolution of the volumes. The main difference between PGGAN and
this architecture is the conditional input and that the discriminator is
also a classifier (Muzahid et al., 2021).
VA-GAN (Visual Attribution GAN): The VA-GAN presented by
Baumgartner et al. (2018) consists of a mapping function (ğ‘€(ğ‘¥), UNetbased) that creates a mapping between the differences of two volumes
from two different classes (0 and 1), e.g. MRI brain scans from healthy
patients and from patients with Alzheimerâ€™s disease. Therefore, instead
of using a generator to generate class (1) volumes, this architecture
trains an ğ‘€(â‹…) to generate a map that is concatenated with the input ğ‘¥
(of class 1) to produce the synthetic class 0 image, i.e., ğ‘¦ = ğ‘€(ğ‘¥) + ğ‘¥.
The discriminator must distinguish between the real and the fake class
0 volume.
3D-RecGAN: This architecture is used by Yang et al. (2017) to
reconstruct 3D volumes from 2.5D partial views of the object. It consists
of a conditional generator (UNet-based) and a conditional discriminator, both conditioned by the 2.5D input view. The 2.5D view is a
partial voxel grid, i.e. an incomplete volume that is reconstructed by the
generator. The ground truth volumes are needed so that a voxel-wise
comparison can be performed.
DLGAN (Depth-preserving Latent GAN): This architecture, used
by Liu et al. (2021), is identical to 3D-RecGAN in that it uses a
conditional generator (UNet-based) and a conditional discriminator to
reconstruct 2.5D voxel grids. DLGAN has an additional autoencoder
that learns how to reconstruct the 3D ground truth volume, which
helps the generator learn how to represent the volumes in latent space
(Appendix A.2.1). Since the final volume consists of binary values, a
classification network (consisting of fully connected layers) is used to
binarise the output of the generator instead of using a fixed threshold
to define which values are 1 and which are 0.
ED-GAN (Encoder-Decoder GAN): Wang et al. (2017) use this
architecture for the inpainting task. The ED-GAN architecture is similar
to the 3D RecGAN architecture in that it consists of an encoderâ€“
decoder generator that receives a corrupted 3D volume as input. The
main differences between these two architectures are that the EDGAN discriminator is not conditional, i.e. it only receives the real
and reconstructed volumes and not the damaged 3D volume, and that
ED-GAN has a recurrent long-term convolutional network (LRCN) to
increase the resolution of the generatorâ€™s output. The LRCN consists
of a 3D CNN encoder, a Long Short Term Memory Network (LSTM)
and a 2D Full CNN decoder. The 3D CNN encoder receives sparse
volumes (more than one slice per input) of the reconstructed volume,
the LSTM captures the relationship between the slices, and 2D FullCNN reconstructs the slices, which are then concatenated back to the
volume. By using the LRCN, it is possible to create volumes with higher
resolution even if GPU memory is limited.
Pix2Pix: This architecture is used for image-to-image translations.
It consists of a conditional generator (UNet-based) and a PatchGAN
discriminator. PatchGAN discriminator uses patches and discriminates
each patch instead of the whole volume at once. The main weakness
of Pix2Pix-based architectures is the need for paired data to perform
image translation, whereas CycleGAN does not. Pix2Pix calculates the
difference between the paired images (using a voxel-wise metric),
while CycleGAN calculates the cycle consistency loss (Appendix A.1.5).
Several architectures are Pix2Pix-based, such as Z-GAN.

3.3.3. Based on conditional GAN
The cGAN allows the generation of synthetic data with specific userdefined features, e.g. the generation of different classes, defining the
position and size of the synthetic tumour, among others. The vanilla
cGAN is formally defined by Eq. (4)
ğ‘šğ‘–ğ‘›ğº ğ‘šğ‘ğ‘¥ğ· ğ‘‰ (ğ·, ğº) = Eğ‘¥âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘¥) [ğ‘™ğ‘œğ‘”(ğ·(ğ‘¥|ğ‘¦))]
+ Eğ‘§âˆ¼ğ‘ğ‘§ (ğ‘§) [ğ‘™ğ‘œğ‘”(1 âˆ’ ğ·(ğº(ğ‘§|ğ‘¦)))]

(4)

where ğ‘¦ is the condition. Typically, both the generator and the discriminator have access to the condition, but some approaches use the
condition only in the generator, as can be seen in the subsequent
architectures. However, the use of the condition in the discriminator
is also important because it helps the discriminator to recognise distinguishing features between different conditions, which leads to better
feedback and thus to realistic results. When the generator input is a
volume and not a random vector, the generator is usually based on an
autoencoder/U-Net.
ICW-GAN (Improved Conditional Wasserstein GAN): This architecture is a conditional version of the WGAN-GP. The generator
and discriminator are conditioned by the labels, i.e. the labels are
concatenated with the input of the generator and the penultimate layer
of the discriminator (Zhuang and Schwing, 2019).
MCGAN (Multi-Conditional GAN): This architecture uses multiple
conditions to guide the generator to produce synthetic data with specific features. Han et al. (2019a), Xu et al. (2019) generate CT lung
tumours using the background volume as input to an autoencoderbased generator. Conditions can be concatenated with the input background volume (Han et al., 2019a) or in the middle of the generator
network (Xu et al., 2019). These conditions are also passed to the
discriminator to make easier the distinction between real and fake
samples, and to force the generator to produce samples with these
specific characteristics. Xu et al. (2019) also add an encoder to predict
the characteristics of the synthetically generated volumes to ensure that
the generator produces the volumes with the correct features.
BMGAN (Bidirectional Mapping GAN): BMGAN consists of a conditional generator, a discriminator and an encoder. Hu et al. (2022)
uses this network to generate PET from MRI scans and encode the
PET scans back into the latent space. This approach is particularly
interesting because using an MRI scan with the random latent vector
gives the network more variability and robustness. This architecture
is similar to the ğ›¼-GAN proposed by Rosca et al. (2017), but they
treat the distribution as implicit, whereas BMGAN assumes a standard
normal distribution. BMGAN is a cGAN, i.e. it receives as input not
only a random vector (ğ‘§) but also the MRI scan, and uses KL divergence (Appendix A.1.1) to bring the coded vector distribution close
to the random distribution. Kwon et al. (2019) also uses the stability
of the ğ›¼-GAN architecture, but replaces the KL divergence with the
Wasserstein distance, achieving better stability in a 3D space. This
architecture prevents mode collapse, which is advantageous for training
with volumetric data, but also requires more computing power, since
four networks are trained instead of only two (Ferreira et al., 2022b).
CAE-ACGAN (Conditional Auto-Encoding, Auxiliary Classifier
GAN): This consists of a variational autoencoder (the generator), a
discriminator and a classifier. It is identical to the BMGAN architecture,
but instead of an encoder that encodes the synthetic data back into
an encoded vector, CAE-ACGAN has a classifier that classifies the
synthetic data into the appropriate class. The generator is tuned based
on whether the classifier is able to classify the synthetic data correctly
or not. Yang et al. (2021b) uses this architecture to perform image
translation from CT to multicontrast MRI instead of using a CycleGAN
architecture. A comparison is made between their approach, WGANGP and Pix2Pix, showing that CAE-ACGAN gives better results, but
CycleGAN should also have been tested.
Progressive Conditional GAN: This architecture consists of a conditional generator that is conditioned by a one-hot coder attached to the
random noise vector. The discriminator is a classifier that distinguishes

3.3.4. CycleGAN based
The CycleGAN is a specific architecture based on the cGAN, which
is often used for image-to-image translation tasks. It consists of four
networks, i.e. two generators and two discriminators. It is very similar
to Pix2Pix as it learns to map images from one modality to another and
14

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

CT-SGAN (CT Synthesis GAN): It is composed of 4 subnetworks:
Bidirectional Long/Short-Term Memory (BiLSTM) network; a slice generator (ğºğ‘ ğ‘™ğ‘–ğ‘ğ‘’ ); a slice discriminator (ğ·ğ‘ ğ‘™ğ‘–ğ‘ğ‘’ ) and a slab discriminator
(ğ·ğ‘ ğ‘™ğ‘ğ‘ ). The ğºğ‘ ğ‘™ğ‘–ğ‘ğ‘’ is able to generate a volume by creating multiple
slices controlled by the BiLSTM network. The ğ·ğ‘ ğ‘™ğ‘ğ‘ is able to learn the
anatomical distribution across the slices of the volumetric CT scans and
provide volumetric feedback (using 3D convolutions), while the ğ·ğ‘ ğ‘™ğ‘–ğ‘ğ‘’
only provides feedback about each slice (Pesaranghader et al., 2021).

vice versa, but it does not require a paired dataset. Fig. 3 shows a basic
illustration of a CycleGAN applied to modality translation of T1 and T2
from MRI brain scans. The CycleGAN is formally defined by the vanilla
GAN loss function (Eq. (3), for each pair generator/discriminator) and
the cycle consistency loss (Eq. (A.11)). It is mainly used for image
translation, but has also been applied for segmentation (SpCycleGAN)
and reconstruction (Yang et al., 2021a).
SpCycleGAN (spatially constrained CycleGAN): This architecture
is essentially based on the CycleGAN, but a spatial constraint is added
to control the intermediate step (Fu et al., 2018), as explained in
Appendix A.3.9.
Dense CycleGAN and Res-cycle GAN (residual block cycleconsistent GAN): The architectures of the dense CycleGAN and the
Res-cycle GAN are very similar to the vanilla CycleGAN, but in the first
dense blocks are added in the middle layers for better contextual and
structural feature extraction, i.e. high and low level features, and in
the second residual blocks are used for the same purpose. These GANs
are described by Liu et al. (2020b), Harms et al. (2019), respectively.
Looking at the two schematic flowcharts, the dense block is identical
to the residual block, but in a dense block all the outputs of the
previous layers are concatenated in the next layers, and the residual
block concatenates the summation of the features, i.e. the dense block
has a denser connection between the layers (Huang et al., 2017).
Dual3D&PatchGAN: It is similar to the CycleGAN architecture
and can also be used for image translation without the need for
paired datasets (Gu and Zheng, 2021). The main difference between
Dual3D&PatchGAN and CycleGAN is that in the first case both generators are updated simultaneously with the same loss value, so that the
weights are shared by both networks. PatchGAN discriminator is used
for memory efficiency.
FGAN (Feature-consistent GAN): It is based on the CycleGAN
architecture, but a disease-specific neural network (DSNN) is used to
extract features from the real and synthetic data, and the difference between these features is calculated. This is called the feature-consistent
component (Appendix A.2.2). The GAN training is thus composed of a
CycleGAN and a DSNN (Pan et al., 2019).
hGAN (hybrid GAN): This architecture is composed by a 3D generator and a 2D discriminator. The generator follows an autoencoder
structure. This network is weakly supervised by the scans generated
by a 2D-CycleGAN, i.e. the volumetric output of the generator is
decomposed into 2D images and the MAE between them and the
images generated by a trained 2D-CycleGAN is calculated. Using a 3D
generator allows for better modelling of spatial information and solves
the discontinuity between layers that occurs when layers are generated
individually, and using a 2D discriminator is well suited for situations
where limited data is available (Zeng and Zheng, 2019).
RevGAN (Reversible GAN): RevGAN is based on the CycleGAN
architecture, but uses only one reversible generator and two discriminators to perform the image conversion instead of two generators and
two discriminators (Lin et al., 2021a). This is only possible thanks to
the reversible core of the generator, which consists of several reversible
blocks (Gomez et al., 2017). This architecture is more memory-efficient
than the vanilla CycleGAN, since three networks are trained instead of
four.

da-GAN (difficulty-aware attention GAN): It consists of two discriminators with difficulty-aware attention mechanism. The first discriminator classifies the entire volume as real or fake, but the second
discriminator is specialised in a particular part of the volume that is
crucial. This architecture is presented in Ma et al. (2020) for generating
2D data, but the concept can also be applied to volumetric data. Ma
et al. (2020) uses a global discriminator for the whole brain and a local
discriminator to enhance the hippocampus.

3.4. 3D volumetric data generation

In this section, it is possible to find all volumetric papers in Tables 1â€“3. Table 1 contain all papers where CT or MRI scans are used,
sorted by modality and then by year. If a dataset has no link, this means
that it is not available or has not been found. The footnotes of the
studies are the links to the available code/framework.
List 1: List of abbreviations of Table 1:
â€¢ AFP â€” Average False Positives;
â€¢ AIBL â€” Australian Imaging Biomarkers and Lifestyle;
â€¢ C3D â€” Convolutional 3D;
â€¢ CBF â€” Cerebral Blood Flows;
â€¢ cGANe â€” constrained GAN ensembles;
â€¢ CPM â€” Competition Performance Metric;
â€¢ CTP â€” CT Perfusion;
â€¢ CT-SGAN â€” CT Synthesis GAN;
â€¢ da-GAN â€” difficulty-aware attention GAN;
â€¢ IBC â€” Individual Brain Charting;
â€¢ ICW-GAN â€” Improved Conditional Wasserstein GAN;
â€¢ IS â€” Inception Score;
â€¢ JS â€” Jensenâ€“Shannon loss;
â€¢ Li-ion battery â€” Lithium-ion battery;
â€¢ MCGAN â€” Multi-Conditional GAN;
â€¢ micro-CT â€” micro Computed-Tomography;
â€¢ LUNA â€” LUng Nodule Analysis;
â€¢ NLST â€” National Lung Screening Trial;
â€¢ Noise SD â€” background noise levels;
â€¢ OCT â€” Optical Coherence Tomography;
â€¢ ODT â€” Optical coherence Doppler Tomography;
â€¢ ROC â€” Receiver Operating Characteristic curve;
â€¢ SIS â€” Semantic Interpretability Score;
â€¢ SOFC â€” Solid Oxide Fuel Cell;
â€¢ SPGAN â€” Slice to Pores GAN;
â€¢ SSA â€” Specific Surface Area;
â€¢ TPCF â€” Two-Point Correlation Function;
â€¢ t-SNE â€” t-distributed Stochastic Neighbour Embedding;
â€¢ VA-GAN â€” Visual Attribution GAN;
â€¢ XCT â€” X-ray Computed Tomography;

3.3.5. Double discriminator
Although this approach has not been extensively researched, it can
provide strong results for a variety of tasks. The use of two discriminators can be advantageous when it is crucial to focus on specific
parts of the generated volume, but also on the whole volume. For
example, when generating MRI scans of the brain with the aim of
using them to segment brain tumours, it might be useful to use a
specific discriminator to distinguish only that part of the brain. Such
an approach is therefore recommended if a specific feature is essential
for the realism of the generated data or for the downstream task.

Table 2 presents a summary of all multimodal papers where CT,
MRI, PET and/or CBCT were used. The papers (Zhang et al., 2019c,
2018; Cai et al., 2019) are interconnected and should therefore be read
together for better understanding.
15

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Table 1
Compact overview of reviewed studies with modalities CT and MRI, sorted by modality and then by year of publication, indicating the modality, and the datasets used with the
respective localisation, if available. It also shows the network used, the loss functions, the evaluation metrics, the resolution of the synthetic data generated, and whether or not
it is a medical application. The abbreviations used in this table can be found in List 1 or in section 1.1.3 (Acronyms and Abbreviations). â€˜â€˜Modal.â€™â€™ stands for Modality.
Study

Modal.

Med

Dataset

Network (G/D)

Loss function

Metrics

Comments

CT

Yes

LUNA16a

cGAN
(3D-UNet/3D
CNN)

Adv

VTT, (Sen, CPM)

32 Ã— 32 Ã— 32

Pesaranghader et al.
(2021)

CT

Yes

NLSTb , LIDCc

CT-SGAN

JS, WGAN-GP

Visual, FID, IS,
(Acc)

> =
224 Ã— 224 Ã— 224
(stacks
224 Ã— 224 Ã— 3)

Zhang et al.
(2021)

CT

Yes

120 intact
prostate cancer
patients

PGGAN

Adv, CE

HU (DSC,
Average HD,
Average Surface
HD)

64 Ã— 128 Ã— 128

Chen et al.
(2020)

CT

No

Berea Sandstone
ct scan

WGAN with two
discriminators

WGAN,
Minkowski
functional

Minkowski
functional

64 Ã— 64 Ã— 64

GayonLombardo
et al. (2020)d

CT (XCT)

No

Li-ion battery
cathodee , SOFC
anodef

DCGAN

Adv

Phase volume
fraction, SSA,
TPB, TPCF,
Relative
diffusivity

64 Ã— 64 Ã— 64

Liu et al.
(2019)

CT

No

Berea
sandstoneg ,
Estaillades
carbonateh

DCGAN

Adv

Visual

64 Ã— 64 Ã— 64,
128 Ã— 128 Ã— 128

Krutko et al.
(2019)

CT

No

Sandstonei

SPGAN (3D
DCGAN based)

Adv

Minkowski
functional,
Absolute
permeability

64 Ã— 64 Ã— 64

Moghari
et al. (2019)

CT (CTP)

Yes

18 acute stroke
patients

cGAN

Adv

PSNR, NMSE,
SSIM

64 Ã— 64 Ã— 64

Han et al.
(2019a)

CT

Yes

LIDCc

MCGAN
(U-Net/Pix2Pix)

LSGAN,
WGAN-GP

VTT, t-SNE,
(CPM)

32 Ã— 32 Ã— 32

Xu et al.
(2019)

CT

Yes

LIDCc

MCGAN

LSGAN, MAE,
MSE

Visual

64 Ã— 64 Ã— 64

Mosser et al.
(2017)j

Micro-CT

No

Spherical Bead
pack, Berea
sandstone,
oolitic, Ketton
limestonek

DCGAN

Adv

Minkowski
functional

64 Ã— 64 Ã— 64

Dikici et al.
(2021)

MRI

Yes

217
post-gadolinium
T1-weighted

cGANe (DCGAN
based)

Adv

FID, (AFP)

16 Ã— 16 Ã— 16

Momeni
et al. (2021)

MRI

Yes

AIBLl , MICCAI
Valdo 2021m

conditional
LesionGAN
(cGAN based)

Adv, Volume,
Border

(AUC, Sen, ROC)

11 Ã— 11 Ã— 11

Rusak et al.
(2020)

MRI

Yes

ADNIn

GAN (Pix2Pix
based)

Adv, MAE

PSNR, SSIM,
MAE, MSE, DSC

181 Ã— 218 Ã— 181

Jung et al.
(2020)

MRI

Yes

ADNIn

CycleGAN
(cGAN based)

WGAN-GP, CE,
Cycle
consistency

FID, (Acc)

192 Ã— 192 Ã— (3, 6, 8)
stacks
192 Ã— 192 Ã— 1

Ma et al.
(2020)

MRI

Yes

KulagaYoskovitzo

da-GAN

Adv, Gradient
Difference,
Frequency
domain, MAE

Visual, PSNR,
SSIM, SIS, (DSC)

384 Ã— 512 Ã— 384
(stacking 512
imgs of
384 Ã— 384)

Zhuang and
Schwing
(2019)

MRI

Yes

OpenfMRIp , IBCq

ICW-GAN

WGAN-GP

(Acc, Macro F1,
Pre, Sen)

53 Ã— 63 Ã— 46

Bu et al.
(2021)

(continued on next page)

16

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.
Table 1 (continued).

a
b
c

Study

Modal.

Med

Dataset

Network (G/D)

Loss function

Metrics

Comments

Baumgartner
et al. (2018)r

MRI

Yes

ADNIn

VA-GAN (WGAN
based 3D
U-Net/C3D)

WGAN-GP, MAE

Visual, NCC

128 Ã— 160 Ã— 112

Li et al.
(2020)

OCT

Yes

3D ODT of
mouse CBF

GAN (N/D)

WGAN

(PSNR, DSC,
Noise SD)

64 Ã— 64,
64 Ã— 64 Ã— 64

LUNA16
NLST
LIDC

d

Poresforthought
Li-ionbatterycathode
f
SOFCanode
g
Bereasandstone
h Estailladescarbonate(wronginthepaper)
i PorousMediaGAN
j Micro-CTImagesandNetworks
e

k

AIBL
MICCAIValdo2021
m
Low-permeable sandstone sample from a hydrocarbon reservoir located in Russia rotationally scanned with an X-ray L240 GE system
n
ADNI
o Kulaga-Yoskovitz
p OpenfMRI
q IBC
l

r

VA-GAN

â€¢ PSF â€” Point Spread Functions;
â€¢ RecGAN â€” Reconstruction GAN;
â€¢ SpCycleGAN â€” Spatially Constrained CycleGAN;
â€¢ TPB â€” Triple Phase Boundary;
â€¢ Voxel/ PC â€” Voxelized point clouds;

List 2: List of abbreviations of Table 2:
â€¢ AC â€” Attenuation Correction;
â€¢ BMGAN â€” Bidirectional Mapping GAN;
â€¢ CAE-ACGAN â€” Conditional Auto-Encoding, Auxiliary Classifier
GAN;
â€¢ CMD â€” Centre of Mass Distance;
â€¢ GMD â€” Gradient Magnitude Distance;
â€¢ FGAN â€” Feature-consistent GAN;
â€¢ hGAN â€” hybrid GAN (3D Generator and 2D Discriminator);
â€¢ MSD â€” Mean Surface Distance;
â€¢ MS-SSIM â€” Multi-Scale Structural Similarity Index Measure;
â€¢ PCC â€” Pearson Correlation Coefficient;
â€¢ PVD â€” Percentage Volume Difference;
â€¢ res-cycle GAN â€” residual block cycle-consistent GAN;
â€¢ RevGAN â€” Reversible GAN;
â€¢ RMSD â€” Residual Mean Square Distance;
â€¢ SNU â€” Spatial Non-Uniformity;
â€¢ S-score â€” Shape-score;

3.5. Visual results
As explained in Appendix B.1.8, visual judgement is currently the
most widely used evaluation tool and one of the best for assessing
the images produced by GANs. Therefore, almost every publication
presents some real and synthetic images that allow the reader to check
the realism of the synthetic data. However, in some cases, it is very
difficult to assess this realism because the 2D rendering processes of
the paper can change the colour, resolution, and other properties of the
images, e.g. MRI or CT scans. Some authors make the trained generator
or some synthetic data freely available, but others do not, making it
difficult for readers to perform visual assessments.
Fortunately, it is usually possible to check the improvements only
on the basis of the images provided, as the authors are aware of these
limitations. In this section, we present some selected images from some
reviewed papers, as they can best reflect the realism of the generated
synthetic data. We have selected images related to different organs
(medical domain) and different structures (non-medical domain).
Fig. 14 shows the cGANe system developed by Dikici et al. (2021).
They offer a novel protocol for sharing data by generating synthetic
data from the original dataset, validating it and making it available at
client sites. This inherent anonymisation capacity of GANs was already
explored by Shin et al. (2018).
The error images are used in Fig. 15 to help the readers identify the
differences between the ground truth, a scan generated by the proposed
method, and two others generated by a 3D Fully Convolutional Neural
network and a vanilla GAN. The whiter the error images are, the more
similar the synthetic and the original images. In image translation with
paired images, this comparison is possible because ground truth is
available. However, some authors choose not to use these error images,
as can be seen in Figs. 16 and 17. In some situations, it is not possible to
create them, e.g., when ground truth is not available (Zeng and Zheng,
2019).

Table 3 contains the papers on the other modalities, such as CAD,
RGB-D, FIB-SEM, microscopy, seismic and synthetic. The papers (Ho
et al., 2019; Han et al., 2019b; Chen et al., 2021b) have some authors in
common and complement each other. Note that Muzahid et al. (2021)
give the same name to their GAN as the point cloud GAN, i.e. PC-GAN,
but these networks are different, as voxels and not point clouds are
used.
List 3: List of abbreviations of Table 3:
â€¢ CC â€” Correlation Coefficient;
â€¢ cryo-EM â€” cryo-Electron Microscopy;
â€¢ diSPIM â€” dual inverted Selective Plane Illumination Microscope;
â€¢ DLGAN â€” Depth-preserving Latent GAN;
â€¢ DPB â€” Double Phase Boundary;
â€¢ FIB â€” Focused Ion Beam;
â€¢ KPFM â€” Kelvin Probe Force Microscopy;
â€¢ LSFM â€” Light Sheet fluorescence Microscopy;
â€¢ MAPE â€” Mean Absolute Percentage Error;
â€¢ MP-GAN â€” Multi-Projection GAN;
â€¢ MSG-GAN â€” Multi-Scale Gradient GAN;
â€¢ NRMSE â€” Normalised Root Mean Square Error;
17

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Table 2
Compact overview of reviewed studies with multimodalities CT, MRI, PET and/or CBCT, sorted by modality and then by year of publication, indicating the modality, and the
datasets used with the respective localisation, if available. It also shows the network used, the loss functions, the evaluation metrics, the resolution of the synthetic data generated.
All studies are medical applications. The abbreviations used in this table can be found in List 3 or in section 1.1 (Acronyms and Abbreviations). â€˜â€˜Modal.â€™â€™ stands for Modality.
Study

Modal.

Dataset

Network (G/D)

Loss function

Metrics

Comments

CT, CBCT

27 advanced
lung cancer

GAN (ResidualUNet/CNN)

Adv

MAE

N/D

Wei et al.
(2020)

CT, CBCT

4D-CT and raw
CBCT projections

cGAN

Adv, MAE

Acc, (Tumour
localisation
error)

128 Ã— 128 Ã—
32

Harms et al.
(2019)

CT, CBCT

24 brain and 20
pelvic CT/CBCT

res-cycle GAN

Adv, Cycle
consistency,
L1.5, GMD

MAE, PSNR,
NCC, SNU

96 Ã— 96 Ã— 5
(stacked to
produce 3D)

Lei et al.
(2020b)

CT, PET

16 CT/PET

CycleGAN

Adv, MSE

Visual, MAE,
NMSE, NCC,
PSNR

64 Ã— 64 Ã— 64

Gu and
Zheng (2021)

MRI, CT

BrainWeba

Dual3D&PatchGAN
(3DGAN)

Adv

SSIM, PSNR

N/D

Yang et al.
(2021b)

MRI, CT

9 healthy MRIb

CAE-ACGAN

Adv, MAE

Visual, PSNR,
SSIM, MAE

128 Ã— 128 Ã— 32

Liu et al.
(2020b)

MRI, CT

21 cancer
hepatocellular

dense CycleGAN

Adv, L1.5,
Gradient
Difference

MAE, PSNR,
NCC, HU

64 Ã— 64 Ã— 64

Lei et al.
(2020a)

MRI, CT

45 prostate
cancer MRI/CT

CycleGAN

Adv, Cycle
consistency

(DSC, Sen, Spe,
HD, MSD,
RMSD, CMD,
PVD)

64 Ã— 64 Ã— 64

Yang et al.
(2019)

MRI, CT

N/D

N/D (3D-UNet/3D
CNN)

Adv, MSE

PSNR, MSE

N/D

Zeng and
Zheng (2019)

MRI, CT

50 subjects
MRI/CT

hGAN (CycleGAN
based)

Adv, MAE

Visual, MAE,
PSNR

256 Ã— 288 Ã— 32,
256 Ã— 288 Ã— 12

Zhang et al.
(2019c)

MRI, CT

4496
cardiovascular
MRI/CT

CycleGAN

Adv, Cycle
and Shape
consistency

Visual, S-score,
(DSC)

112 Ã— 112 Ã— 86

Zhang et al.
(2018)

MRI, CT

4354 contrasted
cardiac CT scans

CycleGAN

Adv, Cycle
and Shape
consistency

Visual, S-score,
(DSC)

86 Ã— 112 Ã— 112

Schaefferkoetter et al.
(2021)

MRI, CT, PET

60 patients

CycleGAN
(Residual-UNet/
patchGAN)

MAE, Cycle
consistency,
Identity, MSE

(MSE, PCC, AC)

96 Ã— 96 Ã— 96

Cai et al.
(2019)

MRI, CT,
X-rays

Several datasetsc

CycleGAN (cGAN/
PatchGAN)

LSGAN, Cycle
and Shape
consistency

Visual, S-score,
(DSC)

80 Ã— 128 Ã— 128

Hu et al.
(2022)

MRI, PET

ADNId

BMGAN(DenseUNet/Patch-Level)

LSGAN, KLdivergence
constraint,
MAE,
Perceptual
(VGG-16e )

MAE, PSNR,
MS-SSIM, FID

128 Ã— 128 Ã— 128

Lin et al.
(2021a)f

MRI, PET

ADNId

RevGAN

Adv, Cycle
consistency

SSIM, PSNR

96 Ã— 96 Ã— 48

Pan et al.
(2019)

MRI, PET

ADNId

FGAN

Adv, FeatureConsistent

MAE, PSNR,
SSIM, (AUC)

N/D

Yan et al.
(2018)

MRI, PET

ADNId

cGAN

Adv, MAE

Visual, SSIM,
(Acc, AUC)

160 Ã— 160 Ã— 96

Qin et al.
(2021)

a

BrainWeb
Dataandcodeavailableuponrequest
c
4354/142contrastedcardiacCT/MRI, 82/78pancreaticabdomenCT/MRIscans, mammography X-rays (BCDR, INbreast)
d
ADNI
e Hu et al. (2022)
f Codeavailableuponrequest
b

If the generation of synthetic data is only an intermediate step,
this visual analysis is not very important if it is successful in its purpose. Han et al. (2019a) improved the nodule detection algorithm by
using synthetic data. Fig. 17 shows the results of generating synthetic
nodules and inserting them into surrounding tissue with and without
using the L1 loss. In this case, it is possible to see the differences
between the two approaches.

Lei et al. (2020a) trained a CycleGAN-based architecture to perform
image translation from CT to MRI to achieve better soft tissue contrast. Again, the focus of this work is not on the realism of synthetic
MRIs, but on improving segmentation when synthetic data are available. The same authors published a work similar Dong et al. (2019),
where in Fig. 18 it can be seen that the segmentation performed with
18

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Table 3
Compact overview of reviewed studies with other modalities, sorted by modality and then by year of publication, indicating the modality, and the datasets used with the respective
localisation, if available. It also shows the network used, the loss functions, the evaluation metrics, the resolution of the synthetic data generated, and whether or not it is a
medical application. The abbreviations used in this table can be found in List 4 or in section 1.1 (Acronyms and Abbreviations)
Study

Modality

Med

Dataset

Network (G/D)

Loss function

Metrics

Comments

CAD

No

ModelNet10/40a

Progressive
Conditional GAN
(PGGAN/cGAN
based)

Adv

Visual,
Clustering, (Acc)

32 Ã— 32 Ã— 32

Greminger
(2020)

CAD

No

Synthetic data
generated by
CadQueryb

MSG-GAN

Hinge

Compliance,
Fraction of
Unmachinable
Voxels

32 Ã— 32 Ã— 32

Kniaz et al.
(2020)c

CAD

No

SyntheticVoxelsd ,
VoxelCitye

Z-GAN (pix2pix
based)

Adv, MAE

IoU

128 Ã— 128 Ã— 128,
Voxel/ PC

Li et al.
(2019)

CAD

No

ShapeNetf , Pix3Dg ,
Stanford carh ,
CUB-Birds-200â€“2011i

MP-GAN

Adv

FID, Acc

64 Ã— 64 Ã— 64

Yang et al.
(2017)j

CAD

No

ModelNet40a

3D-RecGAN

Modified CE,
WGAN-GP

IoU, CE

64 Ã— 64 Ã— 64,
Voxel/ PC

Wang et al.
(2017)k

CAD, RGB-D

No

Real-world scansl ,
ModelNet10/40a

ED-GAN (cGAN
based)

Adv, CE, (MAE)

(CE, Acc)

32 Ã— 32 Ã— 32,
128 Ã— 128 Ã— 128

Sciazko et al.
(2021)

FIB-SEM

No

Ni70GDC30,
Ni30GDC70,
Ni70GDC30 (200 MPa),
Ni30GDC70 (200 MPa)

GAN2Dto3D

Adv

Average volume
fractions,
Volume fraction
variations,
TPB/DPB
densities

64 Ã— 64 Ã— 64

Gupta et al.
(2021)m

Microscopy
(cryo-EM)

Yes

EMPIAR-10061n ,
EMPIAR-10028o

CryoGAN
(cryo-EM physics
simulator)

WGAN-GP

Reconstruction
resolution (Ã…)

180 Ã— 180 Ã— 180

Chen et al.
(2021b)

Microscopy

Yes

Human kidney

SpCycleGAN

Adv, Cycle
consistency,
Spatial

(DSC,
Type-I/Type-II
error, Acc, IoU,
Pre, Sen, F1)

128 Ã— 128 (Ã—128)
Pos-processing

Tang et al.
(2020)

Microscopy

Yes

Janelia-Fly/Tokyo-Flyp

cGAN

Adv, MAE

PSNR, SSIM

128 Ã— 128 Ã— 32

Ho et al.
(2019), Fu
et al. (2018)

Microscopy
(Fluorescence,
two-photon)

Yes

Rat kidney

SpCycleGAN

Adv, Cycle
consistency,
Spatial

(Pre, Sen, F1)

128 Ã— 128 Ã— 128

Han et al.
(2019b)

Microscopy
(Fluorescence,
two-photon)

Yes

Rat kidney

SpCycleGAN

Adv, Cycle
consistency,
Spatial, (MSE)

(MAPE, F1, Sen,
Pre)

128 Ã— 128 Ã— 128,
128 Ã— 128 Ã— 32,
64 Ã— 64 Ã— 64

Baniukiewicz
et al. (2019)q

Microscopy
(diSPIM light
sheet, LSFM,
Confocal
microscope)

Yes

Cells (several sourcesr )

cGAN

Adv, MAE

Visual, (F1, Sen,
Pre)

256 Ã— 256 Ã— 66
(stack of 66
images 256 Ã— 256)

Liu et al.
(2021)

RGB-D

No

ModelNet40a ,
KinectDatas

DLGAN
(ED-GAN)

Adv, Latent
vector, Depth

IoU, CE

64 Ã— 64 Ã— 64

Liu et al.
(2020a)

Seismic
reflection
data

No

Synthetic model, F3
block seismic datat

semi-supervised
GANs

Adv, MSE, (CE)

(Acc, F1)

64 Ã— 64 Ã— 64

Kench and
Cooper
(2021)u

Synthetic,
x-ray, KPFM,
SEM

No

Several materialsv

SliceGAN

WGAN

Visual, Volume
fraction, Relative
surface area,
Relative
diffusivity

64 Ã— 64 Ã— 64

Yang et al.
(2021a)w

Synthetic
(Data
simulator,
PSFx )

Yes

Synthetic isotropic
quad-view embryo and
C. elegansy

CycleGAN based
(CNN
based/multiscale)

LSGAN, Cycle
consistency, MSE

NRMSE, PSNR,
SSIM, CC

64 Ã— 64 Ã— 64

Nozawa et al.
(2021)

Synthetic

No

3D car meshesz ,
Contour sketches

cGAN

Adv, MAE, CE

Visual

64 Ã— 64xViews
(reconstruction
using several
views), Voxel/
PC

Muzahid
et al. (2021)

(continued on next page)

19

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.
Table 3 (continued).
Study

Modality

Med

Dataset

Network (G/D)

Loss function

Metrics

Comments

Shen et al.
(2021)

Synthetic

No

653 3D
Strand-level
modelsaa

WGAN-GP

WGAN-GP,
Content, Style,
Laplacian,
Projection,
Orientation

Visual, MSE

128 Ã— 128 Ã— 96

Danu et al.
(2019)

Synthetic

Yes

2D/3D
vessel-like
structures, 3D
stenosed blood
vessel segments

GAN (N/D)

Adv

Visual

128 Ã— 128,
64 Ã— 64 Ã— 64,
128 Ã— 32 Ã— 32,
Meshes /Voxel

Halpert
(2019)

Synthetic
(Earth
Model)

No

SEAM Phase I
synthetic
modelab

DCGAN

Adv

Visual

32 Ã— 32 Ã— 32

a

ModelNet10/40
CadQuery
c
Z-GAN
d SyntheticVoxels
e VoxelCity
b

f

ShapeNet
Pix3D
h
Stanfordcar
i
CUB-Birds-200--2011
j 3D-RecGAN
k 3DShapeCompletion
l Real-worldscans
g

m

CryoGAN
EMPIAR-10061
o
EMPIAR-10028
p
Janelia-Fly/Tokyo-Fly
q pix2pix_3D_multichannel
r Sources: here, here, here
s KinectData
n

t

F3blockseismicdata
SliceGAN
v
SeetheirTable2
w
MultiViewFusion
x PSF
y Syntheticisotropicquad-viewembryoandC.elegans
z 3Dcarmeshes
u

aa
ab

Strand-level models: here, and here
SEAMPhaseIsyntheticmodel

4. Discussion and conclusion

synthetic data outperforms the other two when compared to manual
segmentation (a3).
The CycleGAN architecture is so versatile that it can also be used for
image translation in small structures such as the heart, pancreas, and
mammograms, as seen in Fig. 16 by Cai et al. (2019). It can even be
used for translating whole body images, as shown in Fig. 19 developed
by Lei et al. (2020b).
Still in the medical field, GANs have been used successfully by Danu
et al. (2019) to generate stenosed segments of coronary arteries and
by Gupta et al. (2021) to generate reconstructions of the 80S Ribosome,
as seen in Figs. 20 and 21. This last work differs from the others because
a CryoEM physics simulator was used as a generator instead of a deep
learning architecture.
The use of GANs to generate 3D data is not limited to medical imaging, as explained earlier. They can also be used to generate synthetic
porous media or 3D car shapes from sketches, as shown in Figs. 22
and 23, or even various everyday objects (Muzahid et al., 2021). These
works demonstrate the wide versatility of GAN architectures.
It would be beneficial to the paper if in addition to the error images,
the authors also represented the evolution of training, as made by Liu
et al. (2019) and shown in Fig. 23. It is known that the GANs training
is very unstable and during training the output may deteriorate, but if
no training breakdown occurs, such as mode collapse, the discriminator
will eventually improve the output of the generator.

Using GANs to generate volumetric data has become a highly researched topic, although it was not explored much in the first two years
of the existence of GANs. This review has shown that GANs are capable
of generating synthetic data that can be used for various medical and
non-medical purposes. The generation of volumetric data is of great
importance for the medical field, as volumetric medical imaging such as
CT, MRI, and PET are increasingly being used, and the need for suitable
pipelines to process these data is growing.
GANs have already been used for tasks such as classification, segmentation or even reconstruction, both for medical and non-medical
fields. This overview contains lists of abbreviations/acronyms, common
loss functions, evaluation metrics, applications and architectures. It
is important to have this overview because of the large number of
publications, the variability between papers and terms, and their major
impact on the generation of synthetic data with GANs. When available,
implementation code is also provided so that new researchers can build
their pipeline more quickly.
4.1. Other papers
The authors are aware of a few relevant papers that were not
covered by the systematic review search. However, they are worth
mentioning because of their novel results.
20

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 14. Constrained GAN ensemble for data sharing (Dikici et al., 2021). Figure reproduced from (Dikici et al., 2021). Publication link.

Fig. 15. Original axial CT images and synthetic CT images generated by dense-cycle-GAN (DCG) developed by Liu et al. (2020b), 3D Fully Convolutional Neural network (FCN) and
vanilla GAN. The second and last rows are the difference image between the original CT and the generated CT images. Reprinted from Medical Imaging 2020: Image Processing,
Vol. 11313, authors Yingzi Liu and Yang Lei and Tonghe Wang and Jun Zhou and Liyong Lin and Tian Liu and Pretesh Patel and Walter J Curran and Lei Ren and Xiaofeng Yang,
â€˜â€˜Liver synthetic CT generation based on a dense-CycleGAN for MRI-only treatment planningâ€™â€™, pages 659â€“664, Copyright (2020), with permission from Society of Photo-Optical
Instrumentation Engineers (SPIE) and one of the authors. Publication link.

Ferreira et al. (2022a) and Ferreira et al. (2022b) developed a
pipeline for generating synthetic rat brain MRIs by relying on the ğ›¼GAN architecture (Rosca et al., 2017; Kwon et al., 2019), but with
two different goals in mind. The first work addressed the need of large
datasets to train deep learning models and a solution to this challenge,

i.e. the use of GANs. They proved that it is possible to use synthetic
data generated by GANs to improve existing segmentation algorithms.
The second paper went even further and proved that synthetic data
can outperform the use of conventional data augmentation and showed
that in this particular case conventional data augmentation is actually
21

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 16. On the left side three orthogonal sections through the centre of the 3D cardiac volumes are shown. The top right displays the pancreatic volumes of the maximum
cross-sectional area. The bottom right shows real and synthetic mammographic lesion patches (Cai et al., 2019). Reprinted from Medical Image Analysis, Vol. 52, authors Jinzheng
Cai and Zizhao Zhang and Lei Cui and Yefeng Zheng and Lin Yang, â€˜â€˜Towards cross-modal organ translation and segmentation: A cycle- and shape-consistent generative adversarial
networkâ€™â€™, pages 174â€“184, Copyright (2018), with permission from Elsevier B.V. Publication link.

Fig. 17. 2D axial view of: First row â€” Real nodules; Second row â€” Lung nodules replaced by noise of dimension 32 Ã— 32 Ã— 32; Third row â€” Synthetic nodules with surrounding
tissue; Fourth row â€” Synthetic nodules using L1 loss with surrounding tissue (Han et al., 2019a). Â© 2019 IEEE. Reprinted, with permission, from 2019 International Conference
on 3D Vision (3DV), authors Changhee Han and Yoshiro Kitamura and Akira Kudo and Akimichi Ichinose and Leonardo Rundo and Yujiro Furukawa and Kazuki Umemoto and
Yuanzhong Li and Hideki Nakayama, â€˜â€˜Synthesizing Diverse Lung Nodules Wherever Massively: 3D Multi-Conditional GAN-Based CT Image Augmentation for Object Detectionâ€™â€™,
pages 729â€“737. Publication link.

suboptimal. As far as we know, these are the first papers in which
synthetic MRI scans of rat brains have been generated. This work is
the evidence that GANs can also be used in preclinical studies.

Sun et al. (2020) proposes an architecture based on GANs to solve
the limited amount of labelled datasets. The MM-GAN can translate
label maps into 3D MRI without distorting the pathology. They proved
the extensibility of their approach by using the BraTS17 dataset and a
new dataset (LIVER100).

The ğ›¼-GAN architecture proposed by Rosca et al. (2017) may solve
both blur and mode collapse by using perceptual similarity metrics
and adversarial loss. It has also been shown that the use of spectral
normalisation (Miyato et al., 2018) can stabilise the training of the discriminator, which helps to avoid mode collapse and non-convergence.
WGAN-GP (Gulrajani et al., 2017) can also be a good strategy to
achieve Lipschitz continuity and, again, avoid mode collapse, nonconvergence and vanishing. All of these techniques were successfully
applied in Ferreira et al. (2022b).

4.2. Dealing with limited data
As already mentioned, the generation of synthetic data using GANs
is a good approach for data argumentation when the amount of data
is not sufficient for the downstream task. However, if the amount of
data is already too small, how can a GAN be trained? It is well known
that GANs require large datasets to produce realistic samples, and large
22

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 18. Comparison between original and synthetic scans generated by the method proposed by Dong et al. (2019), and segmentations: (1) CT image; (2) synthetic MRI; (3)
manual segmentation of bladder (white), prostate (yellow), and rectum (red); (4) segmentation by DSUnet; (5) segmentation by DAUnet trained on CT data; (6) segmentation by
DAUnet trained on synthetic MRI data. (a), (b) and (c) are the results on three distinct patients. Reprinted from Radiotherapy and Oncology, Vol. 141, authors Xue Dong and Yang
Lei and Sibo Tian and Tonghe Wang and Pretesh Patel and Walter J. Curran and Ashesh B. Jani and Tian Liu and Xiaofeng Yang, â€˜â€˜Synthetic MRI-aided multi-organ segmentation
on male pelvic CT using cycle consistent deep attention networkâ€™â€™, pages 192â€“199, Copyright (2019), with permission from Elsevier B.V. Publication link.

Fig. 19. Visual results. (a1â€“a4) shows the sagittal views of CT, low count PET, full count PET and synthetic full count PET images. (b1â€“b4) shows the coronal views (Lei et al.,
2020b). Reprinted from Medical Imaging 2020: Physics of Medical Imaging, Vol. 11312, authors Yang Lei and Tonghe Wang and Xue Dong and Kristin Higgins and Tian Liu and
Walter J Curran and Hui Mao and Jonathon A Nye and Xiaofeng Yang, â€˜â€˜Low dose PET imaging with CT-aided cycle-consistent adversarial networksâ€™â€™, pages 1043â€“1049, Copyright
(2020), with permission from Society of Photo-Optical Instrumentation Engineers (SPIE) and one of the authors. Publication link.

23

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 20. Stenosed segments of coronary arteries (Danu et al., 2019): Top â€” training surfaces; Bottom â€” synthetically generated using the GAN model. Â© 2019 IEEE. Reprinted,
with permission, from 2019 23rd International Conference on System Theory, Control and Computing (ICSTCC), authors Manuela Danu and Cosmin-Ioan Nita and Anamaria Vizitiu
and Constantin Suciu and Lucian Mihai Itu, â€˜â€˜Deep learning based generation of synthetic blood vessel surfacesâ€™â€™, pages 662â€“667. Publication link.

Fig. 21. Different views of the 80S Ribosome and reconstruction using CryoGAN (Gupta et al., 2021). First column â€” 80S Ribosome from EMPIAR-10028; Second column â€”
CryoGAN reconstruction from synthetic data with; Third column â€” 80S Ribosome filtered; Fourth column â€” CryoGAN reconstruction filtered. Figure reproduced from Gupta et al.
(2021). Publication link. Licence CC BY 4.0 DEED.

24

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. 22. Examples of 3D car shapes generated by the system created by Nozawa et al. (2021) with side-view sketches. Top left white lines are input contour sketches; yellow
point clouds are corresponding outputs. Figure reproduced from Nozawa et al. (2021). Publication link. Licence CC BY 4.0 DEED.

Fig. 23. Real and generated images during training process for Berea sandstone and Estaillades carbonate sample (Liu et al., 2019). Reprinted from Energy Procedia, Vol. 158,
authors Siyan Liu and Zhi Zhong and Ali Takbiri-Borujeni and Mohammad Kazemi and Qinwen Fu and Yuhao Yang, â€˜â€˜A case study on homogeneous and heterogeneous reservoir
porous media reconstruction by using generative adversarial networksâ€™â€™, pages 6164â€“6169, Copyright (2019), with permission from Elsevier B.V. Publication link. Licence CC
BY-NC-ND 4.0 DEED.

random vector. ğ‘ is adjusted depending on the overlap between the real
and fake distributions. If the percentage of correct predictions on the
real data is too high, ğ‘ increases, and in the opposite case it decreases.
Although this study was performed on 2D images, the same approach
can be applied to volumetric data, as it does not depend on the loss
function or architecture of the GAN.
Besides data augmentation, other strategies can be used to avoid
overfitting the discriminator. Transfer learning has been shown to
improve the results of deep learning tasks when a small dataset is
available. However, large pre-trained models, e.g. VGG, ResNet50, InceptionV3, only work with 2D data and not volumetric data. The lack of
such a large model pre-trained on volumetric data makes this approach
more difficult. However, even if no large pre-trained model is available,
it is possible to address such a task with similar datasets (Chen et al.,
2019). For example, if the goal is to generate synthetic MRI scans of the
brain for Alzheimerâ€™s disease classification in a personal small dataset,
the discriminator can first be pre-trained with the BraTS and/or ADNI
datasets using a strategy called self-supervised learning (Rani et al.,
2023). Subsequently, this model can be used as a discriminator by
replacing the last layers with a sigmoid and freezing the first layers.
The rest of the training process of the GAN is done as usual. However,
if no data is available, a technique called weight inflation can be used
to perform transfer learning from 2D models to 3D models (Liu et al.,
2022). This technique is very recent and little explored for GANs, but it
could be interesting for further research. Zero-shot learning (Xian et al.,
2018) has also been explored to overcome the lack of data of particular
classes for positron image denoising (Zhu et al., 2023), image-to-image
translation (Lin et al., 2021b), feature generation (Gao et al., 2020), and
others. No work has been found in which zero-shot learning has been
applied to volumetric data, but such an approach could be adapted to
volumetric data for further investigations.
However, what should be done when transfer learning is not possible? In cases where no other data can be found that is similar to

volumetric datasets are not freely available. Training a GAN depends
on the type and complexity of the data to be generated, thus there is
no general approach to train a GAN perfectly with any dataset.
The discriminator is a deep learning classifier. Therefore, if there
is not enough data, overfitting can easily occur when no precaution is
taken. Even with large datasets, overfitting can occur if the discriminator network is large enough. Overfitting the discriminator is a problem
that is usually not addressed, but can lead to poor results because an
overfitted discriminator does not provide meaningful feedback to the
generator. This can be detected by using a validation dataset consisting
of real images that were not seen in the training process of the GAN. If
the discriminator classifies most of these images as fakes, it means that
the discriminator is overfitted.
To address this issue, Karras et al. (2020) have conducted a study
on how the size of the dataset affects the quality of the generated
data and propose the use of an adaptive augmentation mechanism that
stabilises the training and allows the use of smaller datasets without
overfitting the discriminator. Traditional data augmentations, i.e. geometric transformation (rotate, flip, scale, shift), colour transformation,
noise addition, image filtering and cropping, have been shown to be
beneficial for the generality of deep learning tasks. Therefore, these
augmentations can be used for training the discriminator. However,
such transformations should not be learned by the generator, i.e. the
augmentations cannot leak because the augmentation version does not
correspond to reality. The probability (ğ‘) of a transformation occurring should be lower than the probability of the transformation not
occurring. This makes the training dependent on the original data
without transformation, which is actually learned by the generator. It
ensures that if ğœğ‘¥ = ğœğ‘¦ , then ğ‘¥ = ğ‘¦, where ğœ is the stack of invertible
transformations, ğ‘¥ and ğ‘¦ are the real and fake distributions. In order to
make this work, all data (real and fake) are transformed before being
fed into the discriminator, i.e., ğ·(ğœ(ğº(ğ‘§))) and ğ·(ğœ(ğ‘¥)), where ğ‘§ is the
25

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

the dataset to be used, efforts should focus on the discriminatorâ€™s
architecture. No deep learning architecture will overfit if its capacity
is not large enough, so reducing the number of learning parameters of
the discriminator can help. Adding dropout layers (Srivastava et al.,
2014) and using weight decay could also be helpful. Focusing on
hyperparameter tuning is also a good strategy as, e.g., an incorrect
learning rate can cause the network to collapse.
Volumetric data is more complex than two-dimensional data because of the presence of volumetric information. However, some types
of volumetric datasets might be easier to learn and would require fewer
training samples than 2D data. For example, medical data of the same
modality and body part is very identical, which is not the case for
datasets of natural images, such as ImageNet. This low intravariability
is advantageous for training GANs because the data distribution is
more restricted, which makes it easier for the generator to produce
realistic samples. Therefore, in some cases, a few hundred cases may
be sufficient to train a GAN properly.

However, these approaches do not ensure that depth information and
some features that might be important for some tasks, such as symmetry
or long dependencies, are captured. Wang et al. (2017) employ LRCN
to increase the resolution of the generatorâ€™s output, which makes GAN
training faster and more stable by using lower resolutions, achieving
higher resolutions even with memory constraints.
Loss functions: Some researchers have proposed their own loss
functions to take advantage of volumetric information and produce
more realistic data. For example, perceptual loss, which is used in
several works on 2D images and does not take into account the dependencies between layers, is replaced by volumetrically trained networks,
such as in the loss functions feature consistent, latent vector, content
and style. The GANsâ€™ objective functions have already been discussed
in 3.1.1, where it is concluded that there are no major differences
between them, but that the vanilla GAN and the WGAN-GP are the
most commonly used and that the WGAN-GP may perform better
on more complex data. More detailed information on loss functions
for volumetric data can be found in the following sections: latent
vector loss (Appendix A.2.1), feature consistent loss (Appendix A.2.2),
content and style loss (Appendix A.2.3) Gradient difference loss (Appendix A.3.4), identity loss (Appendix A.3.5), Laplacian, projection and
orientation losses (Appendix A.3.6), depth loss (Appendix A.3.7), shape
consistency and spatial (Appendix A.3.9), border and volume losses
(Appendix A.4.1).
Evaluation metrics: A consistent metric for comparing different
models for volumetric data generation is also crucial. For example, if
two papers use the same dataset to generate new data, it is very difficult
to compare which model is better. For reconstruction or noise reduction
problems, this problem is alleviated by the fact that ground truth is
available, e.g. in Harms et al. (2019), Lei et al. (2020b), Liu et al.
(2020b), Li et al. (2020), since the use of PSNR and MS-SSIM/SSIM
is widely accepted. However, in modality translation it is very difficult
to evaluate with unpaired data only, or when the goal is to generate
completely new volumes (Xu et al., 2019). The generation of 3D data
has not been sufficiently researched compared to the generation of 2D
data. The most commonly used metric for comparing models is FID,
which only works with 2D data. Several adaptations of FID to 3D data
have been made, but none of them are optimal, as the volumetric
context is lost (Appendix B.1.11). Metrics such as SIS and S-score
should be considered instead of FID for volumetric data. Even quality
control through human judgement is a bottleneck, as the developer may
not have the required expertise in the field in question, e.g. medical
image analysis, relying on expert feedback, which is difficult to obtain,
very expensive and time-consuming. Nevertheless, the visual Turing
test is one of the best metrics for evaluating synthetic data as well as
applying the data to a downstream task. It is curious, though, that so
few papers use the visual Turing test. For a more detailed discussion
of the best evaluation metric, refer to 3.2.1. No work was found that
uses maximum mean discrepancy (MMD). MMD is able to measure how
similar two distributions are, which can be used to assess whether the
real and fake data follow the same distribution, as used in Ferreira et al.
(2022b).
There is no specific strategy that works in all cases, not even for
2D images. For volumetric data, the difficulty is greater due to the
additional dimension. However, following the recommendations given
in Section 3.1.1 will help greatly to achieve satisfactory results. Continuous monitoring of the loss plots is helpful to detect instability and
mode collapse; choosing a low learning rate makes the training more
stable; using normalisation layers, e.g., spectral normalisation, batch
normalisation or instance normalisation, also stabilises the training;
test the WGAN-GP objective function if the vanilla GAN does not
produce satisfactory results; ensure that the networks are correct and
that the pipeline has no errors; find the GAN network that is best
suited to the problem at hand, e.g. CycleGAN for image translation
and cGAN for reconstructions; use other loss functions to support the
objective function of the GAN as described in Section 3.1, e.g., identity,

4.3. Differences between volumetric and 2D data generation
Although a lot of research has been conducted in this area, many
challenges remain, such as those mentioned in Section 2. Instability,
mode collapse, non-convergence, and overfitting are even more difficult
to overcome when 3D data are used instead of 2D data. Experiments
with 3D data also take longer and there is no large dataset freely
available, which explains the fact that the most important discoveries
of GANs have been made for 2D data first. In Egger et al. (2022), the
main differences between using 2D and 3D are discussed, highlighting
the complexity of dealing with an extra dimension, the difficulty of
capturing large datasets among other issues.
Volumetric data generation is not just an extension of 2D approaches, as some architectures, loss functions and evaluation metrics
are incapable of capturing the contextual and spatial information. 2D
approaches are not able to use the context of adjacent layers since
they cannot learn the spatial relationship between voxels, which is
important for accurate and realistic generation. Such an approach can
lead to shape discontinuities across adjacent layers and no consistency
in voxel intensity among layers. Continuity between layers is very important in some downstream tasks, e.g. the winners of the 2020 (Isensee
et al., 2021) and 2021 (Futrega et al., 2022) BraTS Challenges used
a volumetric architecture for brain tumour segmentation. Therefore,
synthetic volumetric data must contain this information, if not, it might
be useless or even harmful for the downstream task.
Network designs: 3D convolution based networks are often used
to generate volumetric data. These differ from 2D convolutions in that
they are able to capture the depth information of volumetric data
by learning dependencies between layers. Some of the architectures
mentioned in Section 3.3 are derived from 2D architectures, but with
adapted hyperparameters and particular specifications to handle the
additional difficulty of dealing with the complexity of volumetric data.
Depending on the task, the network can process both 2D and 3D data
(Section 3.3.2), accept volumetric data as input (Sections 3.3.1 and
3.3.3) or even use special networks to control the main GAN pipeline,
e.g. a DSNN (Appendix A.2.2). Therefore, these networks must be able
to extract all features contained in volumetric data. Volumetric data
also requires computers with higher processing and storage capacity
than 2D data. Some works address this last challenge and attempt to
solve it. Nozawa et al. (2021) use multiview depth images as an intermediate representation, and then create the volume by recomposing
multiple views. Jung et al. (2020) replaced the 3D generator with a 2D
generator, keeping a 3D and a 2D discriminator, which allowed faster
training with lower memory consumption. Pesaranghader et al. (2021)
has been able to create volumes with dimensions greater than 224 Ã—
224Ã—224 by generating multiple slices and then assembling them, rather
than processing the entire volume at once. Harms et al. (2019) uses
patches as input and generates full resolution by stacking these patches.
26

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

feature consistency, voxel-wise, among others; choose an appropriate
evaluation metric as described in Section 3.2 and visually evaluate the
quality of the generated volumes frequently. Stable training is very
important to avoid mode collapse. It is important to note that the size of
the input vector of the generator should be adapted to the complexity of
the data, as volumetric data is to be used, the size should be increased.
Problems with the GANâ€™s training may also be related to the size of the
dataset, see Section 4.2.
The use of more efficient coding methods is also essential to make
the best use of the available hardware. MONAI (MONAI Consortium,
2022) has done a great effort in this respect, using best practices in
volumetric data processing and offering a wide range of features to
support the user.
The storage, access, and visualisation of volumetric data is also
different and more complex than 2D imagery, which requires specialised pipelines. 2D images are typically stored and loaded as JPG or
PNG, whereas volumetric data encompasses a wider range of formats:
DICOM, NIfTI (generality of medical data), CN3D,3 OFF,4 OBJ+MTL,5
among many others.
Summarising, the generation of 3D data compared to 2D data,
for both medical and non-medical purposes, faces several challenges
related to the limitations of the machines, i.e., need of more memory,
more powerful GPUs and more energy consumption, as well as the lack
of robust and consistent metrics to assess the quality of the new data,
and the lack of large, freely available volumetric datasets.

are in the medical field. They can be used for classification,
reconstruction, denoising, nuclei counting, segmentation, image
translation and general purposes, as can be seen in Tables C.1
and C.2.
2. What are the methods most frequently or successfully employed
by GANs in the generation of volumetric data?
Since it is not possible to make a good comparison between
the models, it is not easy to assert which GANs are more successful in generating volumetric data. However, CycleGAN-based
architectures are chosen very often and have achieved good
results, followed by architectures based on cGAN, DCGAN, and
WGAN/WGAN-GP, as can be seen in Fig. 11 and Tables 1â€“3. The
best architecture also depends heavily on the task, e.g. CycleGAN should be used for image translation but probably not for
reconstruction.
3. What are the strengths and limitations of these methods?
The use of GANs has proven to be a powerful tool for handling
multimodal data, for data augmentation, and for anonymisation.
However, there are several problems that have already been
mentioned in Sections 4.2 and 4.3. The lack of a consensus
metric to evaluate GANs, the instability of training, hardware
limitation, and lack of large datasets remain major challenges,
especially when dealing with volumetric data.
4. What improvements are sought through the use of this technology?
This technology is widely used to generate anonymised synthetic
data for data augmentation, as shown in this review, due to
the inherent anonymisation capacity of GANs and because of its
realism compared to other generative approaches. The synthetic
data can be used to improve other deep learning algorithms by
solving the problem of lack of data and being a good aid for
other data augmentation techniques. In addition, GANs can also
be used for denoising, reconstruction and modality translation
problems, making these architectures a powerful tool to have
always ready when other technologies are not satisfactory.

4.4. Concerns with synthetic data
GANs can also be a threat, as they can be used to deceive people.
Recently, there have been increasing reports of the use of synthetic
videos and voices in video calls.6 These threats only use 2D data and
video, and no problems with volumetric data have been reported yet.
However, biases in medical data could be amplified, rare diseases are
not generated if they are not present in the original dataset (or have not
been appropriately processed), the correct phenotype could be missing,
artefacts/hallucinated details can be misleading or lead to (falsely)
overconfident interpretation by human experts, among other problems.
In more extreme cases, it may be possible with this technology to
fake examinations, e.g. remove brain tumours from images or create
synthetic tumours in healthy brains so that patients have to pay more
for medical treatment.
Furthermore, there are no methods sufficiently robust and objective
to determine whether a synthetic dataset differs from the genuine original dataset to be classified as truly anonymous, leaving a dangerous
gap in sensitive data sharing legislation. As mentioned in Chen et al.
(2021a), the trained models can have vulnerabilities that attackers can
explore, such as information leakage and re-identification.
The generation of volumetric synthetic data comes with several
other challenges, such as reliability, the need for experts to generate
the data and assess its realism, and the possibility of real-world outliers
being excluded. It is clear that the lack of proper metrics to ensure
quality, realism and anonymisation are among the biggest problems in
using synthetic data for medical proposes.

4.6. Tendencies of 3D volumetric data generating with GANs
It is noted that most of the works use improved adaptations of
existing GAN architectures for the problem at hand. Architectures such
as CycleGAN, cGAN, WGAN/WGAN-GP, PGGAN, and DCGAN were first
developed for 2D data generation and then adapted to volumetric data.
As mentioned earlier, the most investigated GAN architectures, loss
functions and evaluation metrics are mainly 2D related. These works
are so remarkable that researchers usually decide to adapt them for
volumetric generation instead of trying to achieve such results from
scratch.
One of the architectures that has attracted the most interest from
researchers is the CycleGAN. The use of CycleGAN-based architectures
is a preferred choice for many applications, namely image translation,
nuclei counting, reconstruction, and segmentation, as it is not limited
to the use of only one modality. Its ability to handle multimodality
is a valuable asset in many applications, especially for medical data.
It is well known that the use of more than one medical modality for
diagnosis is essential, as it offers more information. It is highly desirable
to have pipelines that are able to mimic physicians who use more
than one modality when treating patients (Heiliger et al., 2022). As
shown in Huang et al. (2020), using multimodal data can lead to higher
accuracy and better imitate human experts. Therefore, it is only natural
that versatile mechanisms capable of handling them are the next steps
in which the CycleGAN-based architecture can have a crucial role.
In works such as Zhang et al. (2018, 2019c), this technology is used
to generate MRI scans from CT and vice versa to increase the amount
of data available for multiclass segmentation of the heart. Generating
one modality from another is also applied to the liver, brain or even
the whole body, as can be seen in Table C.1 (Appendix C), showing

4.5. Research questions
1. What are the different applications of GANs in the generation of
volumetric data?
GANs are used in various fields, whereby a distinction can be
made between medical and non-medical fields, since most works

3

EMPIAR-10061
ModelNet10/40
5
ShapeNet
6
Deepfakes new (accessed [30-06-2022])
4

27

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

the versatility of these architectures. In works such as Schaefferkoetter
et al. (2021), Zeng and Zheng (2019), Zhang et al. (2019c, 2018), the
use of GANs has proven to be beneficial for exploiting multimodal medical datasets, even when only unpaired data exists. This architecture,
together with the loss function cycle consistency, feature consistency,
identity, shape consistency and spatial consistency, allows researchers
to obtain high quality results. These consistency losses (except for cycle
consistency) are a good example of an improvement over an existing
GAN architecture, making it more suitable for volumetric data. It is
expected that the use of such specific loss functions will increase in the
future, as each problem is different, so there should be cost functions
adapted to each dataset.
For non-medical applications, cGAN and DCGAN have been the
preferred choice. From this review, it can be seen that volumetric data
generation has been applied more to medical data than to non-medical
data, limiting the development of the latter. In the majority of papers,
authors preferred to keep the use of GAN networks at a simpler level
without using more complex networks. While this is not a shortcoming,
it would be interesting to use more complex networks to provide deeper
conclusions. The use of the Minkowski function is another example of
what is expected in the future, i.e. the use of specific loss functions that
are able to capture the essential features of the dataset.
cGAN is also much explored, e.g. in the work developed by Han
et al. (2019a). They created a pipeline to generate lung nodules in
the desired position, size and attenuation by creating small volumes
(32 Ã— 32 Ã— 32) with nodules and adding them to the surrounding tissue,
creating 64 Ã— 64 Ã— 64 volumes. This technique can also be extended to
various other types of lesions in other organs by converting the scans
from healthy patients to diseased ones. It can also be applied beyond
medicine, for example to create a specific porous media between other
materials or to create synthetic fractures in materials. This allows
saving computational resources and a more stable training due to the
small output size of the generator. cGAN is also a good option for
future work, as generating data conditioned by labelling reduces the
time and cost of obtaining and labelling large datasets. It is therefore
expected that more work will be done in the future with conditions for
generating synthetic volumetric data.
It is also noted that latent space has not been explored as much as
it deserves. As explained in Fragemann et al. (2022), the â€˜â€˜black boxâ€™â€™
inherent in generative models should be eliminated by better understanding the models and exploring techniques such as disentanglement.
Therefore, this deep understanding and exploration can also be the next
approach to the use of GANs, rather than creating new architectures.
Explainable AI applied to volumetric data generation will also be of
great interest, as understanding how the data was generated can help
improve existing mechanisms.
It is noted that researchers are aware of the limitations of 2D data
generation approaches for volumetric data as they strive to use architectures, loss functions and evaluation metrics that can consider the
additional complexity of such data. No major development is expected
in terms of architectures for generating voxel grid data, but most likely
the creation/use of loss functions and evaluation metrics that are able
to account for all dimensions and the dependencies that voxels have
with each other.
As explained in Section 1.3, there are other ways to explore volumetric data besides using voxel grids. In the works found in this
systematic review, the authors avoided using point clouds or meshes
by converting them to voxel grids, as this type of data is better studied
and therefore less difficult to process. The most studied approaches for
generating synthetic volumetric data are specific to voxel grids, so the
data must first be converted to use such approaches. However, converting to a voxel grid has some disadvantages, such as losing information
and adding more constraints to the data. Hence, it is expected that
architectures such as PointNet and PC-GAN will be explored more in
the future. Research into architectures that can directly process meshes
and point clouds would be of great interest.

4.7. Conclusion
In this systematic review, we give an overview of GAN-based approaches for the generation of realistic 3D volumetric data. In doing
so, we group the screened works by the underlying data (dimension),
which can be 2D or 3D, and remove the 2D data works. Then, we
further group them by the targeted modalities, CT, MRI, PET, or combinations of modalities. We furthermore extracted the used datasets and
if they are public ones that are available to the research community,
or if these are so-called private or in-house repositories that are not
(yet) available to the community. Note that even if making datasets
available alongside publications is becoming more and more common
these days, there may be reasons for keeping data under lock. Making data, especially medical data, public requires much more efforts
and extended or additional ethics approvals. It can also be that the
institution that generated the data, e.g. a hospital, allows studying the
data but forbids its publication, or that the disease is so rare in the
institution that it does not make sense to publish it because only a few
cases are involved. Another reason can be, that the authors want first to
exploit the data for further studies and publications, e.g. for rare disease
cases, which can be understandable, because creating data collections
requires a certain amount of effort, especially large ones, including
further manual processing such as creating ground truth segmentations.
Getting an internal review board to share the data with internal and
external partners involved in the study also takes time and a lot of work,
with regard to preparation and anonymisation.
In addition to the datasets and modalities, we extracted the data
types, the network architectures, the loss functions, evaluation metrics
and resolutions for every reviewed work. It was concluded that the
main modality used was CT, followed by MRI, and that the use of
multimodal data is widely used especially in the medical context.
Image translation was the main use of GANs, followed by Reconstruction,
which explains the extensive use of CycleGAN-based and cGAN-based
architectures. It can be seen that, especially in the non-medical context,
the use of synthetically generated datasets to train GANs was a trend
in the last year (2021). In the medical context, the use of multimodal
data has been explored every year, proving the importance and need
for appropriate pipelines to process this type of data.
4.7.1. Research opportunities
Comparable to the study of Lucic et al. (2018), it would be interesting to investigate different GAN architectures (including vanilla GAN,
PGGAN, CycleGAN) and objective functions (including vanilla GAN,
LSGAN, HingeGAN, WGAN and WGAN-GP) with different datasets of
volumetric data to investigate whether one is better than the others.
Such a study would help to determine with greater clarity whether a
particular architecture and/or objective function is preferable over the
others, or whether they do not show any difference in results.
Due to the inherent anonymisation capability of GANs, they could
be used to generate synthetic head scans of faces, as the defacement
of head scans is not always an option, e.g. for medical augmented
reality (Gsaxner et al., 2019) and facial implants (Memon et al., 2021).
In these works, it is not possible to develop algorithms with satisfactory
results because it is very difficult to obtain large datasets with the facial
part, especially with the soft tissue. All data must be approved so that it
can be published and freely shared. However, such data is very difficult
to be approved because it is not anonymous as it contains the face part.
Therefore, creating synthetic faces for these scans would be a good
solution to this challenge.
The creation and deployment of a full data anonymisation framework could be a good solution to data sharing problems, as proposed
by Shin et al. (2018). It would also be very interesting to develop a tool
that is able to automate the whole process of training and generating
synthetic data, similar to the nnUNet (Isensee et al., 2021), which is
able to configure itself and choose the best network architecture, training, pre- and post-processing for each new task. Such an automatic tool
28

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

would be very difficult to design, as each dataset is very different from
the others, but for generating data from the same source, e.g. medical
volumetric data, the development of such a tool would be possible and
very useful.
The use of multimodal data as input is a rarely addressed topic. An
example of the benefits of using multimodal data is Lei et al. (2020b),
which generates a full PET count from low-count PET and CT scans.
There are several works that include multimodal data in the training
dataset, but they are used to train an image translator and do not
exploit the full potential that this dataset can offer, i.e. they only use
one modality at a time. For example, the use of PET+CT scans to
produce MRI could be relevant.
In order to improve GANs, a uniform evaluation metric should be
established to allow better comparison between models from different
studies. It would also be very important to find a metric that is close
to human judgment to improve the results of GANs, especially when
volumetric data is used. FID adaptations to volumetric problems are
not generally accepted because it is computationally intensive and
lacks spatial context. Research in this area is more important than
the development of new GAN architectures, as several different architectures are already available that show great potential, but lack
appropriate metrics and loss functions. The next developments of GANs
can also proceed through the exploration of latent space and use more
techniques such as disentanglement. In a medical context, the best
approaches will probably be the use of multimodal data, which makes
CycleGAN-based architectures highly preferable.
The creation of large databases of volumetric data that are freely
available and easy to use would also be of great interest for pretraining
models (as explained in Section 4.2). The existence of a database
where data can be retrieved via a query would make it easier for the
researcher to download the data needed instead of searching for it in
different places or wasting time requesting it.
The use of attention mechanisms to generate synthetic data will
certainly increase, due to the fast development of architectures based
on Transformers (Vaswani et al., 2017), such Vision Transformer (ViT)
(Dosovitskiy et al., 2020), Swin UNETR (Hatamizadeh et al., 2022).
Also, the fast development of hardware that can process large amounts
of data will encourage research into volumetric data generation and the
employment of more complex/computationally demanding approaches.
The generation of other volumetric data representations, such as
point clouds and meshes, has not been explored much. It would be of
great benefit to investigate ways to generate such data instead of having
to convert them into voxel grids first.
It is undeniable that volumetric data generation is having a greater
impact on the medical field, so research in this area is expected to
increase. There has also been an increase in the number of FDAapproved AI to support healthcare decision-making, which could lead
to greater investment in this area so that more data is available to create
more reliable models.
Volumetric 3D data must be explored as a separate problem from
2D data. The distance from 3D to 2D can be understood as the distance
from 2D to 1D data. There are special networks for text processing
data (1D data) and images (2D data), as both have a very different
complexity and distribution. Volumetric data has an extra dimension
compared to 2D data, which gives it complexity and additional features
that are not present in 2D images.

Declaration of competing interest

CRediT authorship contribution statement

where ğ‘ğ‘‹ and ğ‘ğœƒ are the probability densities of ğ‘ƒğ‘‹ and ğ‘ƒğœƒ .
Pesaranghader et al. (2021) is one of the few papers that mentions
the use of JS divergence for calculating adversarial loss, but it is
assumed that the papers that do not mention any other method for
calculating adversarial loss (e.g. Wasserstein or least squares) used
the original approach, i.e. JS divergence computed with the BCE (Appendix A.3.3). Thus, if â€˜â€˜JSâ€™â€™ appears in the column â€˜â€˜Loss functionâ€™â€™ of
the tables below (1, 2 and 3), it means that the use of this function
is mentioned in the paper, and just â€˜â€˜Advâ€™â€™ otherwise. Pesaranghader
et al. (2021) did not find any drastic changes in their experiments
by using Wasserstein loss or JS. However, Arjovsky et al. (2017b) has

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Data availability
No data was used for the research described in the article.
Acknowledgements
AndrÃ© Ferreira was supported by a scholarship from FundaÃ§Ã£o
para a CiÃªncia e Tecnologia (FCT), Portugal (Scholarship number
2022.11928.BD), Ministry of Science, Technology and Higher Education (MCTES), Fundo Social Europeu (FSE) and European Union.
This work was supported by FCT within the R&D Units Project Scope:
UIDB/00319/2020. It was also supported by by the Advanced Research
Opportunities Program (AROP) of RWTH Aachen University. This work
received funding from enFaced (FWF KLI 678), enFaced 2.0 (FWF
KLI 1044) and KITE (Plattform fÃ¼r KI-Translation Essen) from the
REACT-EU initiative (EFRE-0801977, https://kite.ikim.nrw/).
Appendix A. Loss functions

A.1. Adversarial loss
From GANsâ€™ first paper (Goodfellow et al., 2014), the adversarial
loss is given by the discriminatorâ€™s correct classification of the realism
of the data received. If the discriminator gives a correct prediction, the
generator is penalised; if it gives an incorrect one, the discriminator
itself is penalised. This is also known as the â€˜â€˜minmax-gameâ€™â€™, originally
expressed in Eq. (3), where the output of the discriminator is compared
with the real label (true or fake) using BCE. Several alternatives to
compute this â€˜â€˜gameâ€™â€™ have been purposed.
A.1.1. Jensenâ€“Shannon (JS) and Kullbackâ€“Leibler divergence (KL-divergence)
The JS loss is based on the KL divergence to measure the distance
between two probability distributions. The adversarial loss (minmaxgame, described by Eq. (3)) is also defined in the original paper (Goodfellow et al., 2014) by the JS divergence between the real data distribution and the distribution of the generatorâ€™s parameters, where
ğ‘šğ‘–ğ‘›ğº ğ‘šğ‘ğ‘¥ğ· is given by Eq. (A.1):
ğ‘ƒ + ğ‘ƒğœƒ
ğ‘ƒğ‘‹ + ğ‘ƒğœƒ
(A.1)
) + ğ¾ğ¿(ğ‘ƒğœƒ âˆ¥ ğ‘‹
)
2
2
where ğ¾ğ¿ is the Kullbackâ€“Leibler divergence (Eq. (A.2)), ğ‘ƒğ‘‹ the real
data distribution, and ğ‘ƒğœƒ the distribution of the weights of the generator. Goodfellow et al. (2014) shows that the optimal solution, i.e., ğ‘ƒğ‘‹ =
âˆ— (ğ‘¥) =
ğ‘ƒğœƒ , is obtained when the generator consistently outputs 12 , i.e., ğ·ğº
1
(with a fixed G).
2

ğ½ ğ‘†(ğ‘ƒğ‘‹ âˆ¥ğ‘ƒğœƒ ) = ğ¾ğ¿(ğ‘ƒğ‘‹ âˆ¥

ğ¾ğ¿(ğ‘ƒğ‘‹ âˆ¥ğ‘ƒğœƒ ) =

AndrÃ© Ferreira: Conceptualization, Data curation, Formal analysis,
Investigation, Methodology, Project administration, Validation, Visualization, Writing â€“ original draft, Writing â€“ review & editing. Jianning
Li: Validation, Writing â€“ review & editing. Kelsey L. Pomykala: Validation, Writing â€“ review & editing. Jens Kleesiek: Funding acquisition,
Resources, Supervision, Writing â€“ review & editing. Victor Alves:
Funding acquisition, Resources, Supervision, Writing â€“ review & editing. Jan Egger: Funding acquisition, Resources, Supervision, Writing â€“
review & editing.
29

âˆ«

ğ‘ (ğ‘¥)
ğ‘ğ‘‹ (ğ‘¥)ğ‘™ğ‘œğ‘”( ğ‘‹ )ğ‘‘ğ‘¥
ğ‘ğœƒ (ğ‘¥)

(A.2)

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

proved that using Wasserstein loss (Appendix A.1.4) is preferable over
JS, since KL yields infinity when two distributions are disjoint. Even
when two distributions have no intersection (which is common for lowdimensional manifolds), Wasserstein loss can still provide a reasonable
representation of the distance between the two distributions, without
sudden changes, which is very important for a stable learning process.
The KL-divergence (also known as relative entropy) is used in Hu
et al. (2022) to reduce the distribution divergence between the encoded
latent vector (encoded by a ResNet encoder) and the sampled latent
vector (from the normal distribution) in the purposed GAN (BMGAN).

A.1.4. Wasserstein GAN (WGAN) and WGAN with gradient penalty
(WGAN-GP)
The WGAN (Arjovsky et al., 2017a) uses the Wasserstein distance
(i.e., earth-mover (EM) distance) instead of the JSD (Appendix A.1.1)
to measure the divergence between two distribution. The EM distance
is defined by Eq. (A.6):
ğ‘Š (Pğ‘Ÿ , Pğ‘” ) = ğ‘–ğ‘›ğ‘“ğ›¾âˆˆğ›±(Pğ‘Ÿ ,Pğ‘” ) E(ğ‘¥,ğ‘¥)âˆ¼ğ›¾
[â€–ğ‘¥ âˆ’ ğ‘¥â€–]
Ì‚
Ì‚

where ğ›±(Pğ‘Ÿ , Pğ‘” ) is all possible joint probability distributions between
Pğ‘Ÿ and Pğ‘” , ğ›¾(ğ‘¥, ğ‘¥)
Ì‚ indicates how much â€˜â€˜massâ€™â€™ to transport from ğ‘¥ to ğ‘¥Ì‚
in order to transform Pğ‘Ÿ into Pğ‘” .
The ğ‘–ğ‘›ğ‘“ is an intractable problem, however, based on the Kantorovichâ€“Rubinstein duality (CÃ©dric, 2008), Eq. (A.6) can be transformed
into Eq. (A.7):

A.1.2. Least squares (LSGAN)
The LSGAN loss (Mao et al., 2017) (Eqs. (A.3) and (A.4)) uses the
least squares loss function instead of the BCE (Appendix A.3.3) loss
function to avoid vanishing gradients. Mao et al. (2017) argue that
BCE leads to the problem of vanishing gradients when classifying fake
samples that are on the correct side of the boundary but still far from
reality. LSGAN even penalises the correctly classified by forcing the
generator to produce samples in the direction of the decision boundary
(i.e. towards the manifold of the real data). It also provides a more
stable training process by mitigating the problem of vanishing gradients, i.e. the BCE saturates more easily, resulting in non-meaningful
feedback.
1
E
[(ğ·(ğ‘¥) âˆ’ ğ‘)2 ]
2 ğ‘¥âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘¥)
1
+ Eğ‘§âˆ¼ğ‘ğ‘§ (ğ‘§) [(ğ·(ğº(ğ‘§)) âˆ’ ğ‘)2 ]
2
1
ğ‘šğ‘–ğ‘›ğº ğ‘‰ğ¿ğ‘†ğºğ´ğ‘ (ğº) = Eğ‘§âˆ¼ğ‘ğ‘§ (ğ‘§) [(ğ·(ğº(ğ‘§)) âˆ’ ğ‘)2 ]
2

ğ‘Š (Pğ‘Ÿ , Pğœƒ ) = ğ‘ ğ‘¢ğ‘â€–ğ‘“ â€–ğ¿ â‰¤1 Eğ‘¥âˆ¼Pğ‘Ÿ [ğ‘“ (ğ‘¥)] âˆ’ Eğ‘¥âˆ¼Pğœƒ [ğ‘“ (ğ‘¥)]

ğ‘Š (Pğ‘Ÿ , Pğœƒ ) = ğ‘šğ‘ğ‘¥ğ‘¤âˆˆğ‘Š Eğ‘¥âˆ¼Pğ‘Ÿ [ğ‘“ğ‘¤ (ğ‘¥)] âˆ’ Eğ‘§âˆ¼ğ‘(ğ‘§) [ğ‘“ğ‘¤ (ğ‘”ğœƒ (ğ‘§))]

(A.3)

(A.4)

ğ¿ğºğ‘ƒ (Pğ‘¥Ìƒ ) = Eğ‘¥âˆ¼P
Ìƒ 2 âˆ’ 1)2 ]
Ìƒ ğ‘¥Ìƒ [(â€–âˆ‡ğ‘¥Ìƒ ğ·(ğ‘¥)â€–

(A.9)

where Pğ‘¥Ìƒ is sampled uniformly along straight lines between the data
distribution (Pğ‘Ÿ ) and generator distribution (Pğœƒ ). Since ğ‘“ is 1-Lipschitz
if it has gradients with norm at most 1 everywhere, the model is
penalised by the GP when the gradient norm moves away from 1. The
final critic (i.e., â€˜â€˜discriminatorâ€™â€™) loss is defined by Eq. (A.10).

A.1.3. Hinge
The hinge loss (Eq. (A.5)), as explained in Rosasco et al. (2004), is
a loss function widely used for classification. In this case, it was used
in Greminger (2020) to measure the distance between the discriminator
output and the label. It was shown in Lim and Ye (2017) that the
use of hinge losses leads to a Nash equilibrium between the generator
and the discriminator. Miyato et al. (2018) claims that using this loss
instead of the vanilla GAN loss leads to better results in their spectral
normalisation GAN. Jolicoeur-Martineau (2018) compares the vanilla
GAN, LSGAN, HingeGAN (GAN with hinge loss) and WGAN-GP, but
no conclusions can be drawn from their experiments, as WGAN-GP
performed better with the CIFAR-10 dataset (Krizhevsky et al., 2009),
but HingeGAN performed better with the CAT dataset (Zhang et al.,
2008). However, it is interesting to note that vanilla GAN and LSGAN
had problems with convergence on higher resolution images, while
WGAN-GP was able to converge. Unfortunately, no experiments were
conducted with HingeGAN for higher resolutions. Therefore, this loss
can be used instead of BCE (Appendix A.3.3) or least squares for GAN
loss (Appendix A.1.2). However, no study has been conducted with 3D
GANs comparing these metrics, so it is recommended to try this loss
and evaluate if more training stability and more realistic generations
are archived.
ğ‘šğ‘ğ‘¥(0, 1 âˆ’ ğ‘¦Ì‚ğ‘™ â‹… ğ‘¦ğ‘– )

(A.8)

where ğ‘¤ are the parameters to be learned by the discriminator. The
discriminator is called a critic because he evaluates the authenticity of
the image instead of saying whether the image is real or fake. However,
WGAN suffers from unstable training, slow convergence and vanishing
gradients due to the weight clipping used to preserve the Lipschitz
continuity, which even the authors describe as â€˜â€˜a clearly terrible way
to enforce a Lipschitz constraintâ€™â€™.
The WGAN-GP (Gulrajani et al., 2017) is an improvement over the
original WGAN by replacing the weight clipping with gradient penalty
(defined in Eq. (A.9)).

where ğ‘, ğ‘ and ğ‘ are the labels for fake, real and the value the generator
(ğº) wants the discriminator (ğ·) to believe for the generated data,
respectively. The values of ğ‘, ğ‘ and ğ‘ should satisfy the conditions
ğ‘âˆ’ğ‘ = 1 and ğ‘âˆ’ğ‘ = 2 or b = c = 1 and a = 0, as both performed similarly
in Mao et al. (2017). It is used in for the generation of volumetric data
in Slossberg et al. (2019), Han et al. (2019a), Xu et al. (2019), Cai et al.
(2019), Hu et al. (2022), Yang et al. (2021a).

ğ¿â„ğ‘–ğ‘›ğ‘”ğ‘’ =

(A.7)

where ğ‘ ğ‘¢ğ‘ is the 1-Lipschitz functions ğ‘“ âˆ¶ ğœ’ â†’ P. Supposing that ğ‘“
comes from a family of K-Lipschitz continuous functions {ğ‘“ğ‘¤ }ğ‘¤âˆˆğ‘Š , the
optimisation function can be defined by Eq. (A.8):

ğ‘šğ‘–ğ‘›ğ· ğ‘‰ğ¿ğ‘†ğºğ´ğ‘ (ğ·) =

ğ‘›
âˆ‘

(A.6)

ğ¿ğ‘Š ğºğ´ğ‘âˆ’ğºğ‘ƒ (Pğ‘Ÿ , Pğœƒ , Pğ‘¥Ìƒ ) = âˆ’ğ‘Š (Pğ‘Ÿ , Pğœƒ ) + ğœ†ğ¿ğºğ‘ƒ (Pğ‘¥Ìƒ )

(A.10)

with ğœ† is the weight of the ğºğ‘ƒ (usually 10), and âˆ’ğ‘Š (Pğ‘Ÿ , Pğœƒ ) since in
training the loss is minimised.
Using WGAN-GP makes it possible to achieve Lipschitz continuity
with almost no hyperparameter tuning. Batch normalisation cannot
be used because the objective of WGAN-GP is not valid in such an
environment due to the creation of correlations between samples. It
should therefore be replaced by layer normalisation (Ba et al., 2016).
The WGAN loss is used in Chen et al. (2020), Li et al. (2020), Kench and
Cooper (2021), and the WGAN-GP loss is used in Slossberg et al. (2019),
Pesaranghader et al. (2021), Han et al. (2019a), Jung et al. (2020),
Zhuang and Schwing (2019), Baumgartner et al. (2018), Yang et al.
(2017), Gupta et al. (2021), Shen et al. (2021) to generate synthetic
volumetric data.
A.1.5. Cycle consistency
Cycle consistency loss can be defined by Eq. (A.11):
ğ¿ğ‘ğ‘¦ğ‘ğ‘™ğ‘’ (ğº, ğ¹ ) = Eğ‘¥âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘¥) [â€–ğ¹ (ğº(ğ‘¥)) âˆ’ ğ‘¥â€–1 ]
+ Eğ‘¦âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘¦) [â€–ğº(ğ¹ (ğ‘¦)) âˆ’ ğ‘¦â€–1 ]

(A.11)

where â€– â‹… â€–1 is the MAE (Appendix A.3.1), ğº the forward generator, ğ¹
the backward generator, ğ‘¥ the input image of ğº and ğ‘¦ the input image
of ğ¹ .
This loss ensures that the image generated by the ğº, when used as
input of ğ¹ is similar to the input of ğº, i.e, ğ¹ (ğº(ğ‘¥)) â‰ˆ ğ‘¥, and viceversa, i.e., ğº(ğ¹ (ğ‘¦)) â‰ˆ ğ‘¦. It is mainly used for CycleGAN (Zhu et al.,

(A.5)

ğ‘–=1

where ğ‘¦Ì‚ is the output of the discriminator, ğ‘¦ the label, and ğ‘› the number
of samples. The real and the fake labels are 1 and -1, respectively.
30

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

This loss is more specialised than identity loss (Appendix A.3.5)
because it compares specific disease features in the intermediate state
instead of just comparing voxel values. It is also more suitable for
situations where the ground truth of the translated scan is not available.
Both are important to complement cycle consistency loss by controlling
the intermediate generation.

2017) training and is widely used for image translation when paired
data is not available, as it does not require a comparison between
the generated image and the ground truth in the intermediate step,
i.e., â€–ğº(ğ‘¥) âˆ’ ğ‘¦â€–1 and â€–ğ¹ (ğ‘¦) âˆ’ ğ‘¥â€–1 .
However, the cycle consistency loss does not ensure good quality of
the images generated in the intermediate step. This is done by using
two discriminators (one for each intermediate step), which can lead to
unrealistic synthetic data. When ground truth is available, i.e. the real
images of the intermediate step, the use of shape consistency, spatial
consistency (Appendix A.3.9), identity consistency (Appendix A.3.5)
or feature consistency (Appendix A.2.2) is recommended to control
the middle state of CycleGAN, which is not controlled by the cycle
consistency. Without such control, G(x) and F(y) might have artefacts
that only serve to trick the discriminators, not to look realistic.
The comparison between the two images is made per-pixel using
MAE as in the original paper, or other metrics such as MSE or CE
(Appendix A.3.3). Zhu et al. (2017) tested the use of adversarial loss
instead of MAE, but no improvements were observed, so MAE should be
used as it provides more stability in training, ensures that the generated
images are close to input and is less computationally intensive. It is
used in Jung et al. (2020), Harms et al. (2019), Lei et al. (2020a),
Schaefferkoetter et al. (2021), Lin et al. (2021a), Chen et al. (2021b),
Han et al. (2019b), Yang et al. (2021a) for the generation of volumetric
data.

ğ¿ğ‘“ ğ‘’ğ‘ğ‘¡ğ‘ğ‘œğ‘›ğ‘  = â€–ğ¹ğ‘€ (ğºğ‘ƒ (ğ‘‹ğ‘ƒ )) âˆ’ ğ¹ğ‘€ (ğ‘‹ğ‘€ )â€–+
â€–ğ¹ğ‘ƒ (ğºğ‘€ (ğ‘‹ğ‘€ )) âˆ’ ğ¹ğ‘ƒ (ğ‘‹ğ‘ƒ )â€–

where ğ¹ğ‘€ and ğ¹ğ‘ƒ are the features extracted from MRI and PET scans
by the DSNN, ğºğ‘ƒ and ğºğ‘€ the generators that transform PET into MRI
and vice versa, and ğ‘‹ğ‘€ and ğ‘‹ğ‘ƒ the original MRI and PET scans.
A.2.3. Content, style
Content and style as well as Laplacian, projection, and orientation
(Appendix A.3.6) loss functions are used in Shen et al. (2021) and are
essential for their style transfer work from 2D to 3D. The content loss
(Eq. (A.14)) is the squared error between two feature representations,
i.e. the squared error between the discriminator features in layer ğ‘™ of
the real and the fake samples. Higher-level layer (closer to the highestresolution image) capture high-level information, e.g., entire objects,
and lower-level layers represent voxel values, so for the content loss
only high-level layers are used.
1âˆ‘
Ì‚ ğ¼ğ‘† , ğ¼ğ‘€ ) âˆ’ ğ¹ğ‘™,ğ‘–,ğ‘— (ğ‘‹, ğ¼ğ‘† , ğ¼ğ‘€ )]2
ğ¿ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘›ğ‘¡ (ğ‘™) =
[ğ¹ğ‘™,ğ‘–,ğ‘— (ğ‘‹,
(A.14)
2
ğ‘–,ğ‘—

where ğ¹ğ‘™,ğ‘–,ğ‘— represents the discriminator features of the ğ‘–th filter at
position ğ‘— in the layer ğ‘™, ğ‘‹ and ğ‘‹Ì‚ the real and generated samples, ğ¼ğ‘†
and ğ¼ğ‘€ the sketch image and a mask image, and ğ‘‹Ì‚ = ğºğ‘  (ğ¼ğ‘† , ğ¼ğ‘€ ).
The style loss (Eq. (A.15)) is the squared error between the Gram
matrices in layer ğ‘™ of the real and the fake samples. This serves to
capture the texture and/or colour information, working on lower-level
features, i.e., voxel level features.

A.2. Loss functions to explore the intermediate layers
A.2.1. Latent vector loss
Latent vector loss as well as depth loss (Appendix A.3.7) are used
by Liu et al. (2021) to increase the fidelity of 3D reconstruction from
monocular depth images of objects.
The latent vector loss (Eq. (A.12)) minimises the distance between
the latent vector of a trained autoencoder ğ‘§ğ‘‘ğ‘ğ‘’ (trained to learn the
representation of the ground truth 3D voxel grid) and the latent vector
of the proposed generator ğ‘§ğ‘’ğ‘‘ğ‘”ğ‘ğ‘› (which is also an autoencoder). This
loss can be understood as a way to force the generatorâ€™s encoder to
produce latent vectors (localâ€“global latent vectors) identical to those
of the autoencoder, which has already learned to correctly encode the
ground truth (globalâ€“global latent vectors). This loss can be used not
only for 3D reconstruction, but also for denoising, segmentation or
even image translation as long as the ground truth is available and
a bottleneck architecture, i.e., with a latent vector (e.g. autoencoder,
variational autoencoder) is used.
ğ¿ğ‘™ğ‘œğ‘ğ‘”ğ‘’ğ‘› = |E(ğ‘§ğ‘‘ğ‘ğ‘’ ) âˆ’ E(ğ‘§ğ‘’ğ‘‘ğ‘”ğ‘ğ‘› )|

(A.13)

ğ¿ğ‘ ğ‘¡ğ‘¦ğ‘™ğ‘’ (ğ‘™) =
âˆ‘
1
Ì‚ ğ¼ğ‘† , ğ¼ğ‘€ ) âˆ’ ğ´ğ‘™,ğ‘–,ğ‘— (ğ‘‹, ğ¼ğ‘† , ğ¼ğ‘€ )]2
[ğ´ğ‘™,ğ‘–,ğ‘— (ğ‘‹,
2
2
4ğ‘ğ‘™ ğ‘€ğ‘™ ğ‘–,ğ‘—

(A.15)

where ğ´ğ‘™ is the Gram matrices (defined in Appendix A.2.4 by Eq.
(A.17)), ğ´ğ‘™,ğ‘–,ğ‘— is the inner product between the vectorised feature maps
âˆ‘
ğ‘– and ğ‘— in the ğ‘™th layer (ğ´ğ‘™,ğ‘–,ğ‘— = ğ‘˜ ğ¹ğ‘™,ğ‘–,ğ‘˜ ğ¹ğ‘™,ğ‘—,ğ‘˜ ), ğ‘ğ‘™ the number of feature
channels, and ğ‘€ğ‘™ the size of feature tensors.
Both are based on Gatys et al. (2016) which explains them in
more detail. Both are comparisons that focus on different levels of the
convolutional network and control the network at multiple levels, not
just in the output. These are highly recommended for style transfer
tasks that have a relevant impact on the generation of new styles and
new features of computed generated volumes (e.g., for video games (Shi
et al., 2020) or for animated films).

(A.12)

A.2.2. Feature-consistent
Feature-consistent loss (Eq. (A.13)) is used in Pan et al. (2019) to
ensure that the synthetic and real scans of the same modality have
the same disease pattern. For this purpose, a disease-specific neural
network (DSNN) is trained for disease classification. The features extracted by the model from a real and a generated scan (of the same
modality) should be identical. This loss induces the CycleGAN to focus
more on the disease-specific features, which is particularly important
for disease-specific reconstruction or image translation. Such a loss
would not be appropriate for healthy brains, in this case the DSNN
should be replaced by another trained model with healthy brains.
This loss is similar to the latent vector loss (Appendix A.2.1), but
here the loss is measured with the output features of several layers,
not just the latent vector. This loss function is also very familiar to
the loss of cycle consistency, since the goal is to minimise the distance
between the ground truth and the cycle reconstruction, i.e. ğ¹ (ğº(ğ‘¥)) â‰ˆ ğ‘¥,
where ğ‘¥ is the input, ğº is the generator that transforms ğ‘¥ to the other
modality, and ğ¹ is the generator that transforms ğº(ğ‘¥) back to the
original modality.

A.2.4. Perceptual
Perceptual loss uses a pre-trained deep convolutional neural network to measure the perceptual differences between two images. In the
first publication (Johnson et al., 2016), a VGG-16 (Visual Geometry
Group with 16 layers) network was used (Simonyan and Zisserman,
2014), pre-trained on the ImageNet dataset (Russakovsky et al., 2015).
However, this concept can be extended to other pre-trained networks,
such as VGG-19. This metric was used in one of the state-of-the-art publications on GANs (Karras et al., 2019). The perceptual loss is defined
by two losses, feature reconstruction loss and style reconstruction loss.
The feature reconstruction loss (Eq. (A.16)) measures the MSE
between the features extracted from the different layers (of the VGG
network) of the generated image ğ‘¥Ì‚ and the target image ğ‘¥. The features
extracted from higher layers contain information about the image content and spatial structure, but colour, shape and texture are lost. Therefore, the higher layers are used to calculate this metric by comparing
the image ğ‘¥Ì‚ with the target content image (ğ‘¥ğ‘ ).
ğ‘™ğ‘“ğœ™,ğ‘—ğ‘’ğ‘ğ‘¡ (ğ‘¥,
Ì‚ ğ‘¥) =
31

1
â€–ğœ™ (ğ‘¥)
Ì‚ âˆ’ ğœ™ğ‘— (ğ‘¥)â€–22
ğ¶ğ‘— ğ» ğ‘— ğ‘Š ğ‘— ğ‘—

(A.16)

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

where ğœ™ğ‘— (â‹…) are the activations (i.e., feature map) of the layer ğ‘— of the
VGG network (ğœ™), ğ¶ğ‘— Ã— ğ»ğ‘— Ã— ğ‘Šğ‘— the shape of the ğœ™ğ‘— (â‹…).

A.3.2. Lp -norm
The Lp (Eq. (A.21)) proposed in Rahimi et al. (2013) is used to
overcome the limitations of MSE (L2 , blurry images) and MAE (L1 ,
misclassification), as claimed by Liu et al. (2020b), Harms et al. (2019),
who set ğ‘ = 1.5.

The style reconstruction loss penalises the style differences (e.g.,
colour, patterns, and texture) between ğ‘¥Ì‚ and the style target (ğ‘¥ğ‘  ). Defining the Gram matrix by Eq. (A.17), the style loss is the squared Frobenius norm difference between ğ‘¥Ì‚ and ğ‘¥ğ‘  in several layers of the VGG
network (Eq. (A.18)). In Johnson et al. (2016), features are extracted
from higher and lower layers, extracting structural and style-related
features, respectively.
ğºğ‘—ğœ™ (ğ‘¥) =

ğœ“ğœ“ ğ‘‡
ğ¶ğ‘— ğ» ğ‘— ğ‘Š ğ‘—

1 âˆ‘
( |ğ‘¥ âˆ’ ğ‘¥Ì‚ ğ‘– |ğ‘ )1âˆ•ğ‘
ğ‘› ğ‘–=1 ğ‘–
ğ‘›

ğ¿ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘š =

(A.21)

where ğ‘› is the number of samples, ğ‘¥ and ğ‘¥Ì‚ the real and generated
data. This loss function is recommended when the use of ğ¿2 produces
too smooth boundaries and ğ‘™1 leads to misclassifications, as empirically
found by Harms et al. (2019).

(A.17)

where ğœ“ is the ğœ™ğ‘— (ğ‘¥) reshaped to ğ¶ğ‘— Ã— ğ»ğ‘— ğ‘Šğ‘—
ğœ™,ğ‘—
Ì‚ âˆ’ ğºğ‘—ğœ™ (ğ‘¥)â€–2ğ¹
ğ‘™ğ‘ ğ‘¡ğ‘¦ğ‘™ğ‘’
(ğ‘¥,
Ì‚ ğ‘¥) = |â€–ğºğ‘—ğœ™ (ğ‘¥)

A.3.3. Cross entropy (CE)
CE, also referred to as reconstruction loss defined in Eq. (A.22), is
used to measure the difference between two distributions. This loss was
used in Zhang et al. (2021) for the downstream tasks (segmentation),
in Jung et al. (2020), Liu et al. (2020a), Muzahid et al. (2021) to train
the discriminator, which is also the classifier, i.e. the discriminator has
two output channels instead of only one related to the realism of the
input, and in Wang et al. (2017) for the object reconstruction loss,
i.e., to compare the generated reconstruction with the ground truth
(voxel by voxel).
Binary Cross Entropy (BCE) (Ramos et al., 2018; Ruby and Yendapalli, 2020), also called sigmoid CE, is usually used to train a
binary classifier. Often used as the criterion of comparison between the
discriminatorâ€™s classification and the real label, as mentioned in Zhang
et al. (2021), Gayon-Lombardo et al. (2020), Dikici et al. (2021), Lei
et al. (2020b). When authors do not specify the use of other criteria and
only mention the vanilla loss function (Eq. (3)), this usually means that
BCE was used for the comparison between the discriminator output and
reality. Higher values of BCE correspond to higher uncertainty of the
discriminator, and therefore higher difference between distributions.
Using BCE for this purpose makes the discriminator feedback more
understandable to the user, more meaningful to the generator, as
opposed to using the absolute value (directly using the output of the
discriminator, as in WGAN). It is also used in Nozawa et al. (2021) to
measure the reconstruction loss between the generated masks and the
ground truths.
A modified CE is used in Yang et al. (2017) to strongly penalise
false positives compared to false negatives due to the properties of the
data used, and in Zhang et al. (2021) a weighted multi-class CE is used
to ensure that all classes are learned equally. Such changes make the
model less influenced by the classes that are easier to learn and less
biased.

(A.18)

where, â€– â‹… â€–2ğ¹ is the squared Frobenius norm, i.e., the MSE between two
matrices.
It was also used in for volumetric data by Hu et al. (2022) (VGG-16).
It is assumed that the pre-trained network has already learned how to
encode the perceptual and semantic information, allowing it to give a
meaningful feedback to the generator. However, this loss is rarely used
with volumetric data, especially medical data, because the pre-trained
networks do not accept volumetric inputs, i.e. the volumes would
have to be sliced to fit the pre-trained networks, resulting in a loss of
contextual and spatial information, and because of the large perceptual
gap between the datasets used for pre-training (natural images) and
the target datasets. Therefore, this metric is not recommended for
volumetric architectures. However, some other approaches are similar
to this, such using a pre-trained segmentation model to assess if the
features extracted from the original data are the same as the synthetic
data, e.g., feature consistent (Appendix A.2.2), and latent vector loss
(Appendix A.2.1).

A.3. Pixel/voxel-wise loss functions
A.3.1. Mean absolute error (MAE or L1 ), mean squared error (MSE or L2 )
MAE (Sammut and Webb, 2010d) (Eq. (A.19)) and MSE (Sammut
and Webb, 2010e) (Eq. (A.20)) are often used in regression problems
to compare the distance between two images/volumes. MAE is the
absolute average difference between the predicted (ğ‘¥)
Ì‚ and the expected
(ğ‘¥) values, and MSE is the average of the squared difference thereof.
Both are often used to stabilise GAN training. MAE is used more
frequently than MSE in the papers of this review (Fig. 9), suggesting
that the use of MAE may lead to more realistic volumetric data. Isola
et al. (2017) and Zhang et al. (2018) claim that MAE encourages
less blurring and better visual results, but Pathak et al. (2016) did
not find significant difference between both. Therefore, both metrics
should be tested to find out which metric gives the best results, or
use the ğ¿ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘š (Appendix A.3.2). Normally, MSE is used to compare
pixel/voxel values, but Liu et al. (2020a) calculates the MSE between
feature vectors extracted from the discriminator. MAE and MSE are two
very versatile metrics that can be used in different settings, e.g. cycle
consistency (Appendix A.1.5), comparison in the frequency domain
(Appendix A.3.8), calculation of identity loss (Appendix A.3.5), among
others. MSE and MAE loss functions can be used for both generation
and downstream tasks.
1âˆ‘
|ğ‘¥ âˆ’ ğ‘¥Ì‚ ğ‘– |
ğ‘› ğ‘–=1 ğ‘–

(A.19)

1âˆ‘
(ğ‘¥ âˆ’ ğ‘¥Ì‚ ğ‘– )2
ğ‘› ğ‘–=1 ğ‘–

(A.20)

ğ¿ğ¶ğ¸ = âˆ’

(A.22)

where ğ‘› is the number of classes, ğ‘¡ğ‘– the true class label, and ğ‘™ğ‘œğ‘”(ğ‘ğ‘– ) the
predicted probability of class ğ‘–.
A.3.4. Gradient difference, gradient magnitude distance (GMD)
Gradient difference loss (GDL) were found in the literature, defined
in two distinct ways. GDL is described as another voxel-wise metric
for measuring the distance between two volumes, enhancing edge
sharpness. A Sobel operator (edge detecting) (Kanopoulos et al., 1988)
is used by Ma et al. (2020) to measure the gradient difference between
the generated image and the target image (Eq. (A.23)).
ğ¿ğ‘”ğ‘‘ (ğº) = Eğ‘–,ğ‘—,ğ‘˜âˆ¼ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘–,ğ‘—,ğ‘˜) [â€–ğ‘”ğ‘‘(ğ‘¥) âˆ’ ğ‘”ğ‘‘(ğº(ğ‘¥))â€–ğ‘€ğ´ğ¸ ]

ğ‘›

ğ‘€ğ‘†ğ¸ =

ğ‘¡ğ‘– ğ‘™ğ‘œğ‘”(ğ‘ğ‘– )

ğ‘–=1

ğ‘›

ğ‘€ğ´ğ¸ =

ğ‘›
âˆ‘

(A.23)

where ğ‘”ğ‘‘(â‹…) is the function that calculates the gradient using a Sobel
operator, and ğ‘–, ğ‘—, ğ‘˜ the three axis.
32

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

This is particularly useful to ensure fidelity between the input 2D image
and the generated volume only in the visible part.
âˆ‘
ğ¿ğ‘ğ‘Ÿğ‘œğ‘— =
(ğ‘ƒ ğ‘Ÿğ‘œğ‘—(ğ‘¦)ğ‘– âˆ’ ğ‘‹Ìƒ ğ‘– )2
(A.26)

The GDL can also be calculated using Eq. (A.24), which is referred
to as the GMD loss in Harms et al. (2019) but was first introduced as
GDL in Mathieu et al. (2016). Instead of comparing the whole volume,
it is sliced in all three axis and the squared errors between two real
and generated sequential slices are computed. This ensures continuity
between the two layers situated next to each other. Harms et al. (2019)
do not discuss the importance of the loss, but it is assumed that this
loss can lead to sharper and continuous consecutive slices instead of
calculating the difference of only two slices, as explained by Mathieu
et al. (2016). It is also assumed that using one GDL or the other would
give similar results, but no experiments comparing these two losses
have been found. Therefore, GDL is recommended when edge sharpness
is critical to the realism of the generated volume and the blur margins
created by using MSE or MAE need to be reduced. It is expected that
the use of such a metric would be beneficial for all image generation
tasks, especially super-resolution, reconstruction, image translation and
noise reduction.
âˆ‘
Ì‚ =
ğºğ‘€ğ·(ğ‘‹, ğ‘‹)
{(|ğ‘‹ğ‘–,ğ‘—,ğ‘˜ âˆ’ ğ‘‹ğ‘–âˆ’1,ğ‘—,ğ‘˜ | âˆ’ |ğ‘‹Ì‚ ğ‘–,ğ‘—,ğ‘˜ âˆ’ ğ‘‹Ì‚ ğ‘–âˆ’1,ğ‘—,ğ‘˜ |)2

ğ‘–âˆˆğ¼ğ›©

where ğ¼ğ›© represents the 2D region, ğ‘ƒ ğ‘Ÿğ‘œğ‘—(ğ‘¦) the 2D projection of the 3D
orientation field ğ‘¦ in ğ¼ğ›© , and ğ‘‹Ìƒ ğ‘– the fake 2D orientation map.
However, this does not take into account the entire 3D orientation.
For this reason, the Laplacian loss (Eq. (A.27)) helps to diffuse the
constraints of the entire 3D orientation and not just the visible area.
This loss is the representation of the gradient divergence difference
between the ground truth and the generated 3D orientation.
âˆ‘
(ğ›¥(ğ‘¦Ìƒğ‘– ) âˆ’ ğ›¥(ğ‘¦ğ‘– ))2
ğ¿ğ‘™ğ‘ğ‘ =
(A.27)
ğ‘–

where ğ‘¦Ìƒ is the generated 3D orientation, and ğ‘¦ the ground truth.
Orientation loss (reference (A.28)) is used to create volumes, taking
into account the style of other volumes. This is very similar to projection loss, but instead of comparing pixels, it compares voxels, which is
specific to 3D data.
âˆ‘
ğ¿ğ‘œğ‘Ÿğ‘– =
(ğ‘¦Ìƒğ‘– âˆ— âˆ’ ğ‘…(ğ‘¦)
Ìƒ ğ‘– )2
(A.28)

ğ‘–,ğ‘—,ğ‘˜

+(|ğ‘‹ğ‘–,ğ‘—,ğ‘˜ âˆ’ ğ‘‹ğ‘–,ğ‘—âˆ’1,ğ‘˜ | âˆ’ |ğ‘‹Ì‚ ğ‘–,ğ‘—,ğ‘˜ âˆ’ ğ‘‹Ì‚ ğ‘–,ğ‘—âˆ’1,ğ‘˜ |)2
+(|ğ‘‹ğ‘–,ğ‘—,ğ‘˜ âˆ’ ğ‘‹ğ‘–,ğ‘—,ğ‘˜âˆ’1 | âˆ’ |ğ‘‹Ì‚ ğ‘–,ğ‘—,ğ‘˜ âˆ’ ğ‘‹Ì‚ ğ‘–,ğ‘—,ğ‘˜âˆ’1 |)2 }

(A.24)

ğ‘–âˆˆğ›¤

where X and ğ‘‹Ì‚ are two volumes to be compared, and ğ‘–, ğ‘— and ğ‘˜ the
three orientations.
Gradient difference is also mentioned in Liu et al. (2020b) but no
further discussion and explanation about the metric is given.

where ğ›¤ are the visible 3D voxels, ğ‘…(ğ‘¦)
Ìƒ the rotated 3D orientation field,
and ğ‘¦Ìƒâˆ— the generated volume.
A.3.7. Depth
The depth loss (Eq. (A.29)) penalises the difference between the
generated 3D voxel and the input 2.5D voxel grid (used by Liu et al.
(2021)). This ensures that the predicted values corresponding to the
2.5D voxel must be 1 and the values before that must be 0. The hidden
voxels (not visible in the 2.5D grid) are weighted less because multiple
solutions are possible. This ensures the fidelity of the visible parts of
the input in the reconstructed volume. This loss is particularly relevant
in 3D reconstruction problems from 2D or 2.5D images.

A.3.5. Identity
The identity loss used in Schaefferkoetter et al. (2021), as explained
in Zhu et al. (2017), uses the MAE distance to measure the distance
between the input and the corresponding output of the generator and
works as a cycle consistency loss (Appendix A.1.5) helper in the CycleGAN architecture to avoid unnecessary changes in the intermediate
step. The use of this loss is only possible when the images of the target
modality are available. This loss is recommended to avoid unnecessary
changes in the intermediated state of the CycleGAN training. Without
such loss, G and F would be free to generate unrealistic intermediate
scans, which is not controlled by the cycle consistency loss. Therefore,
its use is recommended when CycleGAN-based architectures are used
and when the intermediate target is available. In cases where the intermediate target is not available, feature-consistent loss (Appendix A.2.2)
is recommended.
ğ¿ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦ (ğº, ğ¹ ) = Eğ‘¦âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘¦) [â€–ğº(ğ‘¦) âˆ’ ğ‘¦â€–ğ‘€ğ´ğ¸ ]
+ Eğ‘¥âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘¥) [â€–ğ¹ (ğ‘¥) âˆ’ ğ‘¥â€–ğ‘€ğ´ğ¸ ]

ğ¿ğ‘‘ğ‘’ğ‘ğ‘¡â„ =

ğ‘˜âˆ’1
ğ¼ âˆ‘
ğ½
âˆ‘ ğ‘–ğ‘—ğ‘˜
âˆ‘
(ğ‘¦ğ‘’ğ‘‘ğ‘”ğ‘ğ‘› )2 ),
((1 âˆ’ ğ‘¦ğ‘–ğ‘—ğ‘˜
)2 +
ğ‘’ğ‘‘ğ‘”ğ‘ğ‘›
ğ‘–=1 ğ‘—=1

ğ‘˜=1

(A.29)

ğ‘–ğ‘—ğ‘˜
ğ‘˜ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›{1 â‰¤ ğ‘˜ â‰¤ ğ¾|ğ‘‹ğ‘–ğ‘›
= 1}

where ğ‘¦ğ‘’ğ‘‘ğ‘”ğ‘ğ‘› is the predicted 3D voxel grid by the proposed 3D GAN
(EDGAN), (ğ¼, ğ½ , ğ¾) the size of the voxel grid, and ğ‘‹ğ‘–ğ‘› the input 2.5D
voxel grid.

(A.25)

A.3.8. Frequency domain
Frequency domain loss (Eq. (A.30)) was used by Ma et al. (2020)
to reduce blur. It uses the Fast Fourier Transform to transfer the
target image and the generated image into the frequency domain and
calculate the MAE in the frequency domain. This loss can be used in
various problems to increase the visual realism of the generated images
if ground truth is available. It can be added or even replace other voxelwise functions, giving similar results and stabilising the training of the
generator.

where ğº is the generator that transforms one modality into the other
(e.g. MRI to PET), ğ¹ is the inverse of the generator ğº, i.e. it transforms
back into the original modality (e.g. PET to MRI), ğ‘¥ and ğ‘¦ are the
original MRI and PET scans.
Schaefferkoetter et al. (2021) trains a 3D CycleGAN to generate CT
scans from MRI scans. They also compare the use of a 2D network, a 3D
network and the 3D network with cycle consistency and identity losses
as generator, concluding that the use of a 3D network leads to substantial improved quality in the inference, and that cycle consistency
and identity losses were able to further improve the results (specially
noticeable in the handsâ€™ reconstruction).

ğ¿ğ‘“ ğ‘Ÿğ‘’ğ‘ (ğº) = Eğ‘–,ğ‘—,ğ‘˜âˆ¼ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘ (ğ‘–,ğ‘—,ğ‘˜) [â€–ğ‘“ ğ‘“ ğ‘¡(ğ‘¦) âˆ’ ğ‘“ ğ‘“ ğ‘¡(ğº(ğ‘¥))â€–ğ‘€ğ´ğ¸ ]

(A.30)

where ğ‘“ ğ‘“ ğ‘¡ is the Fast Fourier Transform, ğ‘¥ and ğº(ğ‘¥) are the target and
generated images. Note that this equation has been adapted to 3D from
the original version (Ma et al., 2020).

A.3.6. Laplacian, projection, orientation
Laplacian, projection, and orientation as well as content and style
(Appendix A.2.3) loss functions are used in Shen et al. (2021) and, as
mentioned above, are essential for their style transfer work from 2D to
3D.
The projection loss (Eq. (A.26)) is the squared error of the 2D
projection of the generated volume with the 2D image used as input.

A.3.9. Shape consistency and spatial
Shape consistency loss (Eq. (A.31)) is used by Zhang et al. (2019c,
2018), Cai et al. (2019) to ensure anatomical structure invariance of a
CycleGAN through the use of segmentation networks, i.e. if a scan of a
certain modality (e.g., MRI) has a certain semantic label, the generated
33

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

scan of the other modality (e.g., CT) must have the same semantic label.
The CE (Appendix A.3.3) is used to compare both segmentations.

A.4.2. Minkowski functional
The Minkowski functional is used in Chen et al. (2020) as a loss
function to measure the distance between geometric features of volumes. For an ğ‘›-dimensional space, there are ğ‘›+1 measurements, namely
porosity, specific surface area, average width and Eulerâ€™s number for
volumes (3D). This is used to assess if GANs captured correctly the
geometric information of the images. For this, a second discriminator
is trained to distinguish between Minkowski functional extracted from
real or generated volumes. Chen et al. (2020) approach is similar to
extract the features of medical images (using radiomics (Van Timmeren
et al., 2020) or like Pan et al. (2019) (Appendix A.2.2) who uses a
DL network to extract disease features) and use a discriminator to
distinguish between features extracted from real and generated images.
Minkowski functional is a precise technique for analysing geometric
structures that can also be used for biomedical image analysis (Depeursinge et al., 2014; Boehm et al., 2008) (the last explains these
measures in more detail).
The volume (ğ‘‰ ), surface area (ğ‘†), average width (ğ‘€) and Eulerâ€™s
number (ğ‘‹) are defined in Chen et al. (2020) by Eq. (A.35). The
porosity is defined by ğ‘‰ğ‘ âˆ•ğ‘‰ where ğ‘‰ğ‘ is the connected pore space
volume and ğ‘‰ the total volume.

ğ¿ğ‘ â„ğ‘ğ‘ğ‘’ (ğ‘†ğ´ , ğ‘†ğµ , ğºğ´ , ğºğµ ) =
1 âˆ‘ ğ‘–
ğ‘¦ ğ‘™ğ‘œğ‘”(ğ‘†ğ´ (ğºğ´ (ğ‘¥ğµ ))ğ‘– )]
ğ‘ ğ‘–=1 ğ´
ğ‘

Eğ‘¥ğµ âˆ¼ğ‘ğ‘‘ (ğ‘¥ğµ ) [âˆ’

+ Eğ‘¥ğ´ âˆ¼ğ‘ğ‘‘ (ğ‘¥ğ´ ) [âˆ’

ğ‘
1 âˆ‘

ğ‘ ğ‘–=1

(A.31)

ğ‘¦ğ‘–ğµ ğ‘™ğ‘œğ‘”(ğ‘†ğµ (ğºğµ (ğ‘¥ğ´ ))ğ‘– )]

where ğ‘¥ğ´ and ğ‘¥ğµ are the input scans of the modality ğ´ and ğµ, ğ‘¦ğ´
and ğ‘¦ğµ are the respective ground truth labels, ğ‘†ğ´ and ğ‘†ğµ the semantic
segmentation networks, ğºğ´ and ğºğµ the generators, and ğ´ and ğµ the
two modalities (CT and MRI).
The spatial loss (Fu et al., 2018) (Eq. (A.32)) is also related to the
cycle consistency loss used in Chen et al. (2021b), Han et al. (2019b),
Ho et al. (2019) and serves the same purpose, but the segmentation
network is trained to generate binary labels instead of semantic labels.
MSE is used instead of CE to compare the two segmentations. In their
experiments, a spatially constrained CycleGAN (SpCycleGAN) is used
where the input is the label (ğ‘¦) and the output is the synthetic volumetric nuclei (ğ‘¥) and vice versa in the other direction of the CycleGAN.
In this case, only ğ‘¦ is compared with the output of the segmentation
network and not both, as in the case of shape consistency.
1 âˆ‘ ğ‘–
â€–ğ‘¥ âˆ’ (ğ‘†(ğº(ğ‘¦))ğ‘– )â€–ğ‘€ğ‘†ğ¸ ]
ğ‘ ğ‘–=1

ğ‘‰ = ğ‘‰ (ğ‘¦)
ğ‘†=

ğ‘

ğ¿ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (ğ‘†, ğº) = Eğ‘¥âˆ¼ğ‘ğ‘‘ (ğ‘¥) [âˆ’

(A.32)

Shape consistency and Spatial losses are similar to identity (Appendix A.3.5) and feature-consistent A.2.2 losses, as they are used to
regulate the intermediate step of CycleGAN training. Therefore, one of
these approaches should be considered to avoid unrealistic generations
in the intermediate state of CycleGAN.

âˆ«ğœ•ğ‘¦

ğ‘‘ğ‘ 

ğ‘€=

1
1 1
[
+
]ğ‘‘ğ‘ 
âˆ«ğœ•ğ‘¦ 2 ğ‘Ÿ1 (ğ‘ ) ğ‘Ÿ2 (ğ‘ )

ğ‘‹=

1
ğ‘‘ğ‘ 
âˆ«ğœ•ğ‘¦ ğ‘Ÿ1 (ğ‘ )ğ‘Ÿ2 (ğ‘ )

(A.35)

where ğ‘Ÿ1 (ğ‘ ) and ğ‘Ÿ2 (ğ‘ ) represents the maximum and minimum curvature
on the surface, and ğ‘¦ the data (3D volume).
The ğ‘‰ , ğ‘†, ğ‘€ and ğ‘‹ can also be defined by Eq. (A.36) (Boehm et al.,
2008).
ğ‘‰ = ğ‘›ğ‘£ğ‘œğ‘¥ğ‘’ğ‘–ğ‘ 
ğ‘† = âˆ’6ğ‘‰ + 2ğ‘›ğ‘“ ğ‘ğ‘ğ‘’ğ‘ 

A.4. Other loss functions

ğ‘€ = 3ğ‘‰ âˆ’ 2ğ‘›ğ‘“ ğ‘ğ‘ğ‘’ğ‘  + ğ‘›ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘ 

(A.36)

A.4.1. Border and volume

ğ‘‹ = âˆ’ğ‘‰ + ğ‘›ğ‘“ ğ‘ğ‘ğ‘’ğ‘  âˆ’ ğ‘›ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  + ğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘’ğ‘ 

Implemented in Momeni et al. (2021), border and volume losses
are used to improve the quality of the MRI scans generated for their
GAN conditioned by volume and location. Volume losses (Eq. (A.33))
penalise the differences between the input volume (a numerical value)
and the volume of the cerebral microbleed generated. Border loss
(Eq. (A.34)) forces the edges of the generated scan to be 1, making
the generated microbleed blend with the surrounding tissue. These
losses are particularly important for this cGAN, which involves creating
lesions with specific volumes, and it is crucial that they blend correctly
into the surroundings. However, the border loss does not take into
account that the voxels next to the border may have random values, so
that discontinuity (no correct blending) may be possible. Such losses
are not necessary if the generated lesions are not intended to fuse with
the surrounding tissue or if the volume of the lesion is not important,
i.e. the generator is not conditioned by the volume of the lesion.

where ğ‘›ğ‘£ğ‘œğ‘¥ğ‘’ğ‘–ğ‘  , ğ‘›ğ‘“ ğ‘ğ‘ğ‘’ğ‘  , ğ‘›ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  ğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘’ğ‘  are the number of voxels, faces, edges
and vertices, respectively.
The Minkowski functional is more commonly used for evaluating
geometric features, but when the features it defines are important for
a realistic generation, the use of such metrics is recommended. Chen
et al. (2020) could also have chosen to compare the real and fake
features using other metrics, e.g. MAE, MSE or CE, instead of training a
discriminator, which would reduce the computational power required,
but the loss could not be as meaningful as using the discriminator.
However, no conclusion can be drawn as this approach has not been
researched in depth.

ğ‘– |
ğ‘ | ğ‘–
1 âˆ‘ || ğ‘‰ğ‘“ ğ‘ğ‘˜ğ‘’ âˆ’ ğ‘‰ğ‘–ğ‘› ||
ğ¿ğ‘œğ‘ ğ‘ ğ‘£ğ‘œğ‘™ğ‘¢ğ‘šğ‘’ =
|
|
ğ‘–
ğ‘ ğ‘–=1 |
ğ‘‰ğ‘–ğ‘›
|
|
|

Appendix B. Evaluation metrics

B.1. Generation task

(A.33)

B.1.1. Correlation coefficient (CC), normalised cross correlation (NCC),
pearson CC (PCC)
The CC and NCC (Kirch, 2008) measure the linear association of
two variables. They are used in Yang et al. (2021a), Baumgartner et al.
(2018), Harms et al. (2019), Lei et al. (2020b), Liu et al. (2020b)
to evaluate the quality of the generated volumes. The main problem
with CC is that it depends on the amplitude of the variables being
compared, i.e. the size of the signals, which can be a problem when
comparing scans from different instruments, for example. By using the

where ğ‘ is the number of samples in the dataset, ğ‘‰ğ‘“ğ‘– ğ‘ğ‘˜ğ‘’ and ğ‘‰ğ‘–ğ‘›ğ‘– are the
fake and real volumes.
1 âˆ‘ |
|
ğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ =
|1 âˆ’ ğº(ğ‘§)ğ‘–ğ‘—ğ‘˜ |
(A.34)
|
|
ğ¾
(ğ‘–,ğ‘—,ğ‘˜)âˆˆğµ

where ğµ are the voxels at the edge, ğº(ğ‘§)ğ‘–ğ‘—ğ‘˜ the generated lesion and ğ¾
the number of samples
34

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

normalised version, i.e. the NCC, this dependency is eliminated (Baumgartner et al., 2018). Its calculation is very fast and efficient, so it
is recommended for real-time systems. Therefore, the use of NCC is
highly recommended to statistically evaluate the agreement between
real and synthetically generated datasets. NCC is generally defined
by Eq. (B.37), and by Eq. (B.38) when adapted to volume.
âˆ‘
ğ‘â‹…ğ‘
ğ‘– ğ‘ğ‘– ğ‘ ğ‘–
ğ‘ğ¶ğ¶ = ğ‘ğ‘œğ‘ ğœƒ =
= âˆš
(B.37)
âˆ‘ 2 âˆšâˆ‘ 2
|ğ‘||ğ‘|
ğ‘– ğ‘ğ‘–
ğ‘– ğ‘ğ‘–

datasets from different sources, as is MAPE, which can be useful for
comparing synthetic data that is intended to have different scales from
the real dataset, e.g. generating PET scans from low-count PET, or CT
scans from low-dose CT. However, as mentioned in Appendix B.1.14,
other metrics must be used to ensure that the intensity of the voxels
matches the real tissue HU values.
âˆš
âˆ‘ğ‘›
Ì‚ 2
ğ‘–=1 (ğ‘¥ğ‘– âˆ’ ğ‘¥)
ğ‘…ğ‘€ğ‘†ğ¸ =
ğ‘›
(B.40)
ğ‘…ğ‘€ğ‘†ğ¸
ğ‘ğ‘…ğ‘€ğ‘†ğ¸ =
ğœ”
where ğ‘¥Ì‚ and ğ‘¥ are the output and ground truth, and ğœ” âˆˆ {ğ‘¥, ğ‘šğ‘’ğ‘ğ‘›, ğ‘ ğ‘¡ğ‘‘}
(depending on what
âˆš is to be normalised). NMSE is identical to NRMSE,
but without the â‹….

where ğœƒ is the angle between the two vectors (ğ‘ and ğ‘), and âˆ’1
corresponds to not correlated and 1 to highly correlated.
In the context of image comparison:
ğ‘›ğ‘› ğ‘›

ğ‘ğ¶ğ¶ =

ğ‘– ğ‘— ğ‘˜
âˆ‘ 1
1
[ğ‘¥
Ã— ğ‘¥Ì‚ ğ‘–,ğ‘—,ğ‘˜ ]
ğ‘›ğ‘– ğ‘›ğ‘— ğ‘›ğ‘˜ ğ‘–,ğ‘—,ğ‘˜ ğœğ‘¥ ğœğ‘¥Ì‚ ğ‘–,ğ‘—,ğ‘˜

(B.38)

B.1.5. Intersection-over-Union (IoU)
The IoU or Jaccard index (Taha and Hanbury, 2015) is used in Kniaz
et al. (2020), Yang et al. (2017), Liu et al. (2021) to measure the
intersection of two distributions over the union of the two distributions
(Eq. (B.41)). The goal of these works is to produce 3D volumes from
2D/2.5D images. The result is a voxel grid where each voxel is 1 or
0, which allows the calculation of such a metric. This metric cannot
be used to evaluate the generation of data where the intensities of the
voxels are not binary, e.g. when transferring from MRI to CT. Therefore,
this metric is only relevant when the output is a binary grid.

where ğ‘›ğ‘– , ğ‘›ğ‘— and ğ‘›ğ‘˜ the dimensions in the ğ‘¥, ğ‘¦ and ğ‘§ axis, i.e., ğ‘›ğ‘– Ã—ğ‘›ğ‘— Ã—ğ‘›ğ‘˜ is
the number of voxels, ğœğ‘¥ and ğœğ‘¥Ì‚ the standard deviation of each volume,
ğ‘¥ğ‘–,ğ‘—,ğ‘˜ and ğ‘¥Ì‚ ğ‘–,ğ‘—,ğ‘˜ the voxel values of each volume.
PCC is also the correlation between two datasets, such as CC and
NCC, and in the context of image comparison they are all equivalent,
but NCC is recommended in this context due to the magnitude invariance. PCC is mentioned in Schaefferkoetter et al. (2021) to evaluate the
downstream task.
We decided to include these metrics in the voxel-wise evaluation
section, since they utilise the values of each voxel to measure the linear
association between the real and fake data distributions.

ğ‘‡ğ‘ƒ
(B.41)
ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ + ğ¹ğ‘
where TP, FP and FN are True positives, false positives and false
negatives, respectively. This metric is formally adapted to volumes
by Eq. (B.42).
âˆ‘ ğ‘›ğ‘– ğ‘›ğ‘— ğ‘›ğ‘˜
(ğ‘¥Ì‚ â‹… ğ‘¥)
ğ‘–=1,ğ‘—=1,ğ‘˜=1
ğ½ ğ´ğ¶ = âˆ‘ğ‘› ğ‘› ğ‘›
(B.42)
ğ‘– ğ‘— ğ‘˜
(ğ‘¥Ì‚ + ğ‘¥)
ğ‘–=1,ğ‘—=1,ğ‘˜=1

ğ½ ğ´ğ¶ =

B.1.2. Cross entropy (CE)
CE has already been explained in Appendix A.3.3. Since this metric
is able to compare two distributions, it is a good metric to compare
the distribution of the real dataset and the generator. Lower values of
CE correspond to better results, i.e. a distribution that is closer to the
real distribution. It was used to assess the generated data in Yang et al.
(2017), Liu et al. (2021) and to evaluate the downstream task in Wang
et al. (2017).

where ğ‘¥Ì‚ is the generated volume, and ğ‘¥ the ground truth.
This is formally related with the Dice score (Appendix B.2.7) by
Eq. (B.43).
ğ·ğ¼ğ¶ğ¸
(B.43)
2 âˆ’ ğ·ğ¼ğ¶ğ¸
Both metrics measure sensibly the same, so there is no need to use
both metrics. As can be seen in this review, IoU is used to assess the
quality of the generated scans, and the Dice score for the downstream
task (frequently used to assess segmentations).
ğ½ ğ´ğ¶ =

B.1.3. MAE, mean absolute percentage error (MAPE)
The MAE is already explained in Appendix A.3.1. MAE is used
in Lei et al. (2020b), Rusak et al. (2020), Qin et al. (2021), Harms
et al. (2019), Yang et al. (2021b), Liu et al. (2020b), Zeng and Zheng
(2019), Hu et al. (2022), Pan et al. (2019) for the assessment step.
MAPE (Eq. (B.39)) is similar to the MAE, but the values are different
because MAPE is a percentage and the value is relative to the real
value (De Myttenaere et al., 2016). It is used in Han et al. (2019b)
to evaluate the results of the nuclei counter GAN-based model.
MAPE is better suited for comparisons between models, even if they
have been trained with different datasets, as the error is always relative
to that dataset. However, comparing such models is not good practice
because the different datasets are of different complexity. MAPE values
are also easier to understand than MAE values. Although MAE is more
commonly used, MAPE is also highly recommended.

where ğ‘¥Ì‚ and ğ‘¥ are the output and ground truth.

B.1.6. Peak signal-to-noise ratio (PSNR)
The PSNR (Eq. (B.44)) measures the proportion of desired signal
relative to background noise in decibels, and is commonly used for
volumetric medical imaging applications of GANs. This metric is often
used to assess the quality of enhanced images or reconstructions. Li
et al. (2020) is the only work in which the PSNR is used to evaluate
the downstream task and not the generation task, in contrast to Moghari
et al. (2019), Rusak et al. (2020), Ma et al. (2020), Harms et al. (2019),
Lei et al. (2020b), Gu and Zheng (2021), Yang et al. (2021b), Liu et al.
(2020b), Yang et al. (2019), Zeng and Zheng (2019), Hu et al. (2022),
Lin et al. (2021a), Pan et al. (2019), Tang et al. (2020), Yang et al.
(2021a).
ğ‘€ğ´ğ‘‹ğ¼
)
ğ‘ƒ ğ‘†ğ‘ğ‘… = 20 â‹… ğ‘™ğ‘œğ‘”10 ( âˆš
(B.44)
ğ‘€ğ‘†ğ¸

B.1.4. MSE, normalised MSE (NMSE), normalised root MSE (NRMSE)
The MSE is already explained in Appendix A.3.1. The MSE is used
in Rusak et al. (2020), Yang et al. (2019), Shen et al. (2021) to assess
the quality of the generated data and in Schaefferkoetter et al. (2021)
in the downstream task. The NMSE is a normalised version of the MSE
with respect to the signal intensity used in Moghari et al. (2019), Lei
et al. (2020b). The NRMSE (Eq. (B.40)) is a normalised version of
the Root MSE used in Yang et al. (2021a). Both NMSE and NRMSE
are suitable for comparing models with different scales, i.e. comparing

where MSE is defined in Eq. (A.20), and ğ‘€ğ´ğ‘‹ğ¼ the maximum possible
intensity of a voxel of the scan. PSNR metric if close to the inverse
of MSE (for PSNR the higher, the better), however, PSNR measure the
distance between signal intensities and not absolute values. Although
this metric is widely used, it does not correlate with human perception
of quality, so metrics such as SSIM and MS-SSIM should be used instead.
However, it is able to measure the degree of degradation of a scan,
which is important for super-resolution and reconstruction problems,
e.g. for the generation of high-resolution MRI scans from low-resolution
MRI scans to increase acquisition speed.

100% âˆ‘ ğ‘¥Ì‚ âˆ’ ğ‘¥
|
|
ğ‘› ğ‘–=1
ğ‘¥
ğ‘›

ğ‘€ğ´ğ‘ƒ ğ¸ =

(B.39)

35

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

metrics, e.g., FID or MS-SSIM, try to do so, but still fail in some specific
cases.
Normally, researchers perform a visual inspection when training
generative models, but experts in the field are needed to accurately
assess the quality of the generated data. The visual Turing test is a more
sophisticated visual assessment in which experts are asked to classify a
series of images as real or fake, without prior knowledge of the data.
The best result is a percentage of correct answers close to 50%, which
means that the experts were not able to make a distinction and therefore a random choice was made. Instead of a binary selection, a value
in a specific range can also be specified, e.g. between 0 and 5, where
0 is certainly fake and 5 is certainly real. After a reasonable number of
answers (which depends on the task) have been obtained from different
experts (a reasonable number would be e.g. 3), a confusion matrix can
be created and the accuracy, sensitivity, specificity, precision and other
metrics can be calculated.
As with MS-SSIM, it was expected that more papers would be found
using the visual Turing test (which was only used in Bu et al. (2021),
Han et al. (2019a)). Although visual assessment should always be
an essential approach for evaluating a GAN model, it is subjective.
Therefore, researchers opt to use objective metrics such as SSIM or
PSNR and evaluate the synthetic data applied to the downstream task.

B.1.7. Structural similarity index measure (SSIM), multi-scale structural
similarity index measure (MS-SSIM)
The SSIM (Wang et al., 2004) is widely used to extract the structural
information from digital images and videos, just like human visual
perception. SSIM (Eq. (B.46)) is then composed by three components:
luminance, contrast and structure (Eq. (B.45)). This metric differs from
MAE, MSE and PSNR because it does not only calculate the absolute
error, but also takes into account the dependencies between voxels,
especially when they are close to each other. These dependencies
contain important information about the structure of the evaluated
objects. Its operation is based on the physiology of human perception of
quality, which makes this metric very suitable for assessing the quality
of generated volumes. This metric needs the original volumes, i.e., the
ground truth, to make its comparisons. The comparison is made using
a sliding window approach with a window size chosen by the user
(e.g. 8 Ã— 8 Ã— 8). It is used in Moghari et al. (2019), Rusak et al. (2020),
Ma et al. (2020), Gu and Zheng (2021), Yang et al. (2021b), Lin et al.
(2021a), Pan et al. (2019), Yan et al. (2018), Tang et al. (2020), Yang
et al. (2021a).
Ì‚ =
ğ‘™(ğ‘‹, ğ‘‹)

2ğœ‡ğ‘‹ ğœ‡ğ‘‹Ì‚ + ğ¶1
2 + ğœ‡2 + ğ¶
ğœ‡ğ‘‹
1
Ì‚
ğ‘‹

Ì‚ =
ğ‘(ğ‘‹, ğ‘‹)

2ğœğ‘‹ ğœğ‘‹Ì‚ + ğ¶2

(B.45)

2 + ğœ2 + ğ¶
ğœğ‘‹
2
Ì‚

B.1.9. Clustering/t-distributed stochastic neighbour embedding (t-SNE)
Clustering is used to divide a population of data points into a certain
number of groups (Sammut and Webb, 2010b) specified by the user.
One of the most used algorithms for visualisation of high-dimensional
data distributions is the t-SNE (Van der Maaten and Hinton, 2008). It
is an important technique to visualise whether the distribution of the
generated data follows the real data distribution.
It was used in Muzahid et al. (2021) to test whether the class
distribution of the generated samples could be easily split, i.e. whether
the generator was able to learn to generate data from each class,
and in Han et al. (2019a) to assess whether the distribution of the
generator was similar to the distribution of the real data. t-SNE is then
a qualitative metric that does not provide an absolute value of the
quality of the synthetic data, but shows the user how the distribution of
the synthetic and real data overlap in an easy-to-understand 2D graph.
Therefore, this is recommended for any task of GANs, provided it is
advantageous to have a generator that follows the same distribution as
the real dataset, or when different classes are to be learned from the
GAN, e.g. in a cGAN.

ğ‘‹

Ì‚ =
ğ‘ (ğ‘‹, ğ‘‹)

ğœğ‘‹ ğ‘‹Ì‚ + ğ¶3
ğœğ‘‹ ğœğ‘‹Ì‚ + ğ¶3

where ğ‘‹ and ğ‘‹Ì‚ are two discrete non-negative signals aligned with each
other, i.e., in this case two volume patches extracted from the same
spatial location from the two volumes to be compared, ğœ‡ğ‘‹ , ğœğ‘‹ and ğœğ‘‹ ğ‘‹Ì‚
Ì‚ and ğ¶1 ,
the mean of ğ‘‹, variance of ğ‘‹ and the covariance of ğ‘‹ and ğ‘‹,
ğ¶2 and ğ¶3 are constants given by: ğ¶1 = (ğ¾1 ğ¿)2 , ğ¶2 = (ğ¾2 ğ¿)2 , ğ¶3 = ğ¶2 âˆ•2,
where ğ¿ is the dynamic range of the voxel values, and ğ¾1 , ğ¾2 â‰ª 1 two
scalar constants.
Ì‚ = [ğ‘™(ğ‘‹, ğ‘‹)]
Ì‚ ğ›¼ â‹… [ğ‘ (ğ‘‹, ğ‘‹)]
Ì‚ ğ›½ â‹… [ğ‘ (ğ‘‹, ğ‘‹)]
Ì‚ ğ›¾
ğ‘†ğ‘†ğ¼ğ‘€(ğ‘‹, ğ‘‹)

(B.46)

where the values of ğ›¼, ğ›½ and ğ›¾ depend on the relative importance of
luminance, contrast and structure, respectively. When ğ›¼ = ğ›½ = ğ›¾ = 1,
the SSIM is given by Eq. (B.47).
Ì‚ =
ğ‘†ğ‘†ğ¼ğ‘€(ğ‘‹, ğ‘‹)

(2ğœ‡ğ‘‹ ğœ‡ğ‘‹Ì‚ + ğ‘1 )(2ğ›¿ğ‘‹ ğ‘‹Ì‚ + ğ‘2 )
2 + ğœ‡ 2 + ğ‘ )(ğ›¿ 2 + ğ›¿ 2 + ğ‘ )
(ğœ‡ğ‘‹
1
2
ğ‘‹
Ì‚
Ì‚
ğ‘‹

(B.47)

ğ‘‹

MS-SSIM (Wang et al., 2003) is based on SSIM at multiple scales,
i.e. the input image is subsampled by a factor of 2 ğ‘€ times and the
similarity is calculated by Eq. (B.48). MS-SSIM has been shown to
perform better than the single scale SSIM approach, as it is more robust
to variations in viewing conditions. The only disadvantage compared to
SSIM is the speed of inference, though it is still very fast. However, it
is intriguing that this metric was only used in Hu et al. (2022). Both
metrics can be adapted to work with volumes and capture the semantic
perception of all three dimensions (i.e. depth features as well), making
MS-SSIM highly recommended for evaluating volumes in problems such
as reconstruction, as long as ground truth is available.

B.1.10. Semantic interpretability score (SIS), shape-score(S-score)
The SIS (Seitzer et al., 2018) and S-score (Zhang et al., 2018) are
inspired by the Inception score (Appendix B.1.11). SIS is the Dice
(Appendix B.2.7) overlap between the segmentation of a reconstructed
image using a pre-trained segmentation network and the ground truth
segmentation. The S-Score is the same, but uses a multiclass dice
score instead. These metrics assume that scans with better visual quality, e.g. with defined boundaries and fewer distortions, are better
segmented by the pre-trained model. These are similar to IS, FID,
perceptual, latent vector and feature consistency losses in that they use
a pre-trained network to evaluate the generated images.
However, as mentioned in Section 3.1.2, the perceptual loss as well
as IS and FID should be avoided as they use a model trained on a dataset
consisting of different classes than those used to train the GAN. SIS and
S-score are particularly interesting because they are trained on the same
dataset used to train the GAN. Therefore, the segmentation networks
have been trained to extract the correct features for segmentation
in that specific data distribution. However, it is not guaranteed that
good results correspond to high realism, as the generator may learn
to accurately produce only the part to be segmented and neglect the
remaining part. Nevertheless, SIS and S-score are preferable to IS and

Ì‚ =
ğ‘€ğ‘†ğ‘†ğ‘†ğ¼ğ‘€(ğ‘‹, ğ‘‹)
ğ›¼

Ì‚ ğ‘€ â‹…
[ğ‘™ğ‘š (ğ‘‹, ğ‘‹)]

ğ‘€
âˆ
Ì‚ ğ›½ğ‘— â‹… [ğ‘ ğ‘— (ğ‘‹, ğ‘‹)]
Ì‚ ğ›¾ğ‘—
[ğ‘ğ‘— (ğ‘‹, ğ‘‹)]

(B.48)

ğ‘—=1

where ğ‘€ is the highest scale, i.e., smaller volume, and ğ‘ğ‘— (ğ‘‹, ğ‘ŒÌ‚ ) and
Ì‚ the contrast and structure comparison at the ğ‘—th scale
ğ‘ ğ‘— (ğ‘‹, ğ‘‹)
B.1.8. Visual
Visual assessment is the most commonly used evaluation to obtain
a human perception of the images/scans generated. So far, there is
no metric that can truly replace human perception, even though some
36

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

FID. The authors claim that these metrics are more related to human
judgement than SSIM and PSNR, and better scores correspond to less
geometric distortion. SIS is only used by Ma et al. (2020), and the
S-score is used by Zhang et al. (2019c, 2018), Cai et al. (2019).

not able to detect mode collapse or overfitting. Other metrics must be
used for this, e.g. MS-SSIM and visual assessment are always the most
reliable choice, specially for volumetric data.
The IS (Salimans et al., 2016) is used in Pesaranghader et al. (2021).
The FID is used in Pesaranghader et al. (2021), Dikici et al. (2021), Jung
et al. (2020), Hu et al. (2022), Li et al. (2019).

B.1.11. FrÃ©chet inception distance (FID), inception score (IS)
The IS is known to correlate with human judgement (Salimans et al.,
2016). To calculate this metric, first the Inception v3 model (Szegedy
et al., 2016) trained on the ImageNet dataset is needed. The generated
images are fed into the model, which gives a score as output, i.e., the
label distribution (ğ‘(ğ‘¦|ğ‘¥), where ğ‘¦ is a set of labels and ğ‘¥ the image). A
high score means that the images are varied and look realistic, i.e. they
resemble other images. The inception model assigns the images to a
certain class, i.e. it is a classifier. For a synthetic image to receive
a high score, it must be clearly assigned to a certain class. A good
score means that the collection of synthetic images has low entropy,
i.e., all the images are strongly assigned to a specific class. After ğ‘(ğ‘¦|ğ‘¥)
is determined, the KL divergence for all generated images is calculated
and the IS is determined using Eq. (B.49).
ğ¼ğ‘† = ğ‘’Eğ‘¥ ğ¾ğ¿(ğ‘(ğ‘¦|ğ‘¥)âˆ¥ğ‘(ğ‘¦))
ğ¾ğ¿ = ğ‘(ğ‘¦|ğ‘¥) â‹… (ğ‘™ğ‘œğ‘”(ğ‘(ğ‘¦|ğ‘¥)) âˆ’ ğ‘™ğ‘œğ‘”(ğ‘(ğ‘¦)))

B.1.12. Absolute permeability, Minkowski functional
Absolute permeability is used in Krutko et al. (2019) to assess
the quality of generated porous media volumes. It is usually used in
conjunction with Minkowski functional. This metric is calculated using
a pore network model, which is not explained in detail in the paper, but
it suggests that it is a computer simulator that accepts 3D volumes of
porous media consisting of voxels as input. The Minkowski functional
is already explained in Appendix A.4.2. Chen et al. (2020), Krutko et al.
(2019), Mosser et al. (2017) use it in the evaluation step. These metrics
measure important characteristics of porous media that must be learned
by the GAN in order to reconstruct volumes that are as realistic as
possible. Therefore, in these cases, using these metrics to evaluate and
compare the trained GANs is the best approach.

(B.49)

B.1.13. Compliance, fraction of unmachinable voxels
The compliance and fraction of unmachinable voxels are used
in Greminger (2020) to measure the difference between machinable
designs generated by GANs versus the traditional topology optimisation
algorithm. Compliance roughly stands for the inverse of stiffness, so
the goal of Greminger (2020) is to archive designs with minimum
compliance, i.e. a design that maximises stiffness for a given target mass
of the object. The minimum compliance for the GAN-based solution is
given by Eq. (B.51). For the sake of brevity, please refer to Greminger
(2020), where it is explained in more detail how the compliance is
calculated.
ğ‘‰ (ğº(ğ‘§))
1
ğ‘(ğ‘§) = ğ‘¢ğ‘‡ ğ¾(ğ‘’)ğ‘¢,
= ğ‘£ğ‘“
(B.51)
2
ğ‘‰0

where âˆ¥ is the divergence between two distributions, which in this case
is calculated by the KL divergence, and ğ‘(ğ‘¦) the marginal distribution.
FID is a metric widely used to compare the distribution of real
images with images generated by GANs. Introduced by Heusel et al.
(2017), who claims that the FID metric captures the similarity between
generated images and real images better than IS, as IS does not compare
the statistics of real and synthetic data, making it the most commonly
used metric to assess the quality of GANs (for 2D).
FID uses the feature vector before calculating the probabilities
(before the last layer of the inception model). These activation values
(the feature vector) are then summarised as a multivariate Gaussian
with mean and covariance. These statistics are calculated for all images
in the real and generated datasets, and finally the FrÃ©chet distance
(Wasserstein-2 distance) between the two distributions is calculated
(Eq. (B.50)).
ğ‘‘ 2 ((ğ‘š, ğ¶), (ğ‘šğ‘¤ , ğ¶ğ‘¤ )) = â€–ğ‘š âˆ’ ğ‘šğ‘¤ â€–22 + ğ‘‡ ğ‘Ÿ(ğ¶ + ğ¶ğ‘¤ âˆ’ 2(ğ¶ğ¶ğ‘Š )1âˆ•2 )

where ğ‘§ is the input latent vector of the generator, ğ‘¢ is the vector
of nodal displacements of the finite element model, ğ¾(ğ‘’) the global
stiffness matrix, ğ‘’ the vector of element modulus values calculated from
ğ‘§, ğ‘‰ (ğº(ğ‘§)) the volume of the generated model, ğ‘‰0 the volume of the
model domain, and ğ‘£ğ‘“ the target volume fraction.
The fraction of unmachinable voxels is the fraction of voxels that
cannot be reached in the machining process. The volumes produced
are to be machined with a 3-axis vertical milling machine with a single
setup, and it is imperative that each voxel is reachable in this process so
that their production is possible. The proportion of voxels that cannot
be machined is therefore given by the number of voxels that cannot be
reached/total number of voxels.
These metrics are very specific to this problem, but they are highly
recommended for any related task that physically involves the production of such volumes. The use of metrics that express geometric features
and physical constraints are essential for evaluating the volumes produced. For example, it is not useful to use these metrics to access the
results of an image translation task (e.g. CT to MRI).

(B.50)

where ğ‘(â‹…) and ğ‘ğ‘¤ (â‹…) are the probabilities of real world data and generated data, ğ‘š, ğ¶ the mean and covariance from ğ‘(â‹…) and ğ‘šğ‘¤ , ğ¶ğ‘¤ from
ğ‘ğ‘¤ (â‹…), ğ‘‡ ğ‘Ÿ is the trace of the matrix (i.e., sum of its diagonal entries from
upper left to down right), and â€–ğ‘š âˆ’ ğ‘šğ‘¤ â€–22 the square of the difference
of the two matrices.
Heusel et al. (2017) shows that FID is consistent with human
judgement and with the increase of disturbances on the original images,
but it is not commonly used for volumetric assessment, nor is IS. For its
calculation, the data must be 2D, i.e. volumetric data cannot be used
because the inception model is trained on 2D data. In order for them
to be used, the volumes are usually sliced in all three axes to allow the
use of the FID, but this practice is not well accepted and therefore not
widely used.
These two metrics have some more problems that have already been
mentioned in the 3.1.2 section. They assume that the inception model is
able to correctly classify any existing image. However, in cases where
a class is to be generated that is not represented in the dataset used
to train the inception model, the score will always be low, even if the
generated images are of good quality, i.e., the dataset used to train the
GAN must contain the same or fewer classes than the dataset used to
train the inception model, with no difference between these classes.
Therefore, the use of such metrics to assess the quality of synthetic
medical data should be avoided.
Since the inception model is a CNN classifier, the classification may
rely only on texture and low-level features, rather than also relying on
high-level features such as whole objects, which may result in a high
score for images that are clearly not realistic. These metrics are also

B.1.14. Hounsfield unit (HU), spatial non-uniformity (SNU)
HU (Kamalian et al., 2016) is a relative scale to measure the
attenuation of the X-ray beam in a given voxel, calculated by Eq. (B.52).
HU is approximately linearly proportional to the physical density of the
material (with the exception of some substances such as iodine), but
relative to the intensity of the X-rays. Therefore, in contrast to MRI,
HU describes the actual density of the object under investigation, be it
a porous media or a human organ, e.g. bones have higher HU values
than fat, and white matter lower values than grey matter.
ğœ‡
âˆ’ ğœ‡ğ‘¤ğ‘ğ‘¡ğ‘’ğ‘Ÿ
ğ»ğ‘ˆ = ğ‘šğ‘ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘ğ‘™
âˆ— 1000
(B.52)
ğœ‡ğ‘¤ğ‘ğ‘¡ğ‘’ğ‘Ÿ
37

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

where ğœ‡ corresponds to the attenuation of the value of the X-ray bean
in a given material, i.e., in a given voxel. Higher HU values appear
brighter and lower HU values appear darker.
This is why calculating the difference between a real image and
a reconstructed image using these values is an important method for
evaluating the quality of the generated CT data. Liu et al. (2020b) want
to generate synthetic CT scans based on MRI scans. Therefore, the HU
values must represent the real properties of each tissue, so the MAE
value between the real CT scans and the synthetic CT scans is calculated
to evaluate the realism of the data. The histogram of HU values is also
used, which shows a good overlap of the values. This work has shown
that GANs can be used to predict the HU values of tissues, which can
further increase the speed of treatments.
HU is also used to evaluate the synthetic CT scans in Zhang et al.
(2021), which computes the error (mean and standard deviation) of
each organ between the real value and the value of the generated
images.
The SNU is nothing more than the difference of HU in the same
material, i.e., the difference of HU of a specific tissue between the
generated scans, mentioned in Harms et al. (2019).
These metrics are no different from other voxel-wise comparisons,
but the authors have placed a higher value on them given their importance for CT reconstruction.

These metrics are mainly used to evaluate other downstream tasks
instead of the generation task. Accuracy is measured in Pesaranghader
et al. (2021), Jung et al. (2020), Zhuang and Schwing (2019), Wei et al.
(2020), Yan et al. (2018), Muzahid et al. (2021), Wang et al. (2017),
Chen et al. (2021b), Liu et al. (2020a).
Sensitivity, also known as recall, is measured in Bu et al. (2021),
Momeni et al. (2021), Zhuang and Schwing (2019), Lei et al. (2020a),
Chen et al. (2021b), Ho et al. (2019), Han et al. (2019b), Baniukiewicz
et al. (2019). Specificity is measured only in Lei et al. (2020a).
Precision, also known as positive predictive valve, is measured
in Zhuang and Schwing (2019), Chen et al. (2021b), Ho et al. (2019),
Han et al. (2019b), Baniukiewicz et al. (2019).
The AFP metric is used in Dikici et al. (2021), which is the average
of false positives metastases detected per patient.
Type-I and Type-II errors, also known as false positive and false
negative errors, are used in Chen et al. (2021b). It can be also be used
as 1 âˆ’ ğ‘†ğ‘ğ‘’ and 1 âˆ’ ğ‘†ğ‘’ğ‘›, respectively. Type-I error is rejecting the null
hypothesis when this is actually true, and the Type-II is accepting the
opposite, i.e., accepting the null hypothesis when this is actually false.
ğ‘‡ğ‘ƒ + ğ‘‡ğ‘
ğ‘‡ğ‘ƒ + ğ‘‡ğ‘ + ğ¹ğ‘ƒ + ğ¹ğ‘
ğ‘‡ğ‘ƒ
ğ‘†ğ‘’ğ‘› =
ğ‘‡ğ‘ƒ + ğ¹ğ‘
(B.53)
ğ‘‡ğ‘
ğ‘†ğ‘ğ‘’ =
ğ‘‡ğ‘ + ğ¹ğ‘ƒ
ğ‘‡ğ‘ƒ
ğ‘ƒ ğ‘Ÿğ‘’ =
ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ
where TP, TN, FP and FN are true positive, true negative, false positive
and false negative respectively.
ğ´ğ‘ğ‘ =

B.1.15. Reconstruction resolution
The reconstruction resolution is used to assess the quality of reconstruction of single particles from noisy projections. This metric is very
specific to the work of Gupta et al. (2021), who uses a cryo-EM physics
simulator as a generator, and depends on the noise level of the input.
If this simulator is not used, this metric is not relevant.

B.2.2. Area under the curve (AUC), receiver operating characteristic curve
(ROC)
AUC (Sammut and Webb, 2010a) is the area under the ROC curve
1u (Flach, 2010) and is intrinsically related to the true positive rate
(TPR, also known as ğ‘†ğ‘’ğ‘›) and the false positive rate (FPR, 1 âˆ’ ğ‘†ğ‘ğ‘’ or
Eq. (B.54)). The ROC curve is the graph of TPR on the ğ‘¦-axis and FPR on
the ğ‘¥-axis, to evaluate binary classifications. A high AUC value means
that the model can distinguish well between positive and negative
classes. If the value is close to 0.5, the model is not able to distinguish
between them.
The AUC is used in Momeni et al. (2021), Pan et al. (2019),
Yan et al. (2018). This metric is also used to indirectly assess the
quality of the volumes generated, by measuring the performance of
the downstream task where synthetic data was used, e.g., a classifier
trained with synthetic data.

B.1.16. Volume fractions, triple phase boundary/double phase boundary
(TPB/DPB) densities, relative surface area, relative diffusivity, surface area,
two-point correlation function (TPCF)
All these metrics are used for microstructural characterisation. The
average volume fractions, volume fraction variations, and TPB/DPB
densities are measurements used in Sciazko et al. (2021) to evaluate
the quality of 3D fuel cell electrode microstructures. Volume fraction, relative surface area and relative diffusivity are measurements
used in Kench and Cooper (2021) to evaluate the quality of various
microstructures, including a battery cathode. The phase volume fraction, relative diffusivity, specific surface area, TPCF and TPB are used
in Gayon-Lombardo et al. (2020) to measure the quality of electrode
microstructures.
These are just characteristics inherent to the volume they intend
to generate synthetically, not really comparison metrics. These cases
are important to mention because they use a specific characteristic of
the object to assess the quality of the generated volumes, instead of
using just voxel-wise metrics. Thus, the mean and variance of these
characteristics are compared between the real and generated data using
graphs, where a higher overlap corresponds to a higher degree of
fidelity of the synthetic data compared to the real data.

ğ¹ğ‘ƒğ‘… =

ğ¹ğ‘ƒ
ğ¹ğ‘ƒ + ğ‘‡ğ‘

(B.54)

B.2.3. Attenuation correction (AC)
Attenuation in PET scans is the loss of detection due to absorption
caused by photonâ€“tissue interactions in the body and scattering outside the detector field, resulting in artefacts. The attenuation is given
by Eq. (B.55).

B.2. Downstream task

ğ¼
= ğ‘’âˆ’ğœ‡ğ¿
ğ¼0

B.2.1. Accuracy, sensitivity, specificity, precision, average false positives
(AFP), Type-I/Type-II errors
The metrics accuracy, sensitivity, specificity (Baratloo et al., 2015),
precision (Ting, 2010b) and AFP are often used in machine learning
(Eq. (B.53)). To calculate these metrics, a confusion matrix (Ting,
2010a) is usually created. The metrics used should be chosen carefully
depending on the problem, as high performance on one of these metrics
does not necessarily mean a good model, e.g. when classifying a patient
as positive or negative for a particular disease, high sensitivity is
preferable to high specificity, as it is usually better to classify someone
as sick who does not actually have a disease than the opposite.

where ğ¼ and ğ¼0 are the unattenuated and attenuated PET signals, ğœ‡
and ğ¿ the linear attenuation coefficient (LAC) and thickness of the
tissue. The LAC (also known as ğœ‡ map) is calculated using other imaging
sources such as CT scans where the HU values are transformed into PET
LAC values to perform AC (Carney et al., 2006).
AC can also be performed with MRI scans, but using MRI has
problems with certain body structures, e.g. the bones (Chen and An,
2017). Schaefferkoetter et al. (2021) use synthetic CT scans generated
from MRI scans to perform AC and compare them with MRI-based
correction, i.e. the relative differences between the ğœ‡ maps generated
with PET/CT, PET/MRI and PET/synCT (the synthetically generated
38

(B.55)

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

scans). The purpose of this comparison is not to evaluate the generation
itself, but how significantly the synthetic CT scans were able to improve
the ğœ‡ maps and the corresponding PET scans compared to the use of
MRI scans.

as explained in Appendix B.1.5. The F1-score is used when the focus
is on maximising both precision and recall, being a harmonic mean
between both. The F1-score is mentioned in Chen et al. (2021b), Ho
et al. (2019), Han et al. (2019b), Baniukiewicz et al. (2019), Liu et al.
(2020a). The macro F1-score is used in Zhuang and Schwing (2019).
The macro F1-score is a variation of the F1-score, where the macro F1score is the average of the F1-scores of the individual classes. The DSC
is used in Zhang et al. (2021), Rusak et al. (2020), Ma et al. (2020), Li
et al. (2020), Lei et al. (2020a), Zhang et al. (2019c, 2018), Cai et al.
(2019), Chen et al. (2021b). Li et al. (2020) is a rare example of a work
that uses this metric to evaluate the quality of the generatorâ€™s output,
but these metrics are mainly used to evaluate other downstream tasks,
such as hippocampal subfield segmentation (Ma et al., 2020).

B.2.4. Average Hausdorff distance (HD) and average surface HD
The average HD (Eq. (B.56)) and the average surface HD (Eq. (B.57))
are used in Zhang et al. (2021), Lei et al. (2020a) to measure the
distance between voxel sets in ground truth and prediction (Kazemifar
et al., 2018). The HD is the greatest distance between a voxel in one
volume and the closest voxel in the other volume in millimetres.
Ì‚ =
ğ´ğ»ğ·(ğ‘‹, ğ‘‹)
1 âˆ‘
1 âˆ‘
ğ‘šğ‘ğ‘¥(
ğ‘šğ‘–ğ‘›ğ‘¥âˆˆ
Ì‚
ğ‘šğ‘–ğ‘›ğ‘¥âˆˆğ‘‹ ğ‘‘(ğ‘¥,
Ì‚ ğ‘¥))
Ì‚ ğ‘‹Ì‚ ğ‘‘(ğ‘¥, ğ‘¥),
Ì‚
|ğ‘‹| ğ‘¥âˆˆğ‘‹
|ğ‘‹|
Ì‚

(B.56)

ğ‘¥âˆˆ
Ì‚ ğ‘‹

Ì‚ =
ğ´ğ‘†ğ»ğ·(ğ‘‹, ğ‘‹)
1 1 âˆ‘
1 âˆ‘
ğ‘šğ‘–ğ‘›ğ‘¥âˆˆğ‘‹ ğ‘‘(ğ‘¥,
Ì‚ ğ‘¥))
(
ğ‘šğ‘–ğ‘›ğ‘¥âˆˆ
Ì‚ +
Ì‚ ğ‘‹Ì‚ ğ‘‘(ğ‘¥, ğ‘¥)
Ì‚ ğ‘¥âˆˆğ‘Œ
2 |ğ‘‹|
|
ğ‘‹|
Ì‚
Ì‚

ğ·ğ‘†ğ¶ =
(B.57)

Ì‚
2|ğ‘‹ âˆ© ğ‘‹|
Ì‚
|ğ‘‹| + |ğ‘‹|

(B.58)

where ğ‘‹ and ğ‘‹Ì‚ are the two sets of data to be compared.

ğ‘¥âˆˆğ‘‹

ğ¹1 =

where ğ‘‹ and ğ‘‹Ì‚ are the voxel sets of the ground-truth and generated
segmentation, and ğ‘‘(ğ‘¥, ğ‘¥)
Ì‚ the Euclidean distance from voxel ğ‘¥ to voxel
ğ‘¥.
Ì‚
For example, in Zhang et al. (2021), the average HD and surface
HD are used to evaluate the performance of prostate and at-risk organ
segmentation, but not the generation task. As mentioned earlier, the
better performance using synthetic data compared to not using such
data suggests that the synthetic data is of good quality and provides an
advantage for other deep learning algorithms.

2 Ã— ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› Ã— ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

(B.59)

where precision and recall are calculated as defined in Appendix B.2.1.
ğ·ğ‘†ğ¶ = ğ¹1 =

2ğ‘‡ ğ‘ƒ
2ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ + ğ¹ ğ‘

(B.60)

where TP, FP and FN represent true positive, false positive and false
negative respectively.

B.2.8. Noise SD
B.2.5. Centre of mass distance (CMD), percentage volume difference (PVD),
mean surface distance (MSD), residual mean square distance (RMSD)
All these metrics are used in Lei et al. (2020a). CMD and PVD are
used to measure the distance between the centre distances and volume
differences of the automatic and manual segmentation contours. The
MSD is used to measure the surface distances of the two contours.
The RMSD is similar to the MSE but is calculated between distances
rather than absolute voxel values, just as the MSD is similar to the
MAE but compares distances between surfaces rather than voxel intensities. CMD is simply the position difference between the real and
generated segmentation centroid. CMD, MSD and RMSD are measured
in millimetres.

The noise SD is used in Li et al. (2020) to measure background noise
in order to compare different methods of noise reduction. In this case,
one GAN was used to generate synthetic noise and another GAN was
used to noise reduction, but no metric was used to evaluate the first,
only the second. Two denoising models were trained, one with Gaussian
noise and the other with noise generated by the GAN. Lower noise SD
scores were obtained with the second denoiser, suggesting that using a
GAN that learns how to generate noise is a good strategy to increase
the amount of data for training deep learning denoisers.

B.2.9. Tumour localisation error
Tumour localisation error is used in Wei et al. (2020) to evaluate
the performance of the proposed tumour localisation method. Wei et al.
(2020) believe that their CNN-based tumour localisation method does
not perform better due to the discrepancy between the intensity of the
digitally reconstructed radiographs from CT scans and the measured
x-ray projections. To solve this problem, cone beam computed tomography (CBCT) scans are used, but not all patients have both CT and
CBCT scans. Therefore, a GAN is trained to produce synthetic CBCT
scans from CT. Better tumour localisation performance using synthetic
CBCT scans suggests that the generated data is of good quality for
the proposed downstream task. The tumour localisation error is the
absolute difference between the actual and the predicted values.

B.2.6. Competition performance metric (CPM)
The CPM (Niemeijer et al., 2010) is the average sensitivity at
seven predefined false positive rates: 1âˆ•8, 1âˆ•4, 1âˆ•2, 1, 2, 4 and 8
false positives per scan. The sensitivity is explained in Appendix B.2.1.
CPM reduces the free-response receiver operating characteristic (FROC)
to single numbers. The FROC is very similar to the ROC curve (see
Appendix B.2.2), where the sensitivity is on the ğ‘¦-axis and the average
number of false positives per volume is on the ğ‘¥-axis. Bu et al. (2021),
Han et al. (2019a) use this metric to compare the results with the
lung nodule analysis (LUNA16) challenge (Setio et al., 2017). When
better performance is achieved by using synthetic data, it means that
the synthetic data is of good quality and adds important information
that is captured by other deep learning algorithms.
B.2.7. Dice similarity coefficient (DSC), F1-score
The DSC (Taha and Hanbury, 2015) (Eq. (B.58)) and F1-score (Sammut and Webb, 2010c) (Eq. (B.59)) are two metrics that measure the
similarity between two samples. Both are equivalent when applied to
Boolean data (Eq. (B.60)). DSC is also known as the overlap index,
and is the most commonly used metric for evaluating segmentation in
volumetric medical imaging. This term is usually used in the context
of computer vision. DSC is directly proportional to the Jaccard index,

Appendix C. Applications

This section summarises all works in two different tables, sorted by
application. Table C.1 contains all medical papers, sorted by application
and organ, and Table C.2 contains all non-medical papers, also sorted
by application and structure. Animal and cell studies were considered
medical applications.
39

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Table C.1
Compact overview of reviewed medical papers sorted by application, then by modality and year. The application, the studied organ and the
highlights/notes are the main purpose of this table.
Application

Modality

Study

Network

Organ

Notes

CT

Bu et al.
(2021)

cGAN (3D-UNet/ 3D
CNN)

Lung

Boost the overall
performance of the lung
nodule detection network
by 2.53%

Classification

CT

Pesaranghader et al.
(2021)

CT-SGAN

Lung

Pretraining the nodule
detection classifier on
synthetic volumes and
fine-tuning on the real data

Classification

CT

Han et al.
(2019a)

MCGAN (U-Net/
Pix2Pix)

Lung

Generating lung nodules
using bounding box and
adding surrounding tissues

Classification

Microscopy

Baniukiewicz
et al. (2019)

cGAN

Cells

When 3D label is not
available, create pseudo-3D
synthetic cell data from
individually generated 2D
slices

Classification

MRI

Dikici et al.
(2021)

cGANe (DCGAN
based)

Brain

Provide a novel
data-sharing protocol

Classification

MRI

Momeni
et al. (2021)

conditional
LesionGAN (cGAN
based)

Brain

It can be applied on
unseen dataset with
different MRI parameters
and diseases

Classification

MRI

Jung et al.
(2020)

cGAN

Brain

The classification task
improved

Classification

MRI

Zhuang and
Schwing
(2019)

ICW-GAN

Brain

It showed that not all data
augmentation methods are
equally beneficial

Denoising

OCT

Li et al.
(2020)

GAN (N/D)

Brain

This provided more data
for noise reduction training

Image
translation

CBCT to CT

Qin et al.
(2021)

GAN (ResidualUNet/CNN)

Lung

Investigated the dose
validation accuracy

Image
translation

CBCT to CT
(corrected
CBCT)

Harms et al.
(2019)

res-cycle GAN

Brain, Pelvis

CBCT allows for daily 3D
imaging, for enhanced
image-guided radiation
therapy

Image
translation

CT to CBCT

Wei et al.
(2020)

cGAN

Lung

Handle the discrepancy
between the DRR from CT
and an x-ray projection

Image
translation

CT and Low
count PET to
full count
PET

Lei et al.
(2020b)

CycleGAN

Whole Body

This technique could be a
useful tool for low dose
PET imaging

Image
translation

CT to MRI

Yang et al.
(2021b)

CAE-ACGAN

Brain

Multi-contrast MR
synthesis

Image
translation

CT to MRI
and vice
versa

Gu and
Zheng (2021)

Dual3D&PatchGAN
(3DGAN based)

Brain

Creation of a model using
transfer learning approach

Image
translation

CT to MRI

Lei et al.
(2020a)

CycleGAN

Prostate,
Bladder,
Rectum

Facilitate routine
prostate-cancer
radiotherapy treatment
planning

Image
translation

CT to MRI
and vice
versa

Cai et al.
(2019)

CycleGAN
(cGAN/Patch-GAN)

Heart,
Pancreas,
Breast

Cross-modality to improve
segmentation of multiple
data sources in an online
manner

Image
translation

CT to MRI
and vice
versa

Zhang et al.
(2018,
2019c)

CycleGAN

Heart

Multiclass segmentation
using online synthetic data
generation

Image
translation

MRI to CT

Schaefferkoetter et al.
(2021)

CycleGAN (ResidualUNet/patchGAN)

Whole Body

The synthetic CT data used
for PET attenuation
correction

Classification

(continued on next page)

40

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Table C.1 (continued).
Application

Modality

Study

Network

Organ

Notes

Image
translation

MRI to CT

Liu et al.
(2020b)

dense CycleGAN

Liver

Use MRI-only photon or
proton radiotherapy
treatment planning

Image
translation

MRI to CT

Zeng and
Zheng (2019)

hGAN (CycleGAN
based)

Brain

Less data and
computational resources
needed due to hybrid
approach

Image
translation

MRI to PET

Hu et al.
(2022)

BMGAN(DenseUNet/Patch-Level)

Brain

Preserves the diverse brain
structure details

Image
translation

MRI to PET
and vice
versa

Lin et al.
(2021a)

RevGAN

Brain

It uses only one
bidirectional generator

Image
translation

MRI to PET

Pan et al.
(2019)

FGAN

Brain

Achieves the
state-of-the-art
performance in AD
identification and MCI
conversion prediction

Image
translation

MRI to PET

Yan et al.
(2018)

cGAN

Brain

The MCI progression
classification improved 7%

Nuclei
counting

Microscopy

Han et al.
(2019b)

SpCycleGAN

Cells
(Kidney)

The method is capable of
counting nuclei in 3D

Reconstruction
(2D ->3D)

Microscopy

Gupta et al.
(2021)

CryoGAN (cryo-EM
physics simulator)

Biomolecule

CryoGAN currently
achieves a 10.8 Ã…
resolution on a realistic
synthetic dataset

Reconstruction
(2D ->3D)

Synthetic

Yang et al.
(2021a)

CycleGAN based
(CNN
based/multi-scale)

Cells
(Embryos)

3D multi-view
deconvolution and fusion
using semi- and
self-supervised networks

Reconstruction
(Low ->High)

CT (CTP)

Moghari
et al. (2019)

cGAN

Brain

This method could allow
dose reduction in CT
Perfusion

Segmentation

CT

Zhang et al.
(2021)

PGGAN

Pelvis

Semi-supervised learning
for semantic segmentation

Segmentation

Microscopy

Chen et al.
(2021b)

SpCycleGAN

Cells
(Kidney)

The non-ellipsoidal nuclei
approach achieves
improved segmentation on
volumes with irregularly
shaped nuclei

Segmentation

Microscopy

Ho et al.
(2019)

SpCycleGAN

Cells
(Kidney)

It can segment nuclei
visually and numerically

Segmentation

Microscopy

Tang et al.
(2020)

cGAN

Cells
(Neuron)

It improves the
performance of neuron
segmentation

Segmentation

MRI

Ma et al.
(2020)

da-GAN

Brain

It can improve
Hippocampal subfields
segmentation accuracy

Segmentation

MRI and CT

Yang et al.
(2019)

N/D (3D-Unet/3D
CNN)

Heart

Helps to achieve better
understanding
cardiovascular motion

General

CT

Xu et al.
(2019)

MCGAN

Lung

More vigorous study is
needed to verify

General

MRI

Rusak et al.
(2020)

GAN (Pix2Pix
based)

Brain

Generate brain MRI with
accurate borders between
tissue

General

MRI

Baumgartner
et al. (2018)

VA-GAN (WGAN
based 3D
U-Net/C3D)

Brain

It can capture multiple
regions affected by disease

General

Synthetic

Danu et al.
(2019)

GAN (N/D)

Cells (Blood
vessels)

The synthetic blood vessels
are indistinguishable from
the real ones

41

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Table C.2
Compact overview of reviewed non-medical papers sorted by application, then by modality and year. The application and the studied structure
are the main purpose of this table.
Application

Modality

Study

Network

Structure

CAD

Muzahid
et al. (2021)

Progressive Conditional
GAN (PGGAN/cGAN
based)

Diversified
objects

Classification

CAD

Greminger
(2020)

MSG-GAN

Models to be
manufactured

Classification

Seismic
reflection
data

Liu et al.
(2020a)

semi-supervised GANs

Seismic facies

Reconstruction (2D
->3D)

CAD

Kniaz et al.
(2020)

Z-GAN (pix2pix based)

Cars

Reconstruction (2D
->3D)

CAD

Li et al.
(2019)

MP-GAN

Diversified
objects

Reconstruction (2D
->3D)

CAD

Yang et al.
(2017)

3D-RecGAN

Diversified
objects

Reconstruction (2D
->3D)

CT

Krutko et al.
(2019)

SPGAN (3D DCGAN based)

Porous Media

Reconstruction (2D
->3D)

FIB-SEM

Sciazko et al.
(2021)

GAN2Dto3D

Electrode

Reconstruction (2D
->3D)

RGB-D

Liu et al.
(2021)

DLGAN (EDGAN)

Diversified
objects

Reconstruction (2D
->3D)

Synthetic

Nozawa et al.
(2021)

cGAN

Cars

Reconstruction (2D
->3D)

Synthetic

Shen et al.
(2021)

WGAN-GP

Hair

Reconstruction (2D
->3D)

Synthetic,
x-ray, KPFM,
SEM

Kench and
Cooper
(2021)

SliceGAN

Electrode

Reconstruction (Low
->High)

CAD, RGB-D

Wang et al.
(2017)

ED-GAN (cGAN based)

Diversified
objects

Reconstruction (Low
->High)

Synthetic

Halpert
(2019)

DCGAN

Seismic

General

CT

Chen et al.
(2020)

WGAN with two
discriminators

Porous Media

General

CT (XCT)

GayonLombardo
et al. (2020)

DCGAN

Electrode

General

CT

Liu et al.
(2019)

DCGAN

Porous Media

General

Micro-CT

Mosser et al.
(2017)

DCGAN

Porous Media

Classification

42

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Fig. D.1. Number of papers per year.

Fig. D.2. Number of papers on volumetric data with medical and non-medical
application. The medical papers are divided into human and non-human applications.

In Table C.1 it is possible to find the columns: Application, Modality,
Study, Network, Organ, and Notes. As specified in the Organ column,
the cells have several sources, such as kidneys, embryos, neurons, and
blood vessels. The last column highlights the main contribution of the
paper. Image translation is the main application of GANs in a medical
context, as can be seen in Fig. 7.
Fig. 8 shows the number of medical publications sorted by the
number of papers on a specific organ. It can be seen that the most
studied organ is the brain, followed by the lungs. The study of 3D
structures of cells with GANs is also very popular. Fig. 11 shows all
GAN architectures used in each application. The cGAN architecture is
the most commonly used architecture in various applications, but it can
be seen that CycleGAN-based architectures are the preferred choice for
image translation and multimodal data.

Fig. D.3. Number of papers per modality per year.

References

The application â€˜â€˜Generalâ€™â€™ refers to the use of GANs only for the
generation of synthetic data without a specific purpose, or that only
proposals are presented but not developed in the papers.

Alqahtani, H., Kavakli-Thorne, M., Kumar, G., 2021. Applications of generative adversarial networks (gans): An updated review. Arch. Comput. Methods Eng. 28 (2),
525â€“552.
Apostolopoulos, I.D., Papathanasiou, N.D., Apostolopoulos, D.J., Panayiotakis, G.S.,
2022. Applications of generative adversarial networks (GANs) in positron emission
tomography (PET) imaging: A review. Eur. J. Nucl. Med. Mol. Imaging 1â€“23.

Table C.2 has the columns: Application, Modality, Study, Network,
and Structure. The last column contains the objects used in the study,
although in some cases different objects are used, so it is simply written
Objects or Models. The number of papers per application is shown
in Fig. 7, where Reconstruction is the main application of GANs in a
non-medical context.

Arjovsky, M., Chintala, S., Bottou, L., 2017a. Wasserstein generative adversarial
networks. In: International Conference on Machine Learning. PMLR, pp. 214â€“223.
Arjovsky, M., Chintala, S., Bottou, L., 2017b. Wasserstein generative adversarial
networks. In: International Conference on Machine Learning. PMLR, pp. 214â€“223.
Arora, A., Shantanu, 2020. A review on application of GANs in cybersecurity domain.
IETE Tech. Rev. 1â€“9.
Ba, J.L., Kiros, J.R., Hinton, G.E., 2016. Layer normalization. arXiv preprint arXiv:
1607.06450.

Appendix D. Supplementary figures

Baniukiewicz, P., Lutton, E.J., Collier, S., Bretschneider, T., 2019. Generative adversarial
networks for augmenting training data of microscopic cell images. Front. Comput.
Sci. 1 (November), 1â€“12. http://dx.doi.org/10.3389/fcomp.2019.00010.

See Figs. D.1â€“D.3.
43

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Dumay, J., Fournier, F., 1988. Multivariate statistical analyses applied to seismic facies
recognition. Geophysics 53 (9), 1151â€“1159.
Egger, J., Gsaxner, C., Pepe, A., Pomykala, K.L., Jonske, F., Kurz, M., Li, J., Kleesiek, J.,
2022. Medical deep learningâ€“A systematic meta-review. Comput. Methods Programs
Biomed. 106874.
Egger, J., Pepe, A., Gsaxner, C., Jin, Y., Li, J., Kern, R., 2021. Deep learningâ€”A first
meta-survey of selected reviews across scientific disciplines, their commonalities,
challenges and research impact. PeerJ Comput. Sci. 7, e773.
Fadero, T.C., Gerbich, T.M., Rana, K., Suzuki, A., DiSalvo, M., Schaefer, K.N., Heppert, J.K., Boothby, T.C., Goldstein, B., Peifer, M., et al., 2018. LITE microscopy:
Tilted light-sheet excitation of model organisms offers high resolution and low
photobleaching. J. Cell Biol. 217 (5), 1869â€“1882.
Ferreira, A., MagalhÃ£es, R., Alves, V., 2022a. Generation of synthetic data: A generative
adversarial networks approach. In: Big Data Analytics and Artificial Intelligence in
the Healthcare Industry. IGI Global, pp. 236â€“261.
Ferreira, A., MagalhÃ£es, R., MÃ©riaux, S., Alves, V., 2022b. Generation of synthetic rat
brain MRI scans with a 3D enhanced alpha generative adversarial network. Appl.
Sci. 12 (10), 4844.
Fischer, A.C., MÃ¤ntysalo, M., Niklaus, F., 2020. Inkjet printing, laser-based micromachining, and microâ€“3D printing technologies for MEMS. In: Handbook of Silicon
Based MEMS Materials and Technologies. Elsevier, pp. 531â€“545.
Flach, P.A., 2010. ROC analysis. In: Sammut, C., Webb, G.I. (Eds.), Encyclopedia of
Machine Learning. Springer US, Boston, MA, pp. 869â€“875. http://dx.doi.org/10.
1007/978-0-387-30164-8_733.
Fragemann, J., Ardizzone, L., Egger, J., Kleesiek, J., 2022. Review of disentanglement
approaches for medical applicationsâ€“towards solving the gordian knot of generative
models in healthcare. arXiv preprint arXiv:2203.11132.
Fu, C., Lee, S., Ho, D.J., Han, S., Salama, P., Dunn, K.W., Delp, E.J., 2018. Three
dimensional fluorescence microscopy image synthesis and segmentation. In: IEEE
Computer Society Conference on Computer Vision and Pattern Recognition Workshops, vol. 2018-June, pp. 2302â€“2310. http://dx.doi.org/10.1109/CVPRW.2018.
00298, arXiv:1801.07198.
Futrega, M., Milesi, A., Marcinkiewicz, M., Ribalta, P., 2022. Optimized U-Net for
brain tumor segmentation. In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and
Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021, Held in
Conjunction with MICCAI 2021, Virtual Event, September 27, 2021, Revised
Selected Papers, Part II. Springer, pp. 15â€“29.
Gao, R., Hou, X., Qin, J., Chen, J., Liu, L., Zhu, F., Zhang, Z., Shao, L., 2020. ZeroVAE-GAN: Generating unseen features for generalized and transductive zero-shot
learning. IEEE Trans. Image Process. 29, 3665â€“3680.
Gatys, L.A., Ecker, A.S., Bethge, M., 2016. Image style transfer using convolutional
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 2414â€“2423.
Gayon-Lombardo, A., Mosser, L., Brandon, N.P., Cooper, S.J., 2020. Pores for thought:
Generative adversarial networks for stochastic reconstruction of 3D multi-phase
electrode microstructures with periodic boundaries. npj Comput. Mater. 6 (1), 1â€“11.
http://dx.doi.org/10.1038/s41524-020-0340-7, arXiv:2003.11632.
Gomez, A.N., Ren, M., Urtasun, R., Grosse, R.B., 2017. The reversible residual network:
Backpropagation without storing activations. In: Advances in Neural Information
Processing Systems, vol. 30.
Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y., 2014. Generative adversarial nets. Adv. Neural Inf.
Process. Syst. 3 (January), 2672â€“2680, arXiv:arXiv:1406.2661v1.
Greminger, M., 2020. Generative adversarial networks with synthetic training data for
enforcing manufacturing constraints on topology optimization. In: International
Design Engineering Technical Conferences and Computers and Information in
Engineering Conference, vol. 84003, American Society of Mechanical Engineers,
V11AT11A005.
Gsaxner, C., Wallner, J., Chen, X., Zemann, W., Egger, J., 2019. Facial model collection
for medical augmented reality in oncologic cranio-maxillofacial surgery. Sci. Data
6 (1), 1â€“7.
Gu, Y., Zheng, Q., 2021. A transfer deep generative adversarial network model to
synthetic brain CT generation from MR images. Wirel. Commun. Mob. Comput.
2021, http://dx.doi.org/10.1155/2021/9979606.
Gui, J., Sun, Z., Wen, Y., Tao, D., Ye, J., 2020. A review on generative adversarial
networks: Algorithms, theory, and applications 14 (8). pp. 1â€“28, arXiv:2001.06937.
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A., 2017. Improved training of wasserstein GANs. Adv. Neural Inf. Process. Syst. 2017-Decem,
5768â€“5778, arXiv:1704.00028.
Gupta, H., McCann, M.T., Donati, L., Unser, M., 2021. CryoGAN: A new reconstruction
paradigm for single-particle cryo-EM via deep adversarial learning. IEEE Trans.
Comput. Imaging 7, 759â€“774. http://dx.doi.org/10.1109/TCI.2021.3096491.
GutiÃ©rrez-Becker, B., Sarasua, I., Wachinger, C., 2021. Discriminative and generative
models for anatomical shape analysis on point clouds with deep neural networks.
Med. Image Anal. 67, 101852.
Halpert, A.D., 2019. Deep learning-enabled seismic image enhancement. In: 2018
SEG International Exposition and Annual Meeting. SEG 2018, pp. 2081â€“2085.
http://dx.doi.org/10.1190/segam2018-2996943.1.

Baratloo, A., Hosseini, M., Negida, A., El Ashal, G., 2015. Part 1: simple definition and
calculation of accuracy, sensitivity and specificity.
Baumgartner, C.F., Koch, L.M., Tezcan, K.C., Ang, J.X., Konukoglu, E., 2018. Visual
feature attribution using Wasserstein GANs. In: Proceedicccngs of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, pp.
8309â€“8319. http://dx.doi.org/10.1109/CVPR.2018.00867, arXiv:1711.08998.
Ben-Hamu, H., Maron, H., Kezurer, I., Avineri, G., Lipman, Y., 2018. Multi-chart
generative surface modeling. ACM Trans. Graph. 37 (6), 1â€“15.
Berger, M., Tagliasacchi, A., Seversky, L.M., Alliez, P., Guennebaud, G., Levine, J.A.,
Sharf, A., Silva, C.T., 2017. A survey of surface reconstruction from point clouds.
In: Computer Graphics Forum, vol. 36, (no. 1), Wiley Online Library, pp. 301â€“329.
Bezerra, H.G., Costa, M.A., Guagliumi, G., Rollins, A.M., Simon, D.I., 2009. Intracoronary optical coherence tomography: A comprehensive review: Clinical and research
applications. JACC: Cardiovasc. Interv. 2 (11), 1035â€“1046.
Blanz, V., Vetter, T., 1999. A morphable model for the synthesis of 3D faces. In:
Proceedings of the 26th Annual Conference on Computer Graphics and Interactive
Techniques. pp. 187â€“194.
Boehm, H., Fink, C., Attenberger, U., Becker, C., Behr, J., Reiser, M., 2008. Automated
classification of normal and pathologic pulmonary tissue by topological texture
features extracted from multi-detector CT in 3D. Eur. Radiol. 18 (12), 2745â€“2755.
Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A.,
Scharre, P., Zeitzoff, T., Filar, B., et al., 2018. The malicious use of artificial
intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.
07228.
Bu, T., Yang, Z., Jiang, S., Zhang, G., Zhang, H., Wei, L., 2021. 3D conditional
generative adversarial network-based synthetic medical image augmentation for
lung nodule detection. Int. J. Imaging Syst. Technol. 31 (2), 670â€“681. http:
//dx.doi.org/10.1002/ima.22511.
Cai, J., Zhang, Z., Cui, L., Zheng, Y., Yang, L., 2019. Towards cross-modal organ
translation and segmentation: A cycle- and shape-consistent generative adversarial
network. Med. Image Anal. 52, 174â€“184. http://dx.doi.org/10.1016/j.media.2018.
12.002.
Carney, J.P., Townsend, D.W., Rappoport, V., Bendriem, B., 2006. Method for transforming CT images for attenuation correction in PET/CT imaging. Med. Phys. 33
(4), 976â€“983.
CÃ©dric, V., 2008. Optimal Transport: Old and New (Grundlehren der mathematischen
Wissenschaften, 338). English. Springer, p. 998.
Chen, H., 2021. Challenges and corresponding solutions of generative adversarial
networks (GANs): A survey study. J. Phys.: Conf. Ser. 1827 (1), 012066.
Chen, Y., An, H., 2017. Attenuation correction of PET/MR imaging. Magn. Resonance
Imaging Clin. 25 (2), 245â€“255.
Chen, L., Li, S., Guo, C., 2020. Using ternary adversarial networks to capture geometric
information in the reconstruction of porous media. In: 2020 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and
Optimization. NEMO 2020, pp. 20â€“23. http://dx.doi.org/10.1109/NEMO49486.
2020.9343664.
Chen, R.J., Lu, M.Y., Chen, T.Y., Williamson, D.F., Mahmood, F., 2021a. Synthetic data
in machine learning for medicine and healthcare. Nat. Biomed. Eng. 5 (6), 493â€“497.
Chen, S., Ma, K., Zheng, Y., 2019. Med3d: Transfer learning for 3d medical image
analysis. arXiv preprint arXiv:1904.00625.
Chen, A., Wu, L., Han, S., Salama, P., Dunn, K.W., Delp, E.J., 2021b. Three dimensional synthetic non-ellipsoidal nuclei volume generation using BÃ©zier curves. In:
Proceedings - International Symposium on Biomedical Imaging, vol. 2021-April, pp.
961â€“965. http://dx.doi.org/10.1109/ISBI48211.2021.9434149.
Collis, R., 1970. Lidar. Appl. Opt. 9 (8), 1782â€“1788.
Croitoru, F.A., Hondru, V., Ionescu, R.T., Shah, M., 2022. Diffusion models in vision:
A survey. arXiv preprint arXiv:2209.04747.
Danu, M., Nita, C.I.C.I., Vizitiu, A., Suciu, C., Itu, L.M., 2019. Deep learning based generation of synthetic blood vessel surfaces. In: 2019 23rd International Conference
on System Theory, Control and Computing, ICSTCC 2019 - Proceedings. IEEE, pp.
662â€“667. http://dx.doi.org/10.1109/ICSTCC.2019.8885576.
Dash, A., Gamboa, J.C.B., Ahmed, S., Liwicki, M., Afzal, M.Z., 2017. Tac-gan-text
conditioned auxiliary classifier generative adversarial network. arXiv preprint arXiv:
1703.06412.
De Myttenaere, A., Golden, B., Le Grand, B., Rossi, F., 2016. Mean absolute percentage
error for regression models. Neurocomputing 192, 38â€“48.
Depeursinge, A., Foncubierta-Rodriguez, A., Van De Ville, D., MÃ¼ller, H., 2014. Threedimensional solid texture analysis in biomedical imaging: Review and opportunities.
Med. Image Anal. 18 (1), 176â€“196.
Dhariwal, P., Nichol, A., 2021. Diffusion models beat gans on image synthesis. Adv.
Neural Inf. Process. Syst. 34, 8780â€“8794.
Dikici, E., Bigelow, M., White, R.D., Erdal, B.S., Prevedello, L.M., 2021. Constrained
generative adversarial network ensembles for sharable synthetic medical images. J.
Med. Imaging 8 (02), http://dx.doi.org/10.1117/1.JMI.8.2.024004.
Dong, X., Lei, Y., Tian, S., Wang, T., Patel, P., Curran, W.J., Jani, A.B., Liu, T., Yang, X.,
2019. Synthetic MRI-aided multi-organ segmentation on male pelvic CT using cycle
consistent deep attention network. Radiother. Oncol. 141, 192â€“199.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,
Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al., 2020. An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929.
44

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., Aila, T., 2020. Training generative adversarial networks with limited data. In: Advances in Neural Information
Processing Systems, vol. 33, pp. 12104â€“12114.
Karras, T., Laine, S., Aila, T., 2019. A style-based generator architecture for generative
adversarial networks. In: Proceedings of the IEEE Computer Society Conference
on Computer Vision and Pattern Recognition, vol. 2019-June, pp. 4396â€“4405.
http://dx.doi.org/10.1109/CVPR.2019.00453, arXiv:1812.04948.
Kascenas, A., Pugeault, N., Oâ€™Neil, A.Q., 2022. Denoising autoencoders for unsupervised
anomaly detection in brain MRI. In: International Conference on Medical Imaging
with Deep Learning. PMLR, pp. 653â€“664.
Kazemifar, S., Balagopal, A., Nguyen, D., McGuire, S., Hannan, R., Jiang, S.,
Owrangi, A., 2018. Segmentation of the prostate and organs at risk in male pelvic
CT images using deep learning. Biomed. Phys. Eng. Express 4 (5), 055003.
Kazeminia, S., Baur, C., Kuijper, A., van Ginneken, B., Navab, N., Albarqouni, S.,
Mukhopadhyay, A., 2020. GANs for medical image analysis. Artif. Intell. Med. 109,
101938.
Kench, S., Cooper, S.J., 2021. Generating three-dimensional structures from a
two-dimensional slice with generative adversarial network-based dimensionality
expansion. Nat. Mach. Intell. 3 (4), 299â€“305. http://dx.doi.org/10.1038/s42256021-00322-1.
Kirch, W. (Ed.), 2008. Pearsonâ€™s correlation coefficient. In: Encyclopedia of Public
Health. Springer Netherlands, Dordrecht, pp. 1090â€“1091. http://dx.doi.org/10.
1007/978-1-4020-5614-7_2569.
Kniaz, V.V., Moshkantsev, P.V., Mizginov, V.A., 2020. Deep learning a single photo
voxel model prediction from real and synthetic images. Stud. Comput. Intell. 856,
3â€“16. http://dx.doi.org/10.1007/978-3-030-30425-6_1.
Kohtala, S., Steinert, M., 2021. Leveraging synthetic data from CAD models for
training object detection modelsâ€“A VR industry application case. Procedia CIRP
100, 714â€“719.
Krizhevsky, A., Hinton, G., et al., 2009. Learning multiple layers of features from tiny
images.
Krutko, V., Belozerov, B., Budennyy, S., Sadikhov, E., Kuzmina, O., Orlov, D., Muravleva, E., Koroteev, D., 2019. A new approach to clastic rocks pore-scale topology
reconstruction based on automatic thin-section images and CT scans analysis. In:
Proceedings - SPE Annual Technical Conference and Exhibition, vol. 2019-Septe,
http://dx.doi.org/10.2118/196183-ms.
Kwon, G., Han, C., Kim, D.s., 2019. Generation of 3D brain MRI using auto-encoding
generative adversarial networks. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention. Springer, pp. 118â€“126.
Ledig, C., Theis, L., HuszÃ¡r, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.,
Tejani, A., Totz, J., Wang, Z., Shi, W., 2017. Photo-realistic single image
super-resolution using a generative adversarial network. In: Proceedings - 30th
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, vol.
2017-Janua, pp. 105â€“114. http://dx.doi.org/10.1109/CVPR.2017.19, arXiv:1609.
04802.
Lei, Y., Dong, X., Tian, S., Wang, T., Patel, P., Curran, W.J., Jani, A.B., Liu, T., Yang, X.,
2020a. Multi-organ segmentation in pelvic CT images with CT-based synthetic MRI.
In: Progress in Biomedical Optics and Imaging - Proceedings of SPIE, vol. 11317,
(no. February 2020), p. 63. http://dx.doi.org/10.1117/12.2548470.
Lei, Y., Wang, T., Dong, X., Higgins, K., Liu, T., Curran, W.J., Mao, H., Nye, J.A.,
Yang, X., 2020b. Low dose PET imaging with CT-Aided cycle-consistent adversarial
networks. In: Progress in Biomedical Optics and Imaging - Proceedings of SPIE, vol.
11312, (no. March 2020), p. 152. http://dx.doi.org/10.1117/12.2549386.
Li, X., Dong, Y., Peers, P., Tong, X., 2019. Synthesizing 3D shapes from silhouette image
collections using multi-projection generative adversarial networks. In: Proceedings
of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2019-June, IEEE, pp. 5530â€“5539. http://dx.doi.org/10.1109/CVPR.2019.
00568, arXiv:1906.03841.
Li, A., Du, C., Volkow, N.D., Pan, Y., 2020. A deep-learning-based approach for noise
reduction in high-speed optical coherence Doppler tomography. J. Biophotonics 13
(10), 1â€“11. http://dx.doi.org/10.1002/jbio.202000084.
Li, J., Ellis, D.G., Pepe, A., Gsaxner, C., Aizenberg, M.R., Kleesiek, J., Egger, J., 2022.
Back to the roots: Reconstructing large and complex cranial defects using an
image-based statistical shape model. arXiv preprint arXiv:2204.05703.
Li, C.-L., Zaheer, M., Zhang, Y., Poczos, B., Salakhutdinov, R., 2018. Point cloud gan.
arXiv preprint arXiv:1810.05795.
Lim, J.H., Ye, J.C., 2017. Geometric gan. arXiv preprint arXiv:1705.02894.
Lin, W.W., Lin, W.W., Chen, G., Zhang, H., Gao, Q., Huang, Y., Tong, T., Du, M.,
2021a. Bidirectional mapping of brain MRI and PET with 3D reversible GAN
for the diagnosis of Alzheimerâ€™s disease. Front. Neurosci. 15 (April), 1â€“13. http:
//dx.doi.org/10.3389/fnins.2021.646013.
Lin, J., Xia, Y., Liu, S., Zhao, S., Chen, Z., 2021b. Zstgan: An adversarial approach for
unsupervised zero-shot image-to-image translation. Neurocomputing 461, 327â€“335.
Liu, Y., Dwivedi, G., Boussaid, F., Sanfilippo, F., Yamada, M., Bennamoun, M., 2022.
Inflating 2D convolution weights for efficient generation of 3D medical images.
arXiv preprint arXiv:2208.03934.
Liu, M., Jervis, M., Li, W., Nivlet, P., 2020a. Seismic facies classification using supervised convolutional neural networks and semisupervised generative adversarial
networks. Geophysics 85 (4), O47â€“O58. http://dx.doi.org/10.1190/geo2019-0627.
1.

Han, C., Kitamura, Y., Kudo, A., Ichinose, A., Rundo, L., Furukawa, Y., Umemoto, K.,
Li, Y., Nakayama, H., 2019a. Synthesizing diverse lung nodules wherever massively:
3D multi-conditional GAN-based CT image augmentation for object detection. In:
Proceedings - 2019 International Conference on 3D Vision. 3DV 2019, IEEE, pp.
729â€“737. http://dx.doi.org/10.1109/3DV.2019.00085, arXiv:1906.04962.
Han, S., Salama, P., Dunn, K.W., Delp, E.J., Lee, S., Fu, C., Salama, P., Dunn, K.W.,
Delp, E.J., 2019b. Nuclei counting in microscopy images with three dimensional
generative adversarial networks. In: Progress in Biomedical Optics and Imaging Proceedings of SPIE, vol. 10949, p. 105. http://dx.doi.org/10.1117/12.2512591.
Harms, J., Lei, Y., Wang, T., Zhang, R., Zhou, J., Tang, X., Curran, W.J., Liu, T.,
Yang, X., 2019. Paired cycle-GAN-based image correction for quantitative conebeam computed tomography. Med. Phys. 46, 3998â€“4009. http://dx.doi.org/10.
1002/mp.13656.
Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D., 2022. Swin unetr:
Swin transformers for semantic segmentation of brain tumors in mri images. In:
Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th
International Workshop, BrainLes 2021, Held in Conjunction with MICCAI 2021,
Virtual Event, September 27, 2021, Revised Selected Papers, Part I. Springer, pp.
272â€“284.
Heiliger, L., Sekuboyina, A., Menze, B., Egger, J., Kleesiek, J., 2022. Beyond medical
imaging-a review of multimodal deep learning in radiology.
Heimann, T., Meinzer, H.P., 2009. Statistical shape models for 3D medical image
segmentation: A review. Med. Image Anal. 13 (4), 543â€“563.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S., 2017. GANs
trained by a two time-scale update rule converge to a local Nash equilibrium.
Adv. Neural Inf. Process. Syst. 2017-Decem (Nips), 6627â€“6638, arXiv:arXiv:1706.
08500v6.
Ho, D.J., Han, S., Fu, C., Salama, P., Dunn, K.W., Delp, E.J., 2019. Center-extractionbased three dimensional nuclei instance segmentation of fluorescence microscopy
images. In: 2019 IEEE EMBS International Conference on Biomedical and Health
Informatics, BHI 2019 - Proceedings. IEEE, pp. 1â€“4. http://dx.doi.org/10.1109/BHI.
2019.8834516, arXiv:1909.05992.
Hu, S., Lei, B., Wang, S., Wang, Y., Feng, Z., Shen, Y., 2022. Bidirectional mapping
generative adversarial networks for brain MR to PET synthesis. IEEE Trans. Med.
Imaging 41 (1), 145â€“157. http://dx.doi.org/10.1109/TMI.2021.3107013.
Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely connected
convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 4700â€“4708.
Huang, S.-C., Pareek, A., Seyyedi, S., Banerjee, I., Lungren, M.P., 2020. Fusion of
medical imaging and electronic health records using deep learning: A systematic
review and implementation guidelines. NPJ Digit. Med. 3 (1), 1â€“9.
Huang, Q., Qiao, C., Jing, K., Zhu, X., Ren, K., 2022. Biomarkers identification for
Schizophrenia via VAE and GSDAE-based data augmentation. Comput. Biol. Med.
146, 105603.
Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H., 2021. NnU-Net: A
self-configuring method for deep learning-based biomedical image segmentation.
Nat. Methods 18 (2), 203â€“211.
Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A., 2017. Image-to-image translation with
conditional adversarial networks. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 1125â€“1134.
Jack, Jr., C.R., Bernstein, M.A., Fox, N.C., Thompson, P., Alexander, G., Harvey, D.,
Borowski, B., Britson, P.J., L. Whitwell, J., Ward, C., et al., 2008. The Alzheimerâ€™s
disease neuroimaging initiative (ADNI): MRI methods. J. Magn. Resonance Imaging:
Offic. J. Int. Soc. Magn. Resonance Med. 27 (4), 685â€“691.
Jeong, J.J., Tariq, A., Adejumo, T., Trivedi, H., Gichoya, J.W., Banerjee, I., 2022.
Systematic review of generative adversarial networks (GANs) for medical image
classification and segmentation. J. Digit. Imaging 1â€“16.
Johnson, J., Alahi, A., Fei-Fei, L., 2016. Perceptual losses for real-time style transfer
and super-resolution. In: Lecture Notes in Computer Science (including subseries
Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol.
9906 LNCS, pp. 694â€“711. http://dx.doi.org/10.1007/978-3-319-46475-6_43, arXiv:
arXiv:1603.08155v1.
Jolicoeur-Martineau, A., 2018. The relativistic discriminator: A key element missing
from standard GAN. arXiv preprint arXiv:1807.00734.
Jung, E., Luna, M., Park, S.H., 2020. Conditional generative adversarial network for
predicting 3D medical images affected by Alzheimerâ€™s diseases. In: Lecture Notes in
Computer Science (including subseries Lecture Notes in Artificial Intelligence and
Lecture Notes in Bioinformatics), vol. 12329 LNCS, pp. 79â€“90. http://dx.doi.org/
10.1007/978-3-030-59354-4_8.
Kamalian, S., Lev, M.H., Gupta, R., 2016. Computed tomography imaging and
angiographyâ€“principles. In: Handbook of Clinical Neurology, vol. 135, Elsevier, pp.
3â€“20.
Kanopoulos, N., Vasanthavada, N., Baker, R.L., 1988. Design of an image edge detection
filter using the Sobel operator. IEEE J. Solid-State Circuits 23 (2), 358â€“367.
Karras, T., Aila, T., Laine, S., Lehtinen, J., 2017. Progressive growing of gans for
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.
Karras, T., Aila, T., Laine, S., Lehtinen, J., 2018. Progressive growing of GANs for
improved quality, stability, and variation. In: 6th International Conference on
Learning Representations, ICLR 2018 - Conference Track Proceedings. pp. 1â€“26,
arXiv:1710.10196.
45

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Pan, Y., Liu, M., Lian, C., Xia, Y., Shen, D., 2019. Disease-image specific generative
adversarial network for brain disease diagnosis with incomplete multi-modal
neuroimages. In: Lecture Notes in Computer Science (including subseries Lecture
Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 11766
LNCS, pp. 137â€“145. http://dx.doi.org/10.1007/978-3-030-32248-9_16.

Liu, C., Kong, D., Wang, S., Li, J., Yin, B., 2021. DLGAN: Depth-preserving latent
generative adversarial network for 3D reconstruction. IEEE Trans. Multimed. 23,
2843â€“2856. http://dx.doi.org/10.1109/TMM.2020.3017924.
Liu, Y., Lei, Y., Wang, T., Zhou, J., Lin, L., Liu, T., Patel, P., Curran, W.J., Ren, L.,
Yang, X., 2020b. Liver synthetic CT generation based on dense-cyclegan for
MRI-only treatment planning. In: Progress in Biomedical Optics and Imaging Proceedings of SPIE, vol. 11313, (no. March 2020), p. 92. http://dx.doi.org/10.
1117/12.2549265.
Liu, S., Zhong, Z., Takbiri-Borujeni, A., Kazemi, M., Fu, Q., Yang, Y., 2019. A case
study on homogeneous and heterogeneous reservoir porous media reconstruction
by using generative adversarial networks. In: Energy Procedia. 158, (785), Elsevier
B.V., pp. 6164â€“6169. http://dx.doi.org/10.1016/j.egypro.2019.01.493.
Lucic, M., Kurach, K., Michalski, M., Gelly, S., Bousquet, O., 2018. Are gans created
equal? A large-scale study. In: Advances in Neural Information Processing Systems,
vol. 31.
Ma, B., Zhao, Y., Yang, Y., Zhang, X., Dong, X., Zeng, D., Ma, S., Li, S., 2020. MRI image
synthesis with dual discriminator adversarial learning and difficulty-aware attention
mechanism for hippocampal subfields segmentation. Comput. Med. Imaging Graph.
86 (April), 101800. http://dx.doi.org/10.1016/j.compmedimag.2020.101800.
Man, K., Chahl, J., 2022. A review of synthetic image data and its use in computer
vision. J. Imaging 8 (11), 310.
Mao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Smolley, S.P., 2017. Least squares generative adversarial networks. In: Proceedings of the IEEE International Conference
on Computer Vision, vol. 2017-Octob, pp. 2813â€“2821. http://dx.doi.org/10.1109/
ICCV.2017.304, arXiv:1611.04076.
Marcu, A., Costea, D., Licaret, V., PÃ®rvu, M., Slusanschi, E., Leordeanu, M., 2018.
Safeuav: Learning to estimate depth and safe landing areas for uavs from synthetic
data. In: Proceedings of the European Conference on Computer Vision (ECCV)
Workshops.
Mathieu, M., Couprie, C., LeCun, Y., 2016. Deep multi-scale video prediction beyond
mean square error. In: 4th International Conference on Learning Representations,
ICLR 2016 - Conference Track Proceedings, (no. 2015), pp. 1â€“14, arXiv:1511.05440.
Maturana, D., Scherer, S., 2015. Voxnet: A 3d convolutional neural network for realtime object recognition. In: 2015 IEEE/RSJ International Conference on Intelligent
Robots and Systems. IROS, IEEE, pp. 922â€“928.
McRobbie, D.W., Moore, E.A., Graves, M.J., Prince, M.R., 2017. MRI from Picture to
Proton. Cambridge University Press.
Melitz, W., Shen, J., Kummel, A.C., Lee, S., 2011. Kelvin probe force microscopy and
its application. Surf. Sci. Rep. 66 (1), 1â€“27.
Memon, A.R., Li, J., Egger, J., Chen, X., 2021. A review on patient-specific facial and
cranial implant design using Artificial Intelligence (AI) techniques. Expert Rev. Med.
Dev. 18 (10), 985â€“994.
Mihir Garimella, P.N., 2018. Beyond the pixel plane: Sensing and learning in 3D. The
Gradient.
Mirza, M., Osindero, S., 2014. Conditional generative adversarial nets. pp. 1â€“7, URL
http://arxiv.org/abs/1411.1784, arXiv:1411.1784.
Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y., 2018. Spectral normalization for
generative adversarial networks. In: 6th International Conference on Learning
Representations, ICLR 2018 - Conference Track Proceedings. arXiv:1802.05957.
Moghari, M.D., Zhou, L., Yu, B., Moore, K., Young, N., Fulton, R., Kyme, A.,
2019. Estimation of full-dose 4D CT perfusion images from low-dose images
using conditional generative adversarial networks. In: 2019 IEEE Nuclear Science
Symposium and Medical Imaging Conference. NSS/MIC 2019, pp. 22â€“24. http:
//dx.doi.org/10.1109/NSS/MIC42101.2019.9059723.
Mohamed, S., Lakshminarayanan, B., 2016. Learning in implicit generative models.
arXiv preprint arXiv:1610.03483.
Momeni, S., Fazlollahi, A., Lebrat, L., Yates, P., Rowe, C., Gao, Y.,
Liew, A.W.C.A.C.A.W.e.-C., Salvado, O., 2021. Generative model of brain
microbleeds for MRI detection of vascular marker of neurodegenerative diseases.
Front. Neurosci. 15 (December), 1â€“13. http://dx.doi.org/10.3389/fnins.2021.
778767.
MONAI Consortium, 2022. MONAI: Medical open network for AI. http://dx.doi.org/10.
5281/zenodo.7459814, If you use this software, please cite it using these metadata.
Mosser, L., Dubrule, O., Blunt, M.J., 2017. Reconstruction of three-dimensional porous
media using generative adversarial neural networks. Phys. Rev. E 96 (4), http:
//dx.doi.org/10.1103/PhysRevE.96.043309, arXiv:1704.03225.
Muzahid, A.A.M., Wanggen, W., Sohel, F., Bennamoun, M., Hou, L., Ullah, H.,
2021. Progressive conditional GAN-based augmentation for 3D object recognition.
Neurocomputing 460, 20â€“30. http://dx.doi.org/10.1016/j.neucom.2021.06.091.
Navidan, H., Moshiri, P.F., Nabati, M., Shahbazian, R., Ghorashi, S.A., ShahMansouri, V., Windridge, D., 2021. Generative adversarial networks (GANs) in
networking: A comprehensive survey & evaluation. Comput. Netw. 194, 108149.
Niemeijer, M., Loog, M., Abramoff, M.D., Viergever, M.A., Prokop, M., van Ginneken, B.,
2010. On combining computer-aided detection systems. IEEE Trans. Med. Imaging
30 (2), 215â€“223.
Nozawa, N., Shum, H.P.H., Feng, Q., Ho, E.S.L., Morishima, S., 2021. 3D car shape
reconstruction from a contour sketch using GAN and lazy learning. Vis. Comput.
http://dx.doi.org/10.1007/s00371-020-02024-y.

Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A., 2016. Context encoders:
Feature learning by inpainting. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pp. 2536â€“2544.
Pesaranghader, A., Wang, Y., Havaei, M., 2021. CT-SGAN: Computed tomography
synthesis GAN. In: Lecture Notes in Computer Science (including subseries Lecture
Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 13003
LNCS, pp. 67â€“79. http://dx.doi.org/10.1007/978-3-030-88210-5_6, arXiv:2110.
09288.
Ponomarev, A., 2019. Fooling image copy detection algorithms with GANs.
Preim, B., Bartz, D., 2007. Visualization in Medicine: Theory, Algorithms, and
Applications. Elsevier.
Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 652â€“660.
Qin, A., Liu, G., Chen, S., Zhao, L., Zheng, W., Yan, D., Grills, I.S., Stevens, C.W.,
Li, X., Ding, X., 2021. Investigate the feasibility of using CBCT to assess the
dose validation for spot-scanning proton arc (SPArc) therapy for advanced staged
lung cancer treatment. Int. J. Radiat. Oncol., Biol., Phys. 111 (3), S98. http:
//dx.doi.org/10.1016/j.ijrobp.2021.07.228.
Rahimi, A., Xu, J., Wang, L., 2013. -Norm regularization in volumetric imaging of
cardiac current sources. Comput. Math. Methods Med. 2013.
Ramos, D., Franco-Pedroso, J., Lozano-Diez, A., Gonzalez-Rodriguez, J., 2018. Deconstructing cross-entropy for probabilistic binary classifiers. Entropy 20 (3), http://dx.
doi.org/10.3390/e20030208, URL https://www.mdpi.com/1099-4300/20/3/208.
Rani, V., Nabi, S.T., Kumar, M., Mittal, A., Kumar, K., 2023. Self-supervised learning:
A succinct review. Arch. Comput. Methods Eng. 1â€“15.
Rosasco, L., De Vito, E., Caponnetto, A., Piana, M., Verri, A., 2004. Are loss functions
all the same? Neural Comput. 16 (5), 1063â€“1076.
Rosca, M., Lakshminarayanan, B., Warde-Farley, D., Mohamed, S., 2017. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:
1706.04987.
Ruby, U., Yendapalli, V., 2020. Binary cross entropy with deep learning technique for
image classification. Int. J. Adv. Trends Comput. Sci. Eng. 9 (10).
Rusak, F., Santa Cruz, R., Bourgeat, P., Fookes, C., Fripp, J., Bradley, A., Salvado, O.,
2020. 3D brain mri gan-based synthesis conditioned on partial volume maps. In:
Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics), vol. 12417 LNCS, pp. 11â€“20.
http://dx.doi.org/10.1007/978-3-030-59520-3_2.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al., 2015. Imagenet large scale visual
recognition challenge. Int. J. Comput. Vis. 115 (3), 211â€“252.
Saha, S., Menzel, S., Minku, L.L., Yao, X., Sendhoff, B., Wollstadt, P., 2020. Quantifying
the generative capabilities of variational autoencoders for 3D car point clouds.
In: 2020 IEEE Symposium Series on Computational Intelligence. SSCI, IEEE, pp.
1469â€“1477.
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X., 2016. Improved techniques for training gans. In: Advances in Neural Information Processing
Systems. pp. 2234â€“2242.
Sammut, C., Webb, G.I. (Eds.), 2010a. Area under curve. In: Encyclopedia of Machine
Learning. Springer US, Boston, MA, p. 40. http://dx.doi.org/10.1007/978-0-38730164-8_28.
Sammut, C., Webb, G.I. (Eds.), 2010b. Clustering. In: Encyclopedia of Machine Learning.
Springer US, Boston, MA, p. 180. http://dx.doi.org/10.1007/978-0-387-301648_124.
Sammut, C., Webb, G.I. (Eds.), 2010c. F1-Measure. In: Encyclopedia of Machine
Learning. Springer US, Boston, MA, p. 397. http://dx.doi.org/10.1007/978-0-38730164-8_298.
Sammut, C., Webb, G.I. (Eds.), 2010d. Mean absolute error. In: Encyclopedia of Machine
Learning. Springer US, Boston, MA, p. 652. http://dx.doi.org/10.1007/978-0-38730164-8_525.
Sammut, C., Webb, G.I. (Eds.), 2010e. Mean squared error. In: Encyclopedia of Machine
Learning. Springer US, Boston, MA, p. 653. http://dx.doi.org/10.1007/978-0-38730164-8_528.
Scarfe, W.C., Farman, A.G., Sukovic, P., et al., 2006. Clinical applications of cone-beam
computed tomography in dental practice. J.-Canad. Dental Assoc. 72 (1), 75.
Schaefferkoetter, J., Yan, J., Moon, S., Chan, R., Ortega, C., Metser, U., Berlin, A., VeitHaibach, P., 2021. Deep learning for whole-body medical image generation. Eur. J.
Nucl. Med. Mol. Imaging 48 (12), 3817â€“3826. http://dx.doi.org/10.1007/s00259021-05413-0.
Sciazko, A., Komatsu, Y., Shikazono, N., 2021. Unsupervised generative adversarial
network for 3-d microstructure synthesis from 2-d image. In: ECS Trans.. 103, (1),
pp. 1363â€“1373. http://dx.doi.org/10.1149/10301.1363ecst.
46

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å.,
Polosukhin, I., 2017. Attention is all you need. In: Advances in Neural Information
Processing Systems, vol. 30.
Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., 2004. Image quality assessment:
From error visibility to structural similarity. IEEE Trans. Image Process. 13 (4),
600â€“612.
Wang, W., Huang, Q., You, S., Yang, C., Neumann, U., 2017. Shape inpainting using 3D
generative adversarial network and recurrent convolutional networks. In: Proceedings of the IEEE International Conference on Computer Vision, vol. 2017-Octob,
pp. 2317â€“2325. http://dx.doi.org/10.1109/ICCV.2017.252, arXiv:1711.06375.
Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., Tao, A., Kautz, J., Catanzaro, B., 2018. Highresolution image synthesis and semantic manipulation with conditional gans. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 8798â€“8807.
Wang, Z., Simoncelli, E.P., Bovik, A.C., 2003. Multiscale structural similarity for image
quality assessment. In: The Thrity-Seventh Asilomar Conference on Signals, Systems
& Computers, 2003, vol. 2, Ieee, pp. 1398â€“1402.
Wei, R., Liu, B., Zhou, F., Bai, X., Fu, D., Liang, B., Wu, Q., 2020. A patient-independent
CT intensity matching method using conditional generative adversarial networks
(cGAN) for single X-ray projection-based tumor localization. Phys. Med. Biol. 65
(14), http://dx.doi.org/10.1088/1361-6560/ab8bf2.
Wong, M.Z., Kunii, K., Baylis, M., Ong, W.H., Kroupa, P., Koller, S., 2019. Synthetic
dataset generation for object-to-model deep learning in industrial applications.
PeerJ Comput. Sci. 5, e222.
Wu, Z., Lischinski, D., Shechtman, E., 2021. Stylespace analysis: Disentangled controls
for stylegan image generation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 12863â€“12872.
Xian, Y., Lampert, C.H., Schiele, B., Akata, Z., 2018. Zero-shot learningâ€”A comprehensive evaluation of the good, the bad and the ugly. IEEE Trans. Pattern Anal. Mach.
Intell. 41 (9), 2251â€“2265.
Xu, Z., Wang, X., Shin, H.-C., Roth, H., Yang, D., Milletari, F., Zhang, L., Xu, D., 2019.
Tunable CT lung nodule synthesis conditioned on background image and semantic
features. In: Lecture Notes in Computer Science (including subseries Lecture Notes
in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 11827 LNCS,
pp. 62â€“70. http://dx.doi.org/10.1007/978-3-030-32778-1_7.
Yan, Y., Lee, H., Somer, E., Grau, V., 2018. Generation of Amyloid PET Images Via
Conditional Adversarial Training for Predicting Progression To Alzheimerâ€™s Disease,
vol. 1, Springer International Publishing, pp. 26â€“33. http://dx.doi.org/10.1007/
978-3-030-00320-3_4.
Yang, C., Eschweiler, D., Stegmaier, J., 2021a. Semi- and self-supervised multiview fusion of 3D microscopy images using generative adversarial networks. In:
Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics), vol. 12964 LNCS, pp. 130â€“139.
http://dx.doi.org/10.1007/978-3-030-88552-6_13, arXiv:2108.02743.
Yang, D., Liu, B., Axel, L., Metaxas, D., 2019. 3D LV probabilistic segmentation in
cardiac MRI using generative adversarial network. In: Lecture Notes in Computer
Science (including subseries Lecture Notes in Artificial Intelligence and Lecture
Notes in Bioinformatics), vol. 11395 LNCS, Springer International Publishing, pp.
181â€“190. http://dx.doi.org/10.1007/978-3-030-12029-0_20.
Yang, H., Lu, X., Wang, S.H.S.-H.S.H., Lu, Z., Yao, J., Jiang, Y., Qian, P., 2021b.
Synthesizing multi-contrast MR images via novel 3D conditional variational autoencoding GAN. Mob. Netw. Appl. 26 (1), 415â€“424. http://dx.doi.org/10.1007/
s11036-020-01678-1.
Yang, B., Wen, H., Wang, S., Clark, R., Markham, A., Trigoni, N., 2017. 3D object
reconstruction from a single depth view with adversarial learning. In: Proceedings
- 2017 IEEE International Conference on Computer Vision Workshops, Vol. 2018Janua. ICCVW 2017, pp. 679â€“688. http://dx.doi.org/10.1109/ICCVW.2017.86,
arXiv:1708.07969.
Yi, X., Walia, E., Babyn, P., 2019. Generative adversarial network in medical imaging:
A review. Med. Image Anal. 58, 101552.
Yu, L., Li, J., Egger, J., 2021. Pca-skull: 3d skull shape modelling using principal
component analysis. In: Towards the Automatization of Cranial Implant Design
in Cranioplasty II: Second Challenge, AutoImplant 2021, Held in Conjunction with
MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings 2. Springer, pp.
105â€“115.
Zeng, G., Zheng, G., 2019. Hybrid generative adversarial networks for deep MR
to CT synthesis using unpaired data. In: Lecture Notes in Computer Science
(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in
Bioinformatics), vol. 11767 LNCS, pp. 759â€“767. http://dx.doi.org/10.1007/978-3030-32251-9_83.
Zhang, H., Sindagi, V., Patel, V.M., 2019a. Image de-raining using a conditional
generative adversarial network. IEEE Trans. Circuits Syst. Video Technol. 30 (11),
3943â€“3956.
Zhang, W., Sun, J., Tang, X., 2008. Cat head detection-how to effectively exploit shape
and texture features. In: Computer Visionâ€“ECCV 2008: 10th European Conference
on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part IV
10. Springer, pp. 802â€“816.
Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.N., 2017. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. In: Proceedings of the IEEE International Conference on Computer Vision.
pp. 5907â€“5915.

Seitzer, M., Yang, G., Schlemper, J., Oktay, O., WÃ¼rfl, T., Christlein, V., Wong, T.,
Mohiaddin, R., Firmin, D., Keegan, J., et al., 2018. Adversarial and perceptual
refinement for compressed sensing MRI reconstruction. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. Springer, pp.
232â€“240.
Setio, A.A.A., Traverso, A., De Bel, T., Berens, M.S., Van Den Bogaard, C., Cerello, P.,
Chen, H., Dou, Q., Fantacci, M.E., Geurts, B., et al., 2017. Validation, comparison,
and combination of algorithms for automatic detection of pulmonary nodules in
computed tomography images: The LUNA16 challenge. Med. Image Anal. 42, 1â€“13.
Shen, T., Liu, R., Bai, J., Li, Z., 2018. â€˜â€˜Deep fakesâ€™â€™ using generative adversarial
networks (gan).
Shen, Y., Zhang, C., Fu, H., Zhou, K., Zheng, Y., 2021. DeepSketchHair: Deep sketchbased 3D hair modeling. IEEE Trans. Vis. Comput. Graphics 27 (7), 3250â€“3263.
http://dx.doi.org/10.1109/TVCG.2020.2968433.
Shi, T., Zou, Z., Song, X., Song, Z., Gu, C., Fan, C., Yuan, Y., 2020. Neutral face
game character auto-creation via pokerface-gan. In: Proceedings of the 28th ACM
International Conference on Multimedia. pp. 3201â€“3209.
Shin, H.C., Tenenholtz, N.A., Rogers, J.K., Schwarz, C.G., Senjem, M.L., Gunter, J.L.,
Andriole, K.P., Michalski, M., 2018. Medical image synthesis for data augmentation and anonymization using generative adversarial networks. In: International
Workshop on Simulation and Synthesis in Medical Imaging. Springer, pp. 1â€“11.
Shivegowda, M.D., Boonyasopon, P., Rangappa, S.M., Siengchin, S., 2022. A review
on computer-aided design and manufacturing processes in design and architecture.
Arch. Comput. Methods Eng. 1â€“8.
Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deep
learning. J. Big Data 6 (1), 1â€“48.
Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556.
Sitek, A., Huesman, R.H., Gullberg, G.T., 2006. Tomographic reconstruction using an
adaptive tetrahedral mesh defined by a point cloud. IEEE Trans. Med. Imaging 25
(9), 1172â€“1179.
Slossberg, R., Shamai, G., Kimmel, R., 2019. High quality facial surface and texture
synthesis via generative adversarial networks. In: Lecture Notes in Computer
Science (including subseries Lecture Notes in Artificial Intelligence and Lecture
Notes in Bioinformatics), vol. 11131 LNCS, pp. 498â€“513. http://dx.doi.org/10.
1007/978-3-030-11015-4_36, arXiv:1808.08281.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014.
Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn.
Res. 15 (1), 1929â€“1958.
Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., 2015. Multi-view convolutional
neural networks for 3d shape recognition. In: Proceedings of the IEEE International
Conference on Computer Vision. pp. 945â€“953.
Sulakhe, H., Li, J., Egger, J., Goyal, P., 2022. CranGAN: Adversarial point cloud
reconstruction for patient-specific cranial implant design. In: 2022 44th Annual
International Conference of the IEEE Engineering in Medicine & Biology Society.
EMBC, IEEE, pp. 603â€“608.
Sun, Y., Yuan, P., Sun, Y., 2020. MM-GAN: 3D MRI data augmentation for medical image segmentation via generative adversarial networks. In: 2020 IEEE International
Conference on Knowledge Graph. ICKG, IEEE, pp. 227â€“234.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., 2016. Rethinking the
inception architecture for computer vision. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. pp. 2818â€“2826.
Taha, A.A., Hanbury, A., 2015. Metrics for evaluating 3D medical image segmentation:
Analysis, selection, and tool. BMC Med. Imaging 15 (1), 1â€“28.
Tang, Z., Zhang, D., Song, Y., Wang, H., Liu, D., Zhang, C., Liu, S., Peng, H.,
Cai, W., 2020. 3D conditional adversarial learning for synthesizing microscopic
neuron image using skeleton-to-neuron translation. In: Proceedings - International
Symposium on Biomedical Imaging, vol. 2020-April, pp. 1775â€“1779. http://dx.doi.
org/10.1109/ISBI45749.2020.9098345.
Thanh-Tung, H., Tran, T., 2020. Catastrophic forgetting and mode collapse in GANs. In:
2020 International Joint Conference on Neural Networks. IJCNN, IEEE, pp. 1â€“10.
Ting, K.M., 2010a. Confusion matrix. In: Sammut, C., Webb, G.I. (Eds.), Encyclopedia
of Machine Learning. Springer US, Boston, MA, p. 209. http://dx.doi.org/10.1007/
978-0-387-30164-8_157.
Ting, K.M., 2010b. Precision. In: Sammut, C., Webb, G.I. (Eds.), Encyclopedia of
Machine Learning. Springer US, Boston, MA, p. 780. http://dx.doi.org/10.1007/
978-0-387-30164-8_651.
Townsend, D.W., 2008. Positron emission tomography/computed tomography. In:
Seminars in Nuclear Medicine, vol. 38, (no. 3), Elsevier, pp. 152â€“166.
Tran, L.D., Nguyen, S.M., Arai, M., 2020. GAN-based noise model for denoising real
images. In: Proceedings of the Asian Conference on Computer Vision.
Tudosiu, P.D., Varsavsky, T., Shaw, R., Graham, M., Nachev, P., Ourselin, S., Sudre, C.H., Cardoso, M.J., 2020. Neuromorphologicaly-preserving volumetric data
encoding using VQ-VAE. arXiv preprint arXiv:2002.05692.
Van der Maaten, L., Hinton, G., 2008. Visualizing data using t-SNE. J. Mach. Learn.
Res. 9 (11).
Van Timmeren, J.E., Cester, D., Tanadini-Lang, S., Alkadhi, H., Baessler, B., 2020.
Radiomics in medical imagingâ€”â€˜â€˜how-toâ€™â€™ guide and critical reflection. Insights
Imaging 11 (1), 1â€“16.
47

Medical Image Analysis 93 (2024) 103100

A. Ferreira et al.

Zhou, T., Fan, D.P., Cheng, M.M., Shen, J., Shao, L., 2021. RGB-D salient object
detection: A survey. Comput. Vis. Media 7 (1), 37â€“69.
Zhu, J.Y., Park, T., Isola, P., Efros, A.A., 2017. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: Computer Vision (ICCV), 2017 IEEE
International Conference on.
Zhu, M., Zhao, M., Yao, M., Guo, R., 2023. A generative adversarial network with
â€˜â€˜zero-shotâ€™â€™ learning for positron image denoising. Sci. Rep. 13 (1), 1051.
Zhuang, P., Schwing, A.G., 2019. FMRI data augmentation via synthesis. In: 2019 IEEE
16th International Symposium on Biomedical Imaging (ISBI 2019), (vol. Isbi), IEEE,
pp. 1783â€“1787.
ZollhÃ¶fer, M., Stotko, P., GÃ¶rlitz, A., Theobalt, C., NieÃŸner, M., Klein, R., Kolb, A.,
2018. State of the art on 3D reconstruction with RGB-D cameras. In: Computer
Graphics Forum, vol. 37, (no. 2), Wiley Online Library, pp. 625â€“652.

Zhang, W., Yang, Z., Jiang, H., Nigam, S., Yamakawa, S., Furuhata, T., Shimada, K.,
Kara, L.B., 2019b. 3D shape synthesis for conceptual design and optimization
using variational autoencoders. In: International Design Engineering Technical
Conferences and Computers and Information in Engineering Conference, vol. 59186,
American Society of Mechanical Engineers, V02AT03A017.
Zhang, Z., Yang, L., Zheng, Y., 2018. Translating and segmenting multimodal medical
volumes with cycle- and shape-consistency generative adversarial network. In:
Proceedings of the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition. IEEE, pp. 9242â€“9251. http://dx.doi.org/10.1109/CVPR.2018.
00963, arXiv:1802.09655.
Zhang, Z., Yang, L., Zheng, Y., 2019c. Multimodal Medical Volumes Translation and
Segmentation with Generative Adversarial Network. In: Handbook of Medical Image
Computing and Computer Assisted Intervention, Elsevier Inc., pp. 183â€“204. http:
//dx.doi.org/10.1016/B978-0-12-816176-0.00013-2.
Zhang, Z., Zhao, T., Gay, H., Zhang, W., Sun, B., 2021. Semi-supervised semantic
segmentation of prostate and organs-at-risk on 3D pelvic CT images. Biomed. Phys.
Eng. Express 7 (6), 065023.

48

Update
Medical Image Analysis
Volume 96, Issue , August 2024, Page
DOI: https://doi.org/10.1016/j.media.2024.103174

Medical Image Analysis 96 (2024) 103174

Contents lists available at ScienceDirect

Medical Image Analysis
journal homepage: www.elsevier.com/locate/media

Corrigendum

Corrigendum to: GAN-based generation of realistic 3D volumetric data: A
systematic review and taxonomy [Medical Image Analysis 93 (2024)]
AndreÌ Ferreira a, b, c, d, e, Jianning Li a, c, f, Kelsey L. Pomykala a, Jens Kleesiek a, f, g, h, Victor Alves b,
Jan Egger a, c, f, i
a

Institute for AI in Medicine (IKIM), University Hospital Essen, University Duisburg-Essen, GirardetstraÃŸe 2, Essen, 45131, Germany
Center Algoritmi/LASI, University of Minho, Braga, 4710-057, Portugal
Computer Algorithms for Medicine Laboratory, Graz, Austria
d
Department of Oral and Maxillofacial Surgery, University Hospital RWTH Aachen, 52074, Aachen, Germany
e
Institute of Medical Informatics, University Hospital RWTH Aachen, 52074 Aachen, Germany
f
Cancer Research Center Cologne Essen (CCCE), University Medicine Essen, HufelandstraÃŸe 55, Essen, 45147, Germany
g
German Cancer Consortium (DKTK), Partner Site Essen, HufelandstraÃŸe 55, Essen, 45147, Germany
h
TU Dortmund University, Department of Physics, Otto-Hahn-StraÃŸe 4, 44227 Dortmund, Germany
i
Institute of Computer Graphics and Vision, Graz University of Technology, Inffeldgasse 16, Graz, 801, Austria
b
c

The authors regret that the affiliations for Corresponding Author
AndreÌ Ferreira have been listed incorrectly. The correct affiliations are
presented below.
AndreÌ Ferreira a b c h i, Jianning Li a c d, Kelsey L. Pomykala a, Jens
Kleesiek a d e g, Victor Alves b, Jan Egger a c d f
a
Institute for AI in Medicine (IKIM), University Medicine Essen,
GirardetstraÃŸe 2, Essen, 45131, Germany
b
Center Algoritmi/LASI, University of Minho, Braga, 4710-057,
Portugal
c
Computer Algorithms for Medicine Laboratory, Graz, Austria
d
- Cancer Research Center Cologne Essen (CCCE), University MedÂ­
icine Essen, HufelandstraÃŸe 55, Essen, 45147, Germany

e

- German Cancer Consortium (DKTK), Partner Site Essen,
HufelandstraÃŸe 55, Essen, 45147, Germany
f
- Institute of Computer Graphics and Vision, Graz University of
Technology, Inffeldgasse 16, Graz, 801, Austria
g
- TU Dortmund University, Department of Physics, Otto-HahnStraÃŸe 4, 44227 Dortmund, Germany
h
Department of Oral and Maxillofacial Surgery, University Hospital
RWTH Aachen, 52074 Aachen, Germany
i
Institute of Medical Informatics, University Hospital RWTH Aachen,
52074 Aachen, Germany.
The authors would like to apologise for any inconvenience caused.

DOI of original article: https://doi.org/10.1016/j.media.2024.103100.
E-mail address: id10656@alunos.uminho.pt (A. Ferreira).
https://doi.org/10.1016/j.media.2024.103174
Available online 11 April 2024
1361-8415/Â© 2024 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

