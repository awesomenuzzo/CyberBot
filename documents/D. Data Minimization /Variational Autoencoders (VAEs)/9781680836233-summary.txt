Full text available at: http://dx.doi.org/10.1561/2200000056

An Introduction to
Variational Autoencoders

Full text available at: http://dx.doi.org/10.1561/2200000056

Other titles in Foundations and Trends R in Machine Learning

Computational Optimal Transport
Gabriel Peyre and Marco Cuturi
ISBN: 978-1-68083-550-2
An Introduction to Deep Reinforcement Learning
Vincent Francois-Lavet, Peter Henderson, Riashat Islam,
Marc G. Bellemare and Joelle Pineau
ISBN: 978-1-68083-538-0
An Introduction to Wishart Matrix Moments
Adrian N. Bishop, Pierre Del Moral and Angele Niclas
ISBN: 978-1-68083-506-9
A Tutorial on Thompson Sampling
Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband
and Zheng Wen
ISBN: 978-1-68083-470-3

Full text available at: http://dx.doi.org/10.1561/2200000056

An Introduction to
Variational Autoencoders
Diederik P. Kingma
Google
durk@google.com
Max Welling
University of Amsterdam
Qualcomm
mwelling@qti.qualcomm.com

Boston ‚Äî Delft

Full text available at: http://dx.doi.org/10.1561/2200000056

Foundations and Trends R in Machine Learning

Published, sold and distributed by:
now Publishers Inc.
PO Box 1024
Hanover, MA 02339
United States
Tel. +1-781-985-4510
www.nowpublishers.com
sales@nowpublishers.com
Outside North America:
now Publishers Inc.
PO Box 179
2600 AD Delft
The Netherlands
Tel. +31-6-51115274
The preferred citation for this publication is
D. P. Kingma and M. Welling. An Introduction to
Variational Autoencoders. Foundations and Trends R in Machine Learning, vol. 12,
no. 4, pp. 307‚Äì392, 2019.
ISBN: 978-1-68083-623-3
c 2019 D. P. Kingma and M. Welling

All rights reserved. No part of this publication may be reproduced, stored in a retrieval system,
or transmitted in any form or by any means, mechanical, photocopying, recording or otherwise,
without prior written permission of the publishers.
Photocopying. In the USA: This journal is registered at the Copyright Clearance Center, Inc., 222
Rosewood Drive, Danvers, MA 01923. Authorization to photocopy items for internal or personal
use, or the internal or personal use of specific clients, is granted by now Publishers Inc for users
registered with the Copyright Clearance Center (CCC). The ‚Äòservices‚Äô for users can be found on
the internet at: www.copyright.com
For those organizations that have been granted a photocopy license, a separate system of payment
has been arranged. Authorization does not extend to other kinds of copying, such as that for
general distribution, for advertising or promotional purposes, for creating new collective works,
or for resale. In the rest of the world: Permission to photocopy must be obtained from the
copyright owner. Please apply to now Publishers Inc., PO Box 1024, Hanover, MA 02339, USA;
Tel. +1 781 871 0245; www.nowpublishers.com; sales@nowpublishers.com
now Publishers Inc. has an exclusive license to publish this material worldwide. Permission
to use this content must be obtained from the copyright license holder. Please apply to now
Publishers, PO Box 179, 2600 AD Delft, The Netherlands, www.nowpublishers.com; e-mail:
sales@nowpublishers.com

Full text available at: http://dx.doi.org/10.1561/2200000056

Foundations and Trends R in Machine Learning
Volume 12, Issue 4, 2019
Editorial Board
Editor-in-Chief
Michael Jordan
University of California, Berkeley
United States

Editors
Peter Bartlett
UC Berkeley

Aapo Hyvarinen
Helsinki IIT

Luc de Raedt
KU Leuven

Yoshua Bengio
Universit√© de Montr√©al

Leslie Pack Kaelbling
MIT

Christian Robert
Paris-Dauphine

Avrim Blum
Toyota Technological
Institute

Michael Kearns
UPenn

Sunita Sarawagi
IIT Bombay

Daphne Koller
Stanford University

Robert Schapire
Microsoft Research

Craig Boutilier
University of Toronto
Stephen Boyd
Stanford University
Carla Brodley
Northeastern University
Inderjit Dhillon
Texas at Austin
Jerome Friedman
Stanford University
Kenji Fukumizu
ISM

John Lafferty
Yale
Michael Littman
Brown University
Gabor Lugosi
Pompeu Fabra
David Madigan
Columbia University
Pascal Massart
Universit√© de Paris-Sud

Zoubin Ghahramani
Cambridge University

Andrew McCallum
University of
Massachusetts Amherst

David Heckerman
Amazon

Marina Meila
University of Washington

Tom Heskes
Radboud University

Andrew Moore
CMU

Geoffrey Hinton
University of Toronto

John Platt
Microsoft Research

Bernhard Schoelkopf
Max Planck Institute
Richard Sutton
University of Alberta
Larry Wasserman
CMU
Bin Yu
UC Berkeley

Full text available at: http://dx.doi.org/10.1561/2200000056

Editorial Scope
Topics
Foundations and Trends R in Machine Learning publishes survey and tutorial
articles in the following topics:
‚Ä¢ Adaptive control and signal
processing
‚Ä¢ Applications and case studies
‚Ä¢ Behavioral, cognitive and
neural learning
‚Ä¢ Bayesian learning
‚Ä¢ Classification and prediction
‚Ä¢ Clustering
‚Ä¢ Data mining

‚Ä¢ Inductive logic programming
‚Ä¢ Kernel methods
‚Ä¢ Markov chain Monte Carlo
‚Ä¢ Model choice
‚Ä¢ Nonparametric methods
‚Ä¢ Online learning
‚Ä¢ Optimization
‚Ä¢ Reinforcement learning

‚Ä¢ Dimensionality reduction

‚Ä¢ Relational learning

‚Ä¢ Evaluation

‚Ä¢ Robustness

‚Ä¢ Game theoretic learning

‚Ä¢ Spectral methods

‚Ä¢ Graphical models

‚Ä¢ Statistical learning theory

‚Ä¢ Independent component
analysis

‚Ä¢ Variational inference
‚Ä¢ Visualization

Information for Librarians
Foundations and Trends R in Machine Learning, 2019, Volume 12, 6
issues. ISSN paper version 1935-8237. ISSN online version 1935-8245.
Also available as a combined paper and online subscription.

Full text available at: http://dx.doi.org/10.1561/2200000056

Contents

1 Introduction
2
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2 Aim . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3 Probabilistic Models and Variational Inference . . . . . . .
6
1.4 Parameterizing Conditional Distributions with Neural Networks 8
1.5 Directed Graphical Models and Neural Networks . . . . . .
9
1.6 Learning in Fully Observed Models with Neural Nets . . . 10
1.7 Learning and Inference in Deep Latent Variable Models . . 12
1.8 Intractabilities . . . . . . . . . . . . . . . . . . . . . . . . 13
2 Variational Autoencoders
2.1 Encoder or Approximate Posterior . . . . . . . . . . . . .
2.2 Evidence Lower Bound (ELBO) . . . . . . . . . . . . . . .
2.3 Stochastic Gradient-Based Optimization of the ELBO . . .
2.4 Reparameterization Trick . . . . . . . . . . . . . . . . . .
2.5 Factorized Gaussian posteriors . . . . . . . . . . . . . . .
2.6 Estimation of the Marginal Likelihood . . . . . . . . . . .
2.7 Marginal Likelihood and ELBO as KL Divergences . . . . .
2.8 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9 Related prior and concurrent work . . . . . . . . . . . . .

15
15
16
19
20
24
28
28
30
32

Full text available at: http://dx.doi.org/10.1561/2200000056

3 Beyond Gaussian Posteriors
3.1 Requirements for Computational Tractability . . . . . . . .
3.2 Improving the Flexibility of Inference Models . . . . . . . .
3.3 Inverse Autoregressive Transformations . . . . . . . . . . .
3.4 Inverse Autoregressive Flow (IAF) . . . . . . . . . . . . .
3.5 Related work . . . . . . . . . . . . . . . . . . . . . . . . .

37
37
38
41
42
46

4 Deeper Generative Models
48
4.1 Inference and Learning with Multiple Latent Variables . . . 48
4.2 Alternative methods for increasing expressivity . . . . . . . 51
4.3 Autoregressive Models . . . . . . . . . . . . . . . . . . . . 52
4.4 Invertible transformations with tractable Jacobian determinant 53
4.5 Follow-Up Work . . . . . . . . . . . . . . . . . . . . . . . 54
5 Conclusion

63

Acknowledgements

65

Appendices

66

A Appendix
A.1 Notation and definitions . . . . . . . . . . . . . . . . . . .
A.2 Alternative methods for learning in DLVMs . . . . . . . . .
A.3 Stochastic Gradient Descent . . . . . . . . . . . . . . . .

67
67
70
72

References

74

Full text available at: http://dx.doi.org/10.1561/2200000056

An Introduction to
Variational Autoencoders
Diederik P. Kingma1 and Max Welling2,3
1 Google; durk@google.com
2 University of Amsterdam
3 Qualcomm; mwelling@qti.qualcomm.com

ABSTRACT
Variational autoencoders provide a principled framework
for learning deep latent-variable models and corresponding
inference models. In this work, we provide an introduction
to variational autoencoders and some important extensions.

Diederik P. Kingma and Max Welling (2019), ‚ÄúAn Introduction to
Variational Autoencoders‚Äù, Foundations and Trends R in Machine Learning: Vol. 12,
No. 4, pp 307‚Äì392. DOI: 10.1561/2200000056.

Full text available at: http://dx.doi.org/10.1561/2200000056

1
Introduction

1.1

Motivation

One major division in machine learning is generative versus discriminative modeling. While in discriminative modeling one aims to learn a
predictor given the observations, in generative modeling one aims to
solve the more general problem of learning a joint distribution over all
the variables. A generative model simulates how the data is generated
in the real world. ‚ÄúModeling‚Äù is understood in almost every science as
unveiling this generating process by hypothesizing theories and testing
these theories through observations. For instance, when meteorologists
model the weather they use highly complex partial differential equations
to express the underlying physics of the weather. Or when an astronomer
models the formation of galaxies s/he encodes in his/her equations of
motion the physical laws under which stellar bodies interact. The same
is true for biologists, chemists, economists and so on. Modeling in the
sciences is in fact almost always generative modeling.
There are many reasons why generative modeling is attractive. First,
we can express physical laws and constraints into the generative process
while details that we don‚Äôt know or care about, i.e. nuisance variables,
are treated as noise. The resulting models are usually highly intuitive
2

Full text available at: http://dx.doi.org/10.1561/2200000056
1.1. Motivation

3

and interpretable and by testing them against observations we can
confirm or reject our theories about how the world works.
Another reason for trying to understand the generative process of
data is that it naturally expresses causal relations of the world. Causal
relations have the great advantage that they generalize much better to
new situations than mere correlations. For instance, once we understand
the generative process of an earthquake, we can use that knowledge
both in California and in Chile.
To turn a generative model into a discriminator, we need to use
Bayes rule. For instance, we have a generative model for an earthquake
of type A and another for type B, then seeing which of the two describes
the data best we can compute a probability for whether earthquake A
or B happened. Applying Bayes rule is however often computationally
expensive.
In discriminative methods we directly learn a map in the same
direction as we intend to make future predictions in. This is in the
opposite direction than the generative model. For instance, one can
argue that an image is generated in the world by first identifying the
object, then generating the object in 3D and then projecting it onto an
pixel grid. A discriminative model takes these pixel values directly as
input and maps them to the labels. While generative models can learn
efficiently from data, they also tend to make stronger assumptions on
the data than their purely discriminative counterparts, often leading
to higher asymptotic bias (Banerjee, 2007) when the model is wrong.
For this reason, if the model is wrong (and it almost always is to some
degree!), if one is solely interested in learning to discriminate, and
one is in a regime with a sufficiently large amount of data, then purely
discriminative models typically will lead to fewer errors in discriminative
tasks. Nevertheless, depending on how much data is around, it may pay
off to study the data generating process as a way to guide the training
of the discriminator, such as a classifier. For instance, one may have
few labeled examples and many more unlabeled examples. In this semisupervised learning setting, one can use the generative model of the data
to improve classification (Kingma et al., 2014; S√∏nderby et al., 2016a).
Generative modeling can be useful more generally. One can think
of it as an auxiliary task. For instance, predicting the immediate future

Full text available at: http://dx.doi.org/10.1561/2200000056
4

Introduction

may help us build useful abstractions of the world that can be used
for multiple prediction tasks downstream. This quest for disentangled,
semantically meaningful, statistically independent and causal factors
of variation in data is generally known as unsupervised representation
learning, and the variational autoencoder (VAE) has been extensively
employed for that purpose. Alternatively, one may view this as an
implicit form of regularization: by forcing the representations to be
meaningful for data generation, we bias the inverse of that process, which
maps from input to representation, into a certain mould. The auxiliary
task of predicting the world is used to better understand the world at
an abstract level and thus to better make downstream predictions.
The VAE can be viewed as two coupled, but independently parameterized models: the encoder or recognition model, and the decoder or
generative model. These two models support each other. The recognition model delivers to the generative model an approximation to its
posterior over latent random variables, which it needs to update its
parameters inside an iteration of ‚Äúexpectation maximization‚Äù learning.
Reversely, the generative model is a scaffolding of sorts for the recognition model to learn meaningful representations of the data, including
possibly class-labels. The recognition model is the approximate inverse
of the generative model according to Bayes rule.
One advantage of the VAE framework, relative to ordinary Variational Inference (VI), is that the recognition model (also called inference
model) is now a (stochastic) function of the input variables. This in
contrast to VI where each data-case has a separate variational distribution, which is inefficient for large data-sets. The recognition model uses
one set of parameters to model the relation between input and latent
variables and as such is called ‚Äúamortized inference‚Äù. This recognition
model can be arbitrary complex but is still reasonably fast because
by construction it can be done using a single feedforward pass from
input to latent variables. However the price we pay is that this sampling
induces sampling noise in the gradients required for learning. Perhaps
the greatest contribution of the VAE framework is the realization that
we can counteract this variance by using what is now known as the
‚Äúreparameterization trick‚Äù, a simple procedure to reorganize our gradient
computation that reduces variance in the gradients.

Full text available at: http://dx.doi.org/10.1561/2200000056
1.1. Motivation

5

The VAE is inspired by the Helmholtz Machine (Dayan et al., 1995)
which was perhaps the first model that employed a recognition model.
However, its wake-sleep algorithm was inefficient and didn‚Äôt optimize a
single objective. The VAE learning rules instead follow from a single
approximation to the maximum likelihood objective.
VAEs marry graphical models and deep learning. The generative
model is a Bayesian network of the form p(x|z)p(z), or, if there are
multiple stochastic latent layers, a hierarchy such as p(x|zL )p(zL |zL‚àí1 )
...p(z1 |z0 ). Similarly, the recognition model is also a conditional Bayesian
network of the form q(z|x) or as a hierarchy, such as q(z0 |z1 )...q(zL |X).
But inside each conditional may hide a complex (deep) neural network,
e.g. z|x ‚àº f (x, ), with f a neural network mapping and  a noise
random variable. Its learning algorithm is a mix of classical (amortized,
variational) expectation maximization but through the reparameterization trick ends up backpropagating through the many layers of the
deep neural networks embedded inside of it.
Since its inception, the VAE framework has been extended in many
directions, e.g. to dynamical models (Johnson et al., 2016), models with
attention (Gregor et al., 2015), models with multiple levels of stochastic
latent variables (Kingma et al., 2016), and many more. It has proven
itself as a fertile framework to build new models in. More recently,
another generative modeling paradigm has gained significant attention:
the generative adversarial network (GAN) (Goodfellow et al., 2014).
VAEs and GANs seem to have complementary properties: while GANs
can generate images of high subjective perceptual quality, they tend
to lack full support over the data (Grover et al., 2018), as opposed to
likelihood-based generative models. VAEs, like other likelihood-based
models, generate more dispersed samples, but are better density models
in terms of the likelihood criterion. As such many hybrid models have
been proposed to try to represent the best of both worlds (Dumoulin
et al., 2017; Grover et al., 2018; Rosca et al., 2018).
As a community we seem to have embraced the fact that generative
models and unsupervised learning play an important role in building
intelligent machines. We hope that the VAE provides a useful piece of
that puzzle.

Full text available at: http://dx.doi.org/10.1561/2200000056
6
1.2

Introduction
Aim

The framework of variational autoencoders (VAEs) (Kingma and Welling,
2014; Rezende et al., 2014) provides a principled method for jointly
learning deep latent-variable models and corresponding inference models
using stochastic gradient descent. The framework has a wide array
of applications from generative modeling, semi-supervised learning to
representation learning.
This work is meant as an expanded version of our earlier work
(Kingma and Welling, 2014), allowing us to explain the topic in finer
detail and to discuss a selection of important follow-up work. This is
not aimed to be a comprehensive review of all related work. We assume
that the reader has basic knowledge of algebra, calculus and probability
theory.
In this chapter we discuss background material: probabilistic models,
directed graphical models, the marriage of directed graphical models
with neural networks, learning in fully observed models and deep latentvariable models (DLVMs). In chapter 2 we explain the basics of VAEs.
In chapter 3 we explain advanced inference techniques, followed by an
explanation of advanced generative models in chapter 4. Please refer to
section A.1 for more information on mathematical notation.
1.3

Probabilistic Models and Variational Inference

In the field of machine learning, we are often interested in learning probabilistic models of various natural and artificial phenomena from data.
Probabilistic models are mathematical descriptions of such phenomena.
They are useful for understanding such phenomena, for prediction of
unknowns in the future, and for various forms of assisted or automated
decision making. As such, probabilistic models formalize the notion of
knowledge and skill, and are central constructs in the field of machine
learning and AI.
As probabilistic models contain unknowns and the data rarely paints
a complete picture of the unknowns, we typically need to assume some
level of uncertainty over aspects of the model. The degree and nature
of this uncertainty is specified in terms of (conditional) probability dis-

Full text available at: http://dx.doi.org/10.1561/2200000056
1.3. Probabilistic Models and Variational Inference

7

tributions. Models may consist of both continuous-valued variables and
discrete-valued variables. The, in some sense, most complete forms of
probabilistic models specify all correlations and higher-order dependencies between the variables in the model, in the form of a joint probability
distribution over those variables.
Let‚Äôs use x as the vector representing the set of all observed variables
whose joint distribution we would like to model. Note that for notational
simplicity and to avoid clutter, we use lower case bold (e.g. x) to denote
the underlying set of observed random variables, i.e. flattened and
concatenated such that the set is represented as a single vector. See
section A.1 for more on notation.
We assume the observed variable x is a random sample from an
unknown underlying process, whose true (probability) distribution p‚àó (x)
is unknown. We attempt to approximate this underlying process with a
chosen model pŒ∏ (x), with parameters Œ∏:
x ‚àº pŒ∏ (x)

(1.1)

Learning is, most commonly, the process of searching for a value of
the parameters Œ∏ such that the probability distribution function given
by the model, pŒ∏ (x), approximates the true distribution of the data,
denoted by p‚àó (x), such that for any observed x:
pŒ∏ (x) ‚âà p‚àó (x)

(1.2)

Naturally, we wish pŒ∏ (x) to be sufficiently flexible to be able to
adapt to the data, such that we have a chance of obtaining a sufficiently
accurate model. At the same time, we wish to be able to incorporate
knowledge about the distribution of data into the model that is known
a priori.
1.3.1

Conditional Models

Often, such as in case of classification or regression problems, we are not
interested in learning an unconditional model pŒ∏ (x), but a conditional
model pŒ∏ (y|x) that approximates the underlying conditional distribution
p‚àó (y|x): a distribution over the values of variable y, conditioned on the
value of an observed variable x. In this case, x is often called the input

Full text available at: http://dx.doi.org/10.1561/2200000056
8

Introduction

of the model. Like in the unconditional case, a model pŒ∏ (y|x) is chosen,
and optimized to be close to the unknown underlying distribution, such
that for any x and y:
pŒ∏ (y|x) ‚âà p‚àó (y|x)

(1.3)

A relatively common and simple example of conditional modeling is
image classification, where x is an image, and y is the image‚Äôs class, as
labeled by a human, which we wish to predict. In this case, pŒ∏ (y|x) is
typically chosen to be a categorical distribution, whose parameters are
computed from x.
Conditional models become more difficult to learn when the predicted variables are very high-dimensional, such as images, video or
sound. One example is the reverse of the image classification problem: prediction of a distribution over images, conditioned on the class
label. Another example with both high-dimensional input, and highdimensional output, is time series prediction, such as text or video
prediction.
To avoid notational clutter we will often assume unconditional modeling, but one should always keep in mind that the methods introduced
in this work are, in almost all cases, equally applicable to conditional
models. The data on which the model is conditioned, can be treated as
inputs to the model, similar to the parameters of the model, with the
obvious difference that one doesn‚Äôt optimize over their value.
1.4

Parameterizing Conditional Distributions with Neural Networks

Differentiable feed-forward neural networks, from here just called neural
networks, are a particularly flexible and computationally scalable type
of function approximator. Learning of models based on neural networks
with multiple ‚Äôhidden‚Äô layers of artificial neurons is often referred to
as deep learning (Goodfellow et al., 2016; LeCun et al., 2015). A
particularly interesting application is probabilistic models, i.e. the use of
neural networks for probability density functions (PDFs) or probability
mass functions (PMFs) in probabilistic models. Probabilistic models
based on neural networks are computationally scalable since they allow
for stochastic gradient-based optimization which, as we will explain,

Full text available at: http://dx.doi.org/10.1561/2200000056
1.5. Directed Graphical Models and Neural Networks

9

allows scaling to large models and large datasets. We will denote a deep
neural network as a vector function: NeuralNet(.).
At the time of writing, deep learning has been shown to work well for
a large variety of classification and regression problems, as summarized
in (LeCun et al., 2015; Goodfellow et al., 2016). In case of neuralnetwork based image classification LeCun et al., 1998, for example,
neural networks parameterize a categorical distribution pŒ∏ (y|x) over a
class label y, conditioned on an image x.
p = NeuralNet(x)

(1.4)

pŒ∏ (y|x) = Categorical(y; p)

(1.5)

where the last operation of NeuralNet(.) is typically a softmax() function
P
such that i pi = 1.
1.5

Directed Graphical Models and Neural Networks

We work with directed probabilistic models, also called directed probabilistic graphical models (PGMs), or Bayesian networks. Directed graphical models are a type of probabilistic models where all the variables
are topologically organized into a directed acyclic graph. The joint
distribution over the variables of such models factorizes as a product of
prior and conditional distributions:
pŒ∏ (x1 , ..., xM ) =

M
Y

pŒ∏ (xj |P a(xj ))

(1.6)

j=1

where P a(xj ) is the set of parent variables of node j in the directed graph.
For non-root-nodes, we condition on the parents. For root nodes, the set
of parents is the empty set, such that the distribution is unconditional.
Traditionally, each conditional probability distribution pŒ∏ (xj |P a(xj ))
is parameterized as a lookup table or a linear model (Koller and Friedman, 2009). As we explained above, a more flexible way to parameterize
such conditional distributions is with neural networks. In this case,
neural networks take as input the parents of a variable in a directed

Full text available at: http://dx.doi.org/10.1561/2200000056
10

Introduction

graph, and produce the distributional parameters Œ∑ over that variable:
Œ∑ = NeuralNet(P a(x))
pŒ∏ (x|P a(x)) = pŒ∏ (x|Œ∑)

(1.7)
(1.8)

We will now discuss how to learn the parameters of such models, if
all the variables are observed in the data.
1.6

Learning in Fully Observed Models with Neural Nets

If all variables in the directed graphical model are observed in the data,
then we can compute and differentiate the log-probability of the data
under the model, leading to relatively straightforward optimization.
1.6.1

Dataset

We often collect a dataset D consisting of N ‚â• 1 datapoints:
(1:N )
D = {x(1) , x(2) , ..., x(N ) } ‚â° {x(i) }N
i=1 ‚â° x

(1.9)

The datapoints are assumed to be independent samples from an unchanging underlying distribution. In other words, the dataset is assumed
to consist of distinct, independent measurements from the same (unchanging) system. In this case, the observations D = {x(i) }N
i=1 are said
to be i.i.d., for independently and identically distributed. Under the
i.i.d. assumption, the probability of the datapoints given the parameters factorizes as a product of individual datapoint probabilities. The
log-probability assigned to the data by the model is therefore given by:
log pŒ∏ (D) =

X

log pŒ∏ (x)

(1.10)

x‚ààD

1.6.2

Maximum Likelihood and Minibatch SGD

The most common criterion for probabilistic models is maximum loglikelihood (ML). As we will explain, maximization of the log-likelihood
criterion is equivalent to minimization of a Kullback Leibler divergence
between the data and model distributions.
Under the ML criterion, we attempt to find the parameters Œ∏ that
maximize the sum, or equivalently the average, of the log-probabilities

Full text available at: http://dx.doi.org/10.1561/2200000056
1.6. Learning in Fully Observed Models with Neural Nets

11

assigned to the data by the model. With i.i.d. dataset D of size ND ,
the maximum likelihood objective is to maximize the log-probability
given by equation (1.10).
Using calculus‚Äô chain rule and automatic differentiation tools, we can
efficiently compute gradients of this objective, i.e. the first derivatives
of the objective w.r.t. its parameters Œ∏. We can use such gradients to
iteratively hill-climb to a local optimum of the ML objective. If we
compute such gradients using all datapoints, ‚àáŒ∏ log pŒ∏ (D), then this
is known as batch gradient descent. Computation of this derivative is,
however, an expensive operation for large dataset size ND , since it scales
linearly with ND .
A more efficient method for optimization is stochastic gradient
descent (SGD) (section A.3), which uses randomly drawn minibatches
of data M ‚äÇ D of size NM . With such minibatches we can form an
unbiased estimator of the ML criterion:
1
1 X
1
log pŒ∏ (D) ‚âÉ
log pŒ∏ (M) =
log pŒ∏ (x)
(1.11)
ND
NM
NM x‚ààM
The ‚âÉ symbol means that one of the two sides is an unbiased estimator
of the other side. So one side (in this case the right-hand side) is a
random variable due to some noise source, and the two sides are equal
when averaged over the noise distribution. The noise source, in this case,
is the randomly drawn minibatch of data M. The unbiased estimator
log pŒ∏ (M) is differentiable, yielding the unbiased stochastic gradients:
1
1
1 X
‚àáŒ∏ log pŒ∏ (D) ‚âÉ
‚àáŒ∏ log pŒ∏ (M) =
‚àáŒ∏ log pŒ∏ (x)
ND
NM
NM x‚ààM
(1.12)
These gradients can be plugged into stochastic gradient-based optimizers;
see section A.3 for further discussion. In a nutshell, we can optimize the
objective function by repeatedly taking small steps in the direction of
the stochastic gradient.
1.6.3

Bayesian inference

From a Bayesian perspective, we can improve upon ML through maximum a posteriori (MAP) estimation (section section A.2.1), or, going

Full text available at: http://dx.doi.org/10.1561/2200000056
12

Introduction

even further, inference of a full approximate posterior distribution over
the parameters (see section A.1.4).
1.7
1.7.1

Learning and Inference in Deep Latent Variable Models
Latent Variables

We can extend fully-observed directed models, discussed in the previous
section, into directed models with latent variables. Latent variables are
variables that are part of the model, but which we don‚Äôt observe, and
are therefore not part of the dataset. We typically use z to denote such
latent variables. In case of unconditional modeling of observed variable
x, the directed graphical model would then represent a joint distribution
pŒ∏ (x, z) over both the observed variables x and the latent variables z.
The marginal distribution over the observed variables pŒ∏ (x), is given
by:
Z

pŒ∏ (x) =

pŒ∏ (x, z) dz

(1.13)

This is also called the (single datapoint) marginal likelihood or the model
evidence, when taken as a function of Œ∏.
Such an implicit distribution over x can be quite flexible. If z is
discrete and pŒ∏ (x|z) is a Gaussian distribution, then pŒ∏ (x) is a mixtureof-Gaussians distribution. For continuous z, pŒ∏ (x) can be seen as an
infinite mixture, which are potentially more powerful than discrete mixtures. Such marginal distributions are also called compound probability
distributions.
1.7.2

Deep Latent Variable Models

We use the term deep latent variable model (DLVM) to denote a latent
variable model pŒ∏ (x, z) whose distributions are parameterized by neural networks. Such a model can be conditioned on some context, like
pŒ∏ (x, z|y). One important advantage of DLVMs, is that even when each
factor (prior or conditional distribution) in the directed model is relatively simple (such as conditional Gaussian), the marginal distribution
pŒ∏ (x) can be very complex, i.e. contain almost arbitrary dependen-

Full text available at: http://dx.doi.org/10.1561/2200000056
1.8. Intractabilities

13

cies. This expressivity makes deep latent-variable models attractive for
approximating complicated underlying distributions p‚àó (x).
Perhaps the simplest, and most common, DLVM is one that is
specified as factorization with the following structure:
pŒ∏ (x, z) = pŒ∏ (z)pŒ∏ (x|z)

(1.14)

where pŒ∏ (z) and/or pŒ∏ (x|z) are specified. The distribution p(z) is often
called the prior distribution over z, since it is not conditioned on any
observations.
1.7.3

Example DLVM for multivariate Bernoulli data

A simple example DLVM, used in (Kingma and Welling, 2014) for
binary data x, is with a spherical Gaussian latent space, and a factorized
Bernoulli observation model:
p(z) = N (z; 0, I)

(1.15)

p = DecoderNeuralNetŒ∏ (z)
log p(x|z) =

D
X
j=1

=

D
X

log p(xj |z) =

D
X

log Bernoulli(xj ; pj )

(1.16)
(1.17)

j=1

xj log pj + (1 ‚àí xj ) log(1 ‚àí pj )

(1.18)

j=1

where ‚àÄpj ‚àà p : 0 ‚â§ pj ‚â§ 1 (e.g. implemented through a sigmoid
nonlinearity as the last layer of the DecoderNeuralNetŒ∏ (.)), where D
is the dimensionality of x, and Bernoulli(.; p) is the probability mass
function (PMF) of the Bernoulli distribution.
1.8

Intractabilities

The main difficulty of maximum likelihood learning in DLVMs is that
the marginal probability of data under the model is typically intractable.
This is due to the integral in equation (1.13) for computing the marginal
R
likelihood (or model evidence), pŒ∏ (x) = pŒ∏ (x, z) dz, not having an
analytic solution or efficient estimator. Due to this intractability, we

Full text available at: http://dx.doi.org/10.1561/2200000056
14

Introduction

cannot differentiate it w.r.t. its parameters and optimize it, as we can
with fully observed models.
The intractability of pŒ∏ (x), is related to the intractability of the
posterior distribution pŒ∏ (z|x). Note that the joint distribution pŒ∏ (x, z)
is efficient to compute, and that the densities are related through the
basic identity:
pŒ∏ (z|x) =

pŒ∏ (x, z)
pŒ∏ (x)

(1.19)

Since pŒ∏ (x, z) is tractable to compute, a tractable marginal likelihood
pŒ∏ (x) leads to a tractable posterior pŒ∏ (z|x), and vice versa. Both are
intractable in DLVMs.
Approximate inference techniques (see also section A.2) allow us to
approximate the posterior pŒ∏ (z|x) and the marginal likelihood pŒ∏ (x) in
DLVMs. Traditional inference methods are relatively expensive. Such
methods, for example, often require a per-datapoint optimization loop,
or yield bad posterior approximations. We would like to avoid such
expensive procedures.
Likewise, the posterior over the parameters of (directed models
parameterized with) neural networks, p(Œ∏|D), is generally intractable
to compute exactly, and requires approximate inference techniques.

Full text available at: http://dx.doi.org/10.1561/2200000056

Acknowledgements

We are grateful for the help of Tim Salimans, Alec Radford, Rif A.
Saurous and others who have given us valuable feedback at various
stages of writing.

65

Full text available at: http://dx.doi.org/10.1561/2200000056

Appendices

Full text available at: http://dx.doi.org/10.1561/2200000056

A
Appendix

A.1

Notation and definitions

A.1.1

Notation

Example(s)

Description

x, y z

With characters in bold we typically denote random vectors. We also use this notation for collections of random variables variables.

x, y, z

With characters in italic we typically denote
random scalars, i.e. single real-valued numbers.

X, Y, Z

With bold and capitalized letters we typically
denote random matrices.

P a(z)

The parents of random variable z in a directed
graph.

diag(x)

Diagonal matrix, with the values of vector x on
the diagonal.

67

Full text available at: http://dx.doi.org/10.1561/2200000056
68

Appendix

x

y

Element-wise multiplication of two vectors. The
resulting vector is (x1 y1 , ..., xK yK )T .

Œ∏

Parameters of a (generative) model are typically
denoted with the Greek lowercase letter Œ∏ (theta).

œÜ

Variational parameters are typically denoted
with the bold Greek letter œÜ (phi).

p(x), p(z)

Probability density functions (PDFs) and probability mass functions (PMFs), also simply called
distributions, are denoted by p(.), q(.) or r(.).

p(x, y, z)

Joint distributions are denoted by p(., .)

p(x|z)

Conditional distributions are denoted by p(.|.)

p(.; Œ∏), pŒ∏ (x)

The parameters of a distribution are denoted
with p(.; Œ∏) or equivalently with subscript pŒ∏ (.).

p(x
=
p(x ‚â§ a)

a), We may use an (in-)equality sign within a probability distribution to distinguish between function arguments and value at which to evaluate.
So p(x = a) denotes a PDF or PMF over variable
x evaluated at the value of variable a. Likewise,
p(x ‚â§ a) denotes a CDF evaluated at the value
of a.

p(.), q(.)

We use different letters to refer to different
probabilistic models, such as p(.) or q(.). Conversely, we use the same letter across different
marginals/conditionals to indicate they relate to
the same probabilistic model.

A.1.2

Definitions

Term

Description

Full text available at: http://dx.doi.org/10.1561/2200000056
A.1. Notation and definitions

69

Probability
A function that assigns a probability density to
density func- each possible value of given continuous random
tion (PDF)
variables.
Cumulative
distribution
function
(CDF)

A function that assigns a cumulative probability
density to each possible value of given univariate
continuous random variables.

Probability
mass function
(PMF)

A function that assigns a probability mass to
given discrete random variable.

A.1.3

Distributions

We overload the notation of distributions (e.g. p(x) = N (x; ¬µ, Œ£)) with
two meanings: (1) a distribution from which we can sample, and (2)
the probability density function (PDF) of that distribution.
Term

Description

Categorical(x; p)

Categorical distribution, with parameter p
P
such that i pi = 1.

Bernoulli(x; p)

Multivariate distribution of independent
Bernoulli.
Bernoulli(x; p) =
‚àÄi : 0 ‚â§ pi ‚â§ 1.

Q

i Bernoulli(xi ; pi )

with

Normal(x; ¬µ, Œ£) = Multivariate Normal distribution with mean
N (x; ¬µ, Œ£)
¬µ and covariance Œ£.
Chain rule of probability
p(a, b) = p(a)p(b|a)

(A.1)

Full text available at: http://dx.doi.org/10.1561/2200000056
70

Appendix

Bayes‚Äô Rule
p(a|b) = p(b|a)p(a)/p(b)
A.1.4

(A.2)

Bayesian Inference

Let p(Œ∏) be a chosen marginal distribution over its parameters Œ∏, called
a prior distribution. Let D be observed data, p(D|Œ∏) ‚â° pŒ∏ (D) be the
probability assigned to the data under the model with parameters Œ∏.
Recall the chain rule in probability:
p(Œ∏, D) = p(Œ∏|D)p(D) = p(Œ∏)p(D|Œ∏)
Simply re-arranging terms above, the posterior distribution over the
parameters Œ∏, taking into account the data D, is:
p(Œ∏|D) =

p(D|Œ∏)p(Œ∏)
‚àù p(D|Œ∏)p(Œ∏)
p(D)

(A.3)

where the proportionality (‚àù) holds since p(D) is a constant that is
not dependent on parameters Œ∏. The formula above is known as Bayes‚Äô
rule, a fundamental formula in machine learning and statistics, and is
of special importance to this work.
A principal application of Bayes‚Äô rule is that it allows us to make
predictions about future data x0 , that are optimal as long as the prior
p(Œ∏) and model class pŒ∏ (x) are correct:
0

p(x = x |D) =
A.2
A.2.1

Z

pŒ∏ (x = x0 )p(Œ∏|D)dŒ∏

Alternative methods for learning in DLVMs
Maximum A Posteriori

From a Bayesian perspective, we can improve upon the maximum
likelihood objective through maximum a posteriori (MAP) estimation,
which maximizes the log-posterior w.r.t. Œ∏. With i.i.d. data D, this is:
LM AP (Œ∏) = log p(Œ∏|D)
= log p(Œ∏) + LM L (Œ∏) + constant

(A.4)
(A.5)

Full text available at: http://dx.doi.org/10.1561/2200000056
A.2. Alternative methods for learning in DLVMs

71

The prior p(Œ∏) in equation (A.5) has diminishing effect for increasingly
large N . For this reason, in case of optimization with large datasets,
we often choose to simply use the maximum likelihood criterion by
omitting the prior from the objective, which is numerically equivalent
to setting p(Œ∏) = constant.
A.2.2

Variational EM with local variational parameters

Expectation Maximization (EM) is a general strategy for learning
parameters in partially observed models (Dempster et al., 1977). See
section A.2.3 for a discussion of EM using MCMC. The method can be
explained as coordinate ascent on the ELBO (Neal and Hinton, 1998).
In case of of i.i.d. data, traditional variational EM methods estimate
local variational parameters œÜ(i) , i.e. a separate set of variational
parameters per datapoint i in the dataset. In contrast, VAEs employ a
strategy with global variational parameters.
EM starts out with some (random) initial choice of Œ∏ and œÜ(1:N ) . It
then iteratively applies updates:
‚àÄi = 1, ..., N : œÜ(i) ‚Üê argmax L(x(i) ; Œ∏, œÜ)

(E-step)

(A.6)

(M-step)

(A.7)

œÜ

Œ∏ ‚Üê argmax
Œ∏

N
X

L(x(i) ; Œ∏, œÜ)

i=1

until convergence. Why does this work? Note that at the E-step:
argmax L(x; Œ∏, œÜ)

(A.8)

œÜ

= argmax [log pŒ∏ (x) ‚àí DKL (qœÜ (z|x)||pŒ∏ (z|x))]

(A.9)

œÜ

= argmin DKL (qœÜ (z|x)||pŒ∏ (z|x))

(A.10)

œÜ

so the E-step, sensibly, minimizes the KL divergence of qœÜ (z|x) from
the true posterior.
Secondly, note that if qœÜ (z|x) equals pŒ∏ (z|x), the ELBO equals the
marginal likelihood, but that for any choice of qœÜ (z|x), the M -step
optimizes a bound on the marginal likelihood. The tightness of this
bound is defined by DKL (qœÜ (z|x)||pŒ∏ (z|x)).

Full text available at: http://dx.doi.org/10.1561/2200000056
72
A.2.3

Appendix
MCMC-EM

Another Bayesian approach towards optimizing the likelihood pŒ∏ (x)
with DLVMs is Expectation Maximization (EM) with Markov Chain
Monte Carlo (MCMC). In case of MCMC, the posterior is approximated
by a mixture of a set of approximately i.i.d. samples from the posterior,
acquired by running a Markov chain. Note that posterior gradients
in DLVMs are relatively affordable to compute by differentiating the
log-joint distribution w.r.t. z:
‚àáz log pŒ∏ (z|x) = ‚àáz log[pŒ∏ (x, z)/pŒ∏ (x)]

(A.11)

= ‚àáz [log pŒ∏ (x, z) ‚àí log pŒ∏ (x)]

(A.12)

= ‚àáz log pŒ∏ (x, z) ‚àí ‚àáz log pŒ∏ (x)

(A.13)

= ‚àáz log pŒ∏ (x, z)

(A.14)

One version of MCMC which uses such posterior for relatively fast
convergence, is Hamiltonian MCMC (Neal, 2011). A disadvantage of
this approach is the requirement for running an independent MCMC
chain per datapoint.
A.3

Stochastic Gradient Descent

We work with directed models where the objective per datapoint is
scalar, and due to the differentiability of neural networks that compose
them, the objective is differentiable w.r.t. its parameters Œ∏. Due to the
remarkable efficiency of reverse-mode automatic differentiation (also
known as the backpropagation algorithm (Rumelhart et al., 1988)), the
value and gradient (i.e. the vector of partial derivatives) of differentiable
scalar objectives can be computed with equal time complexity. In SGD,
we iteratively update parameters Œ∏:
Œ∏t+1 ‚Üê Œ∏t + Œ±t ¬∑ ‚àáŒ∏ LÃÉ(Œ∏, Œæ)

(A.15)

where Œ±t is a learning rate or preconditioner,h and LÃÉ(Œ∏,
i Œæ) is an unbiased
estimate of the objective L(Œ∏), i.e. EŒæ‚àºp(Œæ) LÃÉ(Œ∏, Œæ) = L(Œ∏). The random variable Œæ could e.g. be a datapoint index, uniformly sampled from
{1, ..., N }, but can also include different types of noise such posterior
sampling noise in VAEs. In experiments, we have typically used the

Full text available at: http://dx.doi.org/10.1561/2200000056
A.3. Stochastic Gradient Descent

73

Adam and Adamax optimization methods for choosing Œ±t (Kingma and
Ba, 2015); these methods are invariant to constant rescaling of the objective, and invariant to constant re-scalings of the individual gradients.
As a result, LÃÉ(Œ∏, Œæ) only needs to be unbiased up to proportionality. We
iteratively apply eq. (A.15) until a stopping criterion is met. A simple
but effective criterion is to stop optimization as soon as the probability
of a holdout set of data starts decreasing; this criterion is called early
stopping.

Full text available at: http://dx.doi.org/10.1561/2200000056

References

Banerjee, A. 2007. ‚ÄúAn analysis of logistic models: Exponential family
connections and online performance‚Äù. In: Proceedings of the 2007
SIAM International Conference on Data Mining. SIAM. 204‚Äì215.
Bayer, J. and C. Osendorfer. 2014. ‚ÄúLearning stochastic recurrent networks‚Äù. In: NIPS 2014 Workshop on Advances in Variational Inference.
Bengio, Y., A. Courville, and P. Vincent. 2013. Representation Learning:
A Review and New Perspectives. IEEE.
Bengio, Y., E. Laufer, G. Alain, and J. Yosinski. 2014. ‚ÄúDeep generative stochastic networks trainable by backprop‚Äù. In: International
Conference on Machine Learning. 226‚Äì234.
Berg, R. v. d., L. Hasenclever, J. M. Tomczak, and M. Welling. 2017.
‚ÄúSylvester Normalizing Flows for Variational Inference‚Äù. Conference
on Uncertainty in Artificial Intelligence.
Blei, D. M., M. I. Jordan, and J. W. Paisley. 2012. ‚ÄúVariational Bayesian
inference with stochastic search‚Äù. In: International Conference on
Machine Learning. 1367‚Äì1374.
Blundell, C., J. Cornebise, K. Kavukcuoglu, and D. Wierstra. 2015.
‚ÄúWeight Uncertainty in Neural Networks‚Äù. In: International Conference on Machine Learning. 1613‚Äì1622.

74

Full text available at: http://dx.doi.org/10.1561/2200000056
References

75

Bornschein, J., S. Shabanian, A. Fischer, and Y. Bengio. 2016. ‚ÄúBidirectional Helmholtz machines‚Äù. In: Proceedings of the 33rd International
Conference on International Conference on Machine Learning. 2511‚Äì
2519.
Bourlard, H. and Y. Kamp. 1988. ‚ÄúAuto-association by multilayer perceptrons and singular value decomposition‚Äù. Biological Cybernetics.
59(4-5): 291‚Äì294.
Bowman, S. R., L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and
S. Bengio. 2015. ‚ÄúGenerating sentences from a continuous space‚Äù.
arXiv preprint arXiv:1511.06349.
Brock, A., T. Lim, J. M. Ritchie, and N. J. Weston. 2017. ‚ÄúNeural photo
editing with introspective adversarial networks‚Äù. In: International
Conference on Learning Representations.
Burda, Y., R. Grosse, and R. Salakhutdinov. 2015. ‚ÄúImportance weighted
autoencoders‚Äù. arXiv preprint arXiv:1509.00519.
Chen, R. T., X. Li, R. Grosse, and D. Duvenaud. 2018. ‚ÄúIsolating
sources of disentanglement in VAEs‚Äù. In: Proceedings of the 32nd
International Conference on Neural Information Processing Systems.
Curran Associates Inc. 2615‚Äì2625.
Chen, X., D. P. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Schulman, I. Sutskever, and P. Abbeel. 2017. ‚ÄúVariational lossy autoencoder‚Äù. International Conference on Learning Representations.
Chung, J., K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio.
2015. ‚ÄúA recurrent latent variable model for sequential data‚Äù. In:
Advances in neural information processing systems. 2980‚Äì2988.
Cremer, C., Q. Morris, and D. Duvenaud. 2017. ‚ÄúRe-interpreting importance weighted autoencoders‚Äù. International Conference on Learning
Representations.
Dayan, P., G. E. Hinton, R. M. Neal, and R. S. Zemel. 1995. ‚ÄúThe
Helmholtz machine‚Äù. Neural computation. 7(5): 889‚Äì904.
Deco, G. and W. Brauer. 1995. ‚ÄúHigher order statistical decorrelation without information loss‚Äù. Advances in Neural Information
Processing Systems: 247‚Äì254.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. ‚ÄúMaximum
likelihood from incomplete data via the EM algorithm‚Äù. Journal of
the Royal Statistical Society. Series B (Methodological): 1‚Äì38.

Full text available at: http://dx.doi.org/10.1561/2200000056
76

References

Deshpande, A., J. Lu, M.-C. Yeh, M. Jin Chong, and D. Forsyth. 2017.
‚ÄúLearning diverse image colorization‚Äù. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 6837‚Äì
6845.
Dinh, L., D. Krueger, and Y. Bengio. 2014. ‚ÄúNICE: Non-linear independent components estimation‚Äù. arXiv preprint arXiv:1410.8516.
Dinh, L., J. Sohl-Dickstein, and S. Bengio. 2016. ‚ÄúDensity estimation
using Real NVP‚Äù. arXiv preprint arXiv:1605.08803.
Dosovitskiy, A., J. Tobias Springenberg, and T. Brox. 2015. ‚ÄúLearning
to generate chairs with convolutional neural networks‚Äù. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 1538‚Äì1546.
Dumoulin, V., I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. 2017. ‚ÄúAdversarially learned inference‚Äù.
International Conference on Learning Representations.
Edwards, H. and A. Storkey. 2017. ‚ÄúTowards a neural statistician‚Äù.
International Conference on Learning Representations.
Eslami, S. A., N. Heess, T. Weber, Y. Tassa, D. Szepesvari, G. E. Hinton,
et al. 2016. ‚ÄúAttend, infer, repeat: Fast scene understanding with
generative models‚Äù. In: Advances In Neural Information Processing
Systems. 3225‚Äì3233.
Fan, K., Z. Wang, J. Beck, J. Kwok, and K. A. Heller. 2015. ‚ÄúFast
second order stochastic backpropagation for variational inference‚Äù.
In: Advances in Neural Information Processing Systems. 1387‚Äì1395.
Fortunato, M., C. Blundell, and O. Vinyals. 2017. ‚ÄúBayesian recurrent
neural networks‚Äù. arXiv preprint arXiv:1704.02798.
Fraccaro, M., S. K. S√∏nderby, U. Paquet, and O. Winther. 2016. ‚ÄúSequential neural models with stochastic layers‚Äù. In: Advances in Neural
Information Processing Systems. 2199‚Äì2207.
Fu, M. C. 2006. ‚ÄúGradient estimation‚Äù. Handbooks in Operations Research and Management Science. 13: 575‚Äì616.
Gal, Y. and Z. Ghahramani. 2016. ‚ÄúA theoretically grounded application
of dropout in recurrent neural networks‚Äù. In: Advances in neural
information processing systems. 1019‚Äì1027.

Full text available at: http://dx.doi.org/10.1561/2200000056
References

77

Germain, M., K. Gregor, I. Murray, and H. Larochelle. 2015. ‚ÄúMade:
Masked autoencoder for distribution estimation‚Äù. In: International
Conference on Machine Learning. 881‚Äì889.
Gershman, S. and N. Goodman. 2014. ‚ÄúAmortized inference in probabilistic reasoning.‚Äù In: CogSci.
Glasserman, P. 2013. Monte Carlo methods in financial engineering.
Vol. 53. Springer Science & Business Media.
Glynn, P. W. 1990. ‚ÄúLikelihood ratio gradient estimation for stochastic
systems‚Äù. Communications of the ACM. 33(10): 75‚Äì84.
G√≥mez-Bombarelli, R., J. N. Wei, D. Duvenaud, J. M. Hern√°ndezLobato, B. S√°nchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre,
T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. 2018. ‚ÄúAutomatic
chemical design using a data-driven continuous representation of
molecules‚Äù. ACS central science. 4(2): 268‚Äì276.
Goodfellow, I., Y. Bengio, and A. Courville. 2016. Deep learning. MIT
press.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio. 2014. ‚ÄúGenerative adversarial
nets‚Äù. In: Advances in Neural Information Processing Systems. 2672‚Äì
2680.
Graves, A. 2011. ‚ÄúPractical variational inference for neural networks‚Äù.
In: Advances in Neural Information Processing Systems. 2348‚Äì2356.
Gregor, K., F. Besse, D. J. Rezende, I. Danihelka, and D. Wierstra.
2016. ‚ÄúTowards conceptual compression‚Äù. In: Advances In Neural
Information Processing Systems. 3549‚Äì3557.
Gregor, K., I. Danihelka, A. Graves, D. Rezende, and D. Wierstra. 2015.
‚ÄúDRAW: A Recurrent Neural Network For Image Generation‚Äù. In:
International Conference on Machine Learning. 1462‚Äì1471.
Gregor, K., I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra. 2014.
‚ÄúDeep AutoRegressive Networks‚Äù. In: International Conference on
Machine Learning. 1242‚Äì1250.
Grover, A., M. Dhar, and S. Ermon. 2018. ‚ÄúFlow-GAN: Combining
maximum likelihood and adversarial learning in generative models‚Äù.
In: AAAI Conference on Artificial Intelligence.

Full text available at: http://dx.doi.org/10.1561/2200000056
78

References

Gu, S., S. Levine, I. Sutskever, and A. Mnih. 2015. ‚ÄúMuProp: Unbiased
backpropagation for stochastic neural networks‚Äù. arXiv preprint
arXiv:1511.05176.
Gulrajani, I., K. Kumar, F. Ahmed, A. A. Taiga, F. Visin, D. Vazquez,
and A. Courville. 2017. ‚ÄúPixelVAE: A latent variable model for natural images‚Äù. International Conference on Learning Representations.
He, K., X. Zhang, S. Ren, and J. Sun. 2015. ‚ÄúDelving deep into rectifiers:
Surpassing human-level performance on imagenet classification‚Äù. In:
Proceedings of the IEEE International Conference on Computer
Vision. 1026‚Äì1034.
He, K., X. Zhang, S. Ren, and J. Sun. 2016. ‚ÄúDeep residual learning
for image recognition‚Äù. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. 770‚Äì778.
Heess, N., G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. 2015.
‚ÄúLearning continuous control policies by stochastic value gradients‚Äù.
In: Advances in Neural Information Processing Systems. 2944‚Äì2952.
Hern√°ndez-Lobato, J. M., Y. Li, M. Rowland, D. Hern√°ndez-Lobato, T.
Bui, and R. E. Turner. 2016. ‚ÄúBlack-box Œ±-divergence minimization‚Äù.
Higgins, I., L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S.
Mohamed, and A. Lerchner. 2017. ‚Äúbeta-vae: Learning basic visual
concepts with a constrained variational framework‚Äù. International
Conference on Learning Representations.
Hinton, G. E., P. Dayan, B. J. Frey, and R. M. Neal. 1995. ‚ÄúThe
"Wake-Sleep" algorithm for unsupervised neural networks‚Äù. Science:
1158‚Äì1158.
Hochreiter, S. and J. Schmidhuber. 1997. ‚ÄúLong Short-Term Memory‚Äù.
Neural Computation. 9(8): 1735‚Äì1780.
Hoffman, M. D., D. M. Blei, C. Wang, and J. Paisley. 2013. ‚ÄúStochastic
variational inference‚Äù. The Journal of Machine Learning Research.
14(1): 1303‚Äì1347.
Hoffman, M. D. and M. J. Johnson. 2016. ‚ÄúElbo surgery: yet another
way to carve up the variational evidence lower bound‚Äù. In: Workshop
in Advances in Approximate Bayesian Inference, NIPS.

Full text available at: http://dx.doi.org/10.1561/2200000056
References

79

Houthooft, R., X. Chen, Y. Duan, J. Schulman, F. De Turck, and P.
Abbeel. 2016. ‚ÄúVime: Variational information maximizing exploration‚Äù. In: Advances in Neural Information Processing Systems.
1109‚Äì1117.
Hsu, C.-C., H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang. 2016.
‚ÄúVoice conversion from non-parallel corpora using variational autoencoder‚Äù. In: Signal and Information Processing Association Annual
Summit and Conference (APSIPA), 2016 Asia-Pacific. IEEE. 1‚Äì6.
Hsu, C.-C., H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang. 2017.
‚ÄúVoice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks‚Äù. arXiv preprint
arXiv:1704.00849.
Hu, Z., Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. 2017.
‚ÄúControllable text generation‚Äù. arXiv preprint arXiv:1703.00955.
Huang, C.-W., D. Krueger, A. Lacoste, and A. Courville. 2018. ‚ÄúNeural
Autoregressive Flows‚Äù. In: International Conference on Machine
Learning. 2083‚Äì2092.
Jang, E., S. Gu, and B. Poole. 2017. ‚ÄúCategorical Reparameterization with Gumbel-Softmax‚Äù. International Conference on Learning
Representations.
Johnson, M., D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R.
Datta. 2016. ‚ÄúComposing graphical models with neural networks
for structured representations and fast inference‚Äù. In: Advances in
Neural Information Processing Systems. 2946‚Äì2954.
Jozefowicz, R., W. Zaremba, and I. Sutskever. 2015. ‚ÄúAn empirical
exploration of recurrent network architectures‚Äù. In: International
Conference on Machine Learning. 2342‚Äì2350.
Karl, M., M. Soelch, J. Bayer, and P. van der Smagt. 2017. ‚ÄúDeep variational bayes filters: Unsupervised learning of state space models from
raw data‚Äù. International Conference on Learning Representations.
Kavukcuoglu, K., M. Ranzato, and Y. LeCun. 2008. ‚ÄúFast inference in
sparse coding algorithms with applications to object recognition‚Äù.
Tech. rep. No. CBLL-TR-2008-12-01. Computational and Biological
Learning Lab, Courant Institute, NYU.

Full text available at: http://dx.doi.org/10.1561/2200000056
80

References

Kingma, D. P., S. Mohamed, D. J. Rezende, and M. Welling. 2014. ‚ÄúSemisupervised learning with deep generative models‚Äù. In: Advances in
Neural Information Processing Systems. 3581‚Äì3589.
Kingma, D. P., T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever,
and M. Welling. 2016. ‚ÄúImproved variational inference with inverse
autoregressive flow‚Äù. In: Advances in Neural Information Processing
Systems. 4743‚Äì4751.
Kingma, D. P., T. Salimans, and M. Welling. 2015. ‚ÄúVariational dropout
and the local reparameterization trick‚Äù. In: Advances in Neural
Information Processing Systems. 2575‚Äì2583.
Kingma, D. P. and M. Welling. 2014. ‚ÄúAuto-Encoding Variational
Bayes‚Äù. International Conference on Learning Representations.
Kingma, D. and J. Ba. 2015. ‚ÄúAdam: A Method for Stochastic Optimization‚Äù. International Conference on Learning Representations.
Kipf, T. N. and M. Welling. 2016. ‚ÄúVariational graph auto-encoders‚Äù.
arXiv preprint arXiv:1611.07308.
Kleijnen, J. P. and R. Y. Rubinstein. 1996. ‚ÄúOptimization and sensitivity analysis of computer simulation models by the score function
method‚Äù. European Journal of Operational Research. 88(3): 413‚Äì427.
Koller, D. and N. Friedman. 2009. Probabilistic graphical models: Principles and techniques. MIT press.
Krishnan, R. G., U. Shalit, and D. Sontag. 2017. ‚ÄúStructured Inference
Networks for Nonlinear State Space Models.‚Äù In: AAAI. 2101‚Äì2109.
Kucukelbir, A., D. Tran, R. Ranganath, A. Gelman, and D. M. Blei.
2017. ‚ÄúAutomatic differentiation variational inference‚Äù. The Journal
of Machine Learning Research. 18(1): 430‚Äì474.
Kulkarni, T. D., W. F. Whitney, P. Kohli, and J. Tenenbaum. 2015.
‚ÄúDeep convolutional inverse graphics network‚Äù. In: Advances in
Neural Information Processing Systems. 2539‚Äì2547.
Kusner, M. J., B. Paige, and J. M. Hern√°ndez-Lobato. 2017. ‚ÄúGrammar
variational autoencoder‚Äù. In: Proceedings of the 34th International
Conference on Machine Learning-Volume 70. 1945‚Äì1954.
Larsen, A. B. L., S. K. S√∏nderby, H. Larochelle, and O. Winther. 2016.
‚ÄúAutoencoding beyond pixels using a learned similarity metric‚Äù. In:
International Conference on Machine Learning. 1558‚Äì1566.

Full text available at: http://dx.doi.org/10.1561/2200000056
References

81

L√°zaro-Gredilla, M. 2014. ‚ÄúDoubly stochastic variational Bayes for
non-conjugate inference‚Äù. In: International Conference on Machine
Learning.
LeCun, Y., Y. Bengio, and G. Hinton. 2015. ‚ÄúDeep Learning‚Äù. Nature.
521(7553): 436‚Äì444.
LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. ‚ÄúGradient-based
learning applied to document recognition‚Äù. Proceedings of the IEEE.
86(11): 2278‚Äì2324.
Li, Y. and R. E. Turner. 2016. ‚ÄúR√©nyi divergence variational inference‚Äù.
In: Advances in Neural Information Processing Systems. 1073‚Äì1081.
Li, Y., K. Swersky, and R. S. Zemel. 2015. ‚ÄúGenerative moment matching
networks‚Äù. In: International Conference on Machine Learning. 1718‚Äì
1727.
Linsker, R. 1989. An Application of the Principle of Maximum Information Preservation to Linear Systems. Morgan Kaufmann Publishers
Inc.
Louizos, C., K. Swersky, Y. Li, M. Welling, and R. Zemel. 2015. ‚ÄúThe
variational fair autoencoder‚Äù. arXiv preprint arXiv:1511.00830.
Louizos, C., K. Ullrich, and M. Welling. 2017. ‚ÄúBayesian compression
for deep learning‚Äù. In: Advances in Neural Information Processing
Systems. 3288‚Äì3298.
Louizos, C. and M. Welling. 2016. ‚ÄúStructured and efficient variational
deep learning with matrix gaussian posteriors‚Äù. In: International
Conference on Machine Learning. 1708‚Äì1716.
Louizos, C. and M. Welling. 2017. ‚ÄúMultiplicative normalizing flows for
variational Bayesian neural networks‚Äù. In: International Conference
on Machine Learning. 2218‚Äì2227.
Maal√∏e, L., C. K. S√∏nderby, S. K. S√∏nderby, and O. Winther. 2016.
‚ÄúAuxiliary deep generative models‚Äù. In: International Conference on
Machine Learning.
Maddison, C. J., A. Mnih, and Y. W. Teh. 2017. ‚ÄúThe concrete distribution: A continuous relaxation of discrete random variables‚Äù.
International Conference on Learning Representations.
Makhzani, A., J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. 2015.
‚ÄúAdversarial autoencoders‚Äù. arXiv preprint arXiv:1511.05644.

Full text available at: http://dx.doi.org/10.1561/2200000056
82

References

Mansimov, E., E. Parisotto, J. L. Ba, and R. Salakhutdinov. 2015.
‚ÄúGenerating images from captions with attention‚Äù. arXiv preprint
arXiv:1511.02793.
Miao, Y., L. Yu, and P. Blunsom. 2016. ‚ÄúNeural variational inference for
text processing‚Äù. In: International Conference on Machine Learning.
1727‚Äì1736.
Mnih, A. and K. Gregor. 2014. ‚ÄúNeural variational inference and learning in belief networks‚Äù. In: International Conference on Machine
Learning.
Mnih, A. and D. Rezende. 2016. ‚ÄúVariational Inference for Monte Carlo
Objectives‚Äù. In: International Conference on Machine Learning.
2188‚Äì2196.
Mohamed, S. and D. J. Rezende. 2015. ‚ÄúVariational information maximisation for intrinsically motivated reinforcement learning‚Äù. In:
Advances in Neural Information Processing Systems. 2125‚Äì2133.
Molchanov, D., A. Ashukha, and D. Vetrov. 2017. ‚ÄúVariational dropout
sparsifies deep neural networks‚Äù. In: International Conference on
Machine Learning. 2498‚Äì2507.
Naesseth, C., F. Ruiz, S. Linderman, and D. Blei. 2017. ‚ÄúReparameterization gradients through acceptance-rejection sampling algorithms‚Äù.
In: Artificial Intelligence and Statistics. 489‚Äì498.
Neal, R. 2011. ‚ÄúMCMC Using Hamiltonian Dynamics‚Äù. Handbook of
Markov Chain Monte Carlo: 113‚Äì162.
Neal, R. M. and G. E. Hinton. 1998. ‚ÄúA view of the EM algorithm that
justifies incremental, sparse, and other variants‚Äù. In: Learning in
Graphical Models. Springer. 355‚Äì368.
Paisley, J., D. Blei, and M. Jordan. 2012. ‚ÄúVariational Bayesian Inference
with stochastic search‚Äù. In: International Conference on Machine
Learning. 1367‚Äì1374.
Papamakarios, G., I. Murray, and T. Pavlakou. 2017. ‚ÄúMasked autoregressive flow for density estimation‚Äù. In: Advances in Neural
Information Processing Systems. 2335‚Äì2344.
Pritzel, A., B. Uria, S. Srinivasan, A. P. Badia, O. Vinyals, D. Hassabis,
D. Wierstra, and C. Blundell. 2017. ‚ÄúNeural episodic control‚Äù. In:
International Conference on Machine Learning. 2827‚Äì2836.

Full text available at: http://dx.doi.org/10.1561/2200000056
References

83

Pu, Y., Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens, and L. Carin. 2016.
‚ÄúVariational autoencoder for deep learning of images, labels and
captions‚Äù. In: Advances in Neural Information Processing Systems.
2352‚Äì2360.
Ranganath, R., S. Gerrish, and D. Blei. 2014. ‚ÄúBlack Box Variational
Inference‚Äù. In: International Conference on Artificial Intelligence
and Statistics. 814‚Äì822.
Ranganath, R., D. Tran, and D. Blei. 2016. ‚ÄúHierarchical variational
models‚Äù. In: International Conference on Machine Learning. 324‚Äì
333.
Ravanbakhsh, S., F. Lanusse, R. Mandelbaum, J. Schneider, and B.
Poczos. 2017. ‚ÄúEnabling dark energy science with deep generative
models of galaxy images‚Äù. In: AAAI Conference on Artificial Intelligence.
Rezende, D. J., S. Mohamed, I. Danihelka, K. Gregor, and D. Wierstra.
2016a. ‚ÄúOne-shot generalization in deep generative models‚Äù. In:
International Conference on International Conference on Machine
Learning. 1521‚Äì1529.
Rezende, D. J., S. Mohamed, and D. Wierstra. 2014. ‚ÄúStochastic backpropagation and approximate inference in deep generative models‚Äù.
In: International Conference on Machine Learning. 1278‚Äì1286.
Rezende, D. J., S. A. Eslami, S. Mohamed, P. Battaglia, M. Jaderberg,
and N. Heess. 2016b. ‚ÄúUnsupervised learning of 3d structure from
images‚Äù. In: Advances In Neural Information Processing Systems.
4997‚Äì5005.
Rezende, D. and S. Mohamed. 2015. ‚ÄúVariational inference with normalizing flows‚Äù. In: International Conference on Machine Learning.
1530‚Äì1538.
Roeder, G., Y. Wu, and D. K. Duvenaud. 2017. ‚ÄúSticking the landing:
Simple, lower-variance gradient estimators for variational inference‚Äù.
In: Advances in Neural Information Processing Systems. 6925‚Äì6934.
Rosca, M., B. Lakshminarayanan, and S. Mohamed. 2018. ‚ÄúDistribution
matching in variational inference‚Äù. arXiv preprint arXiv:1802.06847.
Roweis, S. 1998. ‚ÄúEM algorithms for PCA and SPCA‚Äù. Advances in
Neural Information Processing Systems: 626‚Äì632.

Full text available at: http://dx.doi.org/10.1561/2200000056
84

References

Ruiz, F. R., M. T. R. AUEB, and D. Blei. 2016. ‚ÄúThe generalized
reparameterization gradient‚Äù. In: Advances in Neural Information
Processing Systems. 460‚Äì468.
Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1988. ‚ÄúLearning
representations by back-propagating errors‚Äù. Cognitive Modeling.
5(3): 1.
Russakovsky, O., J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z.
Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. 2015. ‚ÄúImagenet
large scale visual recognition challenge‚Äù. International Journal of
Computer Vision. 115(3): 211‚Äì252.
Salakhutdinov, R. and H. Larochelle. 2010. ‚ÄúEfficient learning of deep
Boltzmann machines‚Äù. In: International Conference on Artificial
Intelligence and Statistics. 693‚Äì700.
Salimans, T. 2016. ‚ÄúA structured variational auto-encoder for learning
deep hierarchies of sparse features‚Äù. arXiv preprint arXiv:1602.08734.
Salimans, T., D. P. Kingma, and M. Welling. 2015. ‚ÄúMarkov Chain
Monte Carlo and Variational Inference: Bridging the Gap.‚Äù In:
International Conference on Machine Learning. Vol. 37. 1218‚Äì1226.
Salimans, T. and D. A. Knowles. 2013. ‚ÄúFixed-Form variational posterior approximation through stochastic linear regression‚Äù. Bayesian
Analysis. 8(4).
Semeniuta, S., A. Severyn, and E. Barth. 2017. ‚ÄúA hybrid convolutional variational autoencoder for text generation‚Äù. arXiv preprint
arXiv:1702.02390.
Serban, I. V., A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. Courville,
and Y. Bengio. 2017. ‚ÄúA hierarchical latent variable encoder-decoder
model for generating dialogues‚Äù. In: Proceedings of the Thirty-First
AAAI Conference on Artificial Intelligence. AAAI Press. 3295‚Äì3301.
Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli. 2015.
‚ÄúDeep unsupervised learning using nonequilibrium thermodynamics‚Äù.
In: International Conference on Machine Learning. 2256‚Äì2265.
S√∏nderby, C. K., T. Raiko, L. Maal√∏e, S. K. S√∏nderby, and O. Winther.
2016a. ‚ÄúHow to train deep variational autoencoders and probabilistic
ladder networks‚Äù. In: International Conference on Machine Learning.

Full text available at: http://dx.doi.org/10.1561/2200000056
References

85

S√∏nderby, C. K., T. Raiko, L. Maal√∏e, S. K. S√∏nderby, and O. Winther.
2016b. ‚ÄúLadder variational autoencoders‚Äù. In: Advances in Neural
Information Processing Systems. 3738‚Äì3746.
Szegedy, C., W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D.
Erhan, V. Vanhoucke, and A. Rabinovich. 2015. ‚ÄúGoing deeper with
convolutions‚Äù. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 1‚Äì9.
Tomczak, J. M. and M. Welling. 2016. ‚ÄúImproving variational autoencoders using householder flow‚Äù. arXiv preprint arXiv:1611.09630.
Tomczak, J. M. and M. Welling. ‚ÄúImproving variational auto-encoders
using convex combination linear inverse autoregressive flow‚Äù. In:
Benelearn 2017: Proceedings of the Twenty-Sixth Benelux Conference
on Machine Learning, Technische Universiteit Eindhoven, 9-10 June
2017. 162.
Tran, D., M. D. Hoffman, R. A. Saurous, E. Brevdo, K. Murphy, and
D. M. Blei. 2017. ‚ÄúDeep probabilistic programming‚Äù. International
Conference on Learning Representations.
Tran, D., R. Ranganath, and D. M. Blei. 2015. ‚ÄúThe variational Gaussian
process‚Äù. arXiv preprint arXiv:1511.06499.
Van den Oord, A., N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves,
et al. 2016. ‚ÄúConditional image generation with PixelCNN decoders‚Äù.
In: Advances in neural information processing systems. 4790‚Äì4798.
Van Oord, A., N. Kalchbrenner, and K. Kavukcuoglu. 2016. ‚ÄúPixel Recurrent Neural Networks‚Äù. In: International Conference on Machine
Learning. 1747‚Äì1756.
Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol.
2010. ‚ÄúStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion‚Äù. Journal of
Machine Learning Research. 11(Dec): 3371‚Äì3408.
Watter, M., J. Springenberg, J. Boedecker, and M. Riedmiller. 2015.
‚ÄúEmbed to control: A locally linear latent dynamics model for control
from raw images‚Äù. In: Advances in Neural Information Processing
Systems. 2746‚Äì2754.
White, T. 2016. ‚ÄúSampling Generative Networks: Notes on a Few Effective Techniques‚Äù. arXiv preprint arXiv:1609.04468.

Full text available at: http://dx.doi.org/10.1561/2200000056
86

References

Williams, R. J. 1992. ‚ÄúSimple statistical gradient-following algorithms
for connectionist reinforcement learning‚Äù. Machine Learning. 8(3-4):
229‚Äì256.
Wingate, D. and T. Weber. 2013. ‚ÄúAutomated variational inference in
probabilistic programming‚Äù. arXiv preprint arXiv:1301.1299.
Xu, W., H. Sun, C. Deng, and Y. Tan. 2017. ‚ÄúVariational autoencoder
for semi-supervised text classification‚Äù. In: AAAI. 3358‚Äì3364.
Yan, X., J. Yang, K. Sohn, and H. Lee. 2016. ‚ÄúAttribute2image: Conditional image generation from visual attributes‚Äù. In: European
Conference on Computer Vision. Springer. 776‚Äì791.
Yang, Z., Z. Hu, R. Salakhutdinov, and T. Berg-Kirkpatrick. 2017.
‚ÄúImproved variational autoencoders for text modeling using dilated
convolutions‚Äù. In: International Conference on Machine Learning.
3881‚Äì3890.
Zhao, T., R. Zhao, and M. Eskenazi. 2017. ‚ÄúLearning discourse-level
diversity for neural dialog models using conditional variational autoencoders‚Äù. In: Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers).
654‚Äì664.

