Inference Suboptimality in Variational Autoencoders
Chris Cremer 1 Xuechen Li 1 David Duvenaud 1

Abstract
Amortized inference allows latent-variable models trained via variational learning to scale to large
datasets. The quality of approximate inference is
determined by two factors: a) the capacity of the
variational distribution to match the true posterior and b) the ability of the recognition network
to produce good variational parameters for each
datapoint. We examine approximate inference in
variational autoencoders in terms of these factors.
We find that divergence from the true posterior
is often due to imperfect recognition networks,
rather than the limited complexity of the approximating distribution. We show that this is due
partly to the generator learning to accommodate
the choice of approximation. Furthermore, we
show that the parameters used to increase the expressiveness of the approximation play a role in
generalizing inference rather than simply improving the complexity of the approximation.

1. Introduction
In this paper, we analyze inference suboptimality: the mismatch between the true and approximate posterior. More
specifically, we are interested in understanding what factors
cause the gap between the marginal log-likelihood and the
evidence lower bound (ELBO) in variational autoencoders
(VAEs, Kingma & Welling (2014); Rezende et al. (2014)).
We refer to this as the inference gap. Moreover, we break
down the inference gap into two components: the approximation gap and the amortization gap. The approximation
gap comes from the inability of the variational distribution
family to exactly match the true posterior. The amortization gap refers to the difference caused by amortizing the
variational parameters over the entire training set, instead of
optimizing for each training example individually. We refer
the reader to Table 1 for the definitions of the gaps and to
1

Department of Computer Science, University of Toronto,
Toronto, Canada. Correspondence to: Chris Cremer <ccremer@cs.toronto.edu>.
Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Fig. 1 for a simple illustration of the gaps. In Fig. 1, L[q]
refers to the ELBO evaluated using an amortized distribution q, as is typical of VAE training. In contrast, L[q ⇤ ] is the
ELBO evaluated using the optimal approximation within its
variational family.
There has been significant work on improving variational
inference in VAEs through the development of expressive approximate posteriors (Rezende & Mohamed, 2015; Kingma
et al., 2016; Ranganath et al., 2016; Tomczak & Welling,
2016; 2017). These works have shown that with more expressive approximate posteriors, the model learns a better
distribution over the data. Our study aims to gain a better understanding of the relationship between expressive
approximations and improved generative models.
Our experiments investigate how the choice of encoder,
posterior approximation, decoder, and optimization affect
the approximation and amortization gaps. We train VAE
models in a number of settings on the MNIST (LeCun et al.,
1998), Fashion-MNIST (Xiao et al., 2017), and CIFAR-10
(Krizhevsky & Hinton, 2009) datasets.
Our contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps,
providing insight to guide future improvements in VAE inference, b) we quantitatively demonstrate that the learned
generative model accommodates the choice of approximation, and c) we demonstrate that parameterized functions
that improve the expressiveness of the approximation play a
significant role in reducing amortization error.

log $(&)
,$$/.&1-301.4
63$
ℒ[* ∗ ]
,-./012301.4
63$
ℒ[*]
Figure 1. Gaps in Inference

Inference Suboptimality in Variational Autoencoders

Term
Inference
Approximation
Amortization

Definition
log p(x) L[q]
log p(x) L[q ⇤ ]
L[q ⇤ ] L[q]

VAE Formulation
KL (q(z|x)||p(z|x))
KL (q ⇤ (z|x)||p(z|x))
KL (q(z|x)||p(z|x)) KL (q ⇤ (z|x)||p(z|x))

Table 1. Summary of Gap Terms. The middle column refers to the general case where our variational objective is a lower bound on the
marginal log-likelihood. The right most column demonstrates the specific case in VAEs. q ⇤ (z|x) refers to the optimal approximation
within a family Q, i.e. q ⇤ (z|x) = arg minq2Q KL (q(z|x)||p(z|x)).

2. Background
2.1. Inference in Variational Autoencoders
Let x be the observed variable, z the latent variable, and
p(x, z) be their joint distribution. Given a dataset X =
{x1 , x2 , ..., xN }, we would like to maximize the marginal
log-likelihood with respect to the model parameters ✓:
Z
N
N
X
X
log p✓ (X) =
log p✓ (xi ) =
log p✓ (xi , zi )dzi .
i=1

i=1

In practice, the marginal log-likelihood is computationally
intractable due to the integration over the latent variable
z. Instead, VAEs introduce an inference network q (z|x)
to approximate the true posterior p(z|x) and optimize the
ELBO with respect to model parameters ✓ and inference
network parameters (parameterization subscripts omitted
for brevity):
 ✓
◆
p(x, z)
log p(x) = Eq(z|x) log
+ KL (q(z|x)||p(z|x))
q(z|x)
(1)
 ✓
◆
p(x, z)
Eq(z|x) log
= LVAE [q].
(2)
q(z|x)

From the above equation, we see that the ELBO is tight
when q(z|x) = p(z|x). The choice of q(z|x) is often a
factorized Gaussian distribution for its simplicity and efficiency. By utilizing the inference network (also referred to
as encoder or recognition network), VAEs amortize inference over the entire dataset. Furthermore, the overall model
is trained by stochastically optimizing the ELBO using the
reparametrization trick (Kingma & Welling, 2014).
2.2. Expressive Approximate Posteriors

There are a number of strategies for increasing the expressiveness of approximate posteriors, going beyond the original factorized-Gaussian. We briefly summarize normalizing
flows and auxiliary variables.
2.2.1. N ORMALIZING F LOWS
Normalizing flow (Rezende & Mohamed, 2015) is a change
of variables procedure for constructing complex distributions by transforming probability densities through a series of invertible mappings. Specifically, if we transform

a random variable z0 with distribution q0 (z), the resulting
random variable zT = T (z0 ) has a distribution:
qT (zT ) = q0 (z0 ) det

@zT
@z0

1

(3)

.

By successively applying these transformations, we can
build arbitrarily complex distributions. Stacking these transformations remains tractable due to the determinant being
decomposable: det(AB) = det(A)det(B). An important
property of these transformations is that we can take expectations with respect to the transformed density qT (zT )
without explicitly knowing its formula due to the law of the
unconscious statistician (LOTUS):
EqT [h(zT )] = Eq0 [h(fT (fT

1 (...f1 (z0 ))))].

(4)

Using equations (3) and (4), the lower bound with the transformed approximation can be written as:
2 0
13
6 B
Ez0 ⇠q0 (z|x) 4log @

p(x, zT )
QT
q0 (z0 |x) t=1 det @z@zt t 1

C7

1 A5 .

(5)

The main constraint on these transformations is that the
determinant of their Jacobian needs to be easily computable.
2.2.2. AUXILIARY VARIABLES
Deep generative models can be extended with auxiliary variables which leave the generative model unchanged but make
the variational distribution more expressive. Just as hierarchical Bayesian models induce dependencies between data,
hierarchical variational models can induce dependencies between latent variables. The addition of the auxiliary variable
changes the lower bound to:
 ✓
◆
p(x, z)r(v|x, z)
Ez,v⇠q(z,v|x) log
(6)
q(z, v|x)
 ✓
◆
⇣
⌘
p(x, z)
= Eq(z|x) log
KL q(v|z, x)kr(v|x, z)
q(z|x)
(7)
where r(v|x, z) is called the reverse model. From Eqn. 7,
we see that this bound is looser than the regular ELBO,

Inference Suboptimality in Variational Autoencoders

however the extra flexibility provided by the auxiliary variable can result in a higher lower bound. This idea has been
employed in works such as auxiliary deep generative models (ADGM, (Maaløe et al., 2016)), hierarchical variational
models (HVM, (Ranganath et al., 2016)) and Hamiltonian
variational inference (HVI, (Salimans et al., 2015)).

3. Methods
The inference gap G is the difference between the marginal
log-likelihood log p(x) and a lower bound L[q]. Given
the distribution in the family that maximizes the bound,
q ⇤ (z|x) = arg maxq2Q L[q], the inference gap decomposes
as the sum of approximation and amortization gaps:
L[q] = log p(x) L[q ⇤ ] + L[q ⇤ ] L[q] .
|
{z
} |
{z
}
Approximation

Amortization

For VAEs, we can translate the gaps to KL divergences by
rearranging Eqn. (1):
GVAE = KL q ⇤ (z|x)||p(z|x)
|
{z
}
Approximation

+ KL q(z|x)||p(z|x)
KL q ⇤ (z|x)||p(z|x) .
|
{z
}
Amortization

(8)

3.2. Flexible Approximate Posteriors
Our experiments involve expressive approximations which
use flow transformations and auxiliary variables. The flow
transformation that we employ is of the same type as the
transformations of Real NVP (Dinh et al., 2017). We partition the latent variable z into two, z1 and z2 , then perform
the following transformations:
z10 = z1
z20 = z2

1 (z2 ) + µ1 (z2 )
0
0
2 (z1 ) + µ2 (z1 )

6 B
Eq0 (z,v|x) 4log @
= L[qAF ].

3.1. Approximation and Amortization Gaps

G = log p(x)

replaced with z and z2 with v. We refer to this approximate
distribution as qAF , where AF stands for auxiliary flow. We
train this model by optimizing the following bound:
2 0
13

(9)
(10)

where 1 , 2 , µ1 , µ2 : Rn ! Rn are differentiable
mappings parameterized by neural nets and takes the
Hadamard or element-wise product. We partition the latent variable by simply indexing the elements of the first
half and the second half. The determinant
of the combined
⇣ 0⌘
transformation’s Jacobian, det @z
,
can
be easily eval@z
uated. See section 7.3 of the Supplementary material for a
derivation. The lower bound of this approximation is the
same as Eqn. (5). We refer to this approximation as qF low .
We also experiment with an approximation that combines
flow transformations and auxiliary variables. Let z 2 Rn
be the variable of interest and v 2 Rn the auxiliary variable.
The flow is the same as equations (9) and (10), where z1 is

p(x, zT )r(vT |x, zT )
⇣
⌘
qT (zT , vT |x) det @zt@z1t vvtt 1

C7

1 A5

(11)

Note that this lower bound is looser as explained in Section
2.2.2. We refer readers to Section 7.0.2 in the Supplementary material for specific details of the flow configuration
adopted in the experiments.
3.3. Marginal Log-Likelihood Estimation and Evidence
Lower Bounds
In this section, we describe the estimates we use to compute
the bounds of the inference gaps: log p(x), L[q ⇤ ], and L[q].
We use two bounds to estimate the marginal log-likelihood,
log p(x): IWAE (Burda et al., 2016) and AIS (Neal, 2001).
The IWAE bound takes multiple importance weighted samples from the variational q distribution resulting in a tighter
lower bound than the VAE bound. The IWAE bound is
computed as:
"
!#
k
1 X p(x, zi )
log p(x) Ez1 ...zk ⇠q(z|x) log
(12)
k i=1 q(zi |x)
= LIWAE [q].

As the number of importance samples approaches infinity,
the bound approaches the marginal log-likelihood. It is
often used as an evaluation metric for generative models
(Burda et al., 2016; Kingma et al., 2016). AIS is potentially
an even tighter lower bound. AIS weights samples from
distributions which are sequentially annealed from an initial
proposal distribution to the true posterior. See Section 7.4
in the Supplementary material for further details regarding
AIS. To compute the AIS bound, we use 100 chains, each
with 10000 intermediate distributions, where each transition
consists of one HMC trajectory with 10 leapfrog steps. The
initial distribution for AIS is the prior, so that it is encoderindependent.
We estimate the marginal log-likelihood by independently
computing our tightest lower bounds then take the maximum
of the two:
log p̂(x) = max(LAIS , LIWAE ).
The L[q ⇤ ] and L[q] bounds are the standard ELBOs, LVAE ,
from Eqn. (2), computed with either the amortized q or the
optimal q ⇤ (see below). When computing LVAE and LIWAE ,
we use 5000 samples.

Inference Suboptimality in Variational Autoencoders

A

B

C

D

Figure 2. True Posterior and Approximate Distributions of a VAE with 2D latent space. The columns represent four different datapoints.
The green distributions are the true posterior distributions, highlighting the mismatch with the blue approximations. Amortized: Variational
parameters learned over the entire dataset. Optimal: Variational parameters optimized for each individual datapoint. Flow: Using a
flexible approximate distribution.

3.4. Local Optimization of the Approximate
Distribution
To compute LVAE [q ⇤ ], we optimize the parameters of the
variational distribution for every datapoint. For the local optimization of qF F G , we initialize the mean and variance as
the prior, i.e. N (0, I). We optimize the mean and variance
using the Adam optimizer with a learning rate of 10 3 . To
determine convergence, after every 100 optimization steps,
we compute the average of the previous 100 ELBO values
and compare it to the best achieved average. If it does not
improve for 10 consecutive iterations then the optimization
is terminated. For qF low and qAF , the same process is used
to optimize all of its parameters. All neural nets for the flow
were initialized with a variant of the Xavier initilization
(Glorot & Bengio, 2010). We use 100 Monte Carlo samples
to compute the ELBO to reduce variance.
3.5. Validation of Bounds
The soundness of our empirical analysis depends on the
reliability of the marginal log-likelihood estimator. For
general importance sampling based estimators, the sample
variance of the normalized importance weights can serve as
an indicator of accuracy (Geweke, 1989; Neal, 2001). This
quantitative measure, however, can also be unreliable, e.g.
when the proposal misses an important mode of the target
distribution (Neal, 2001).

In this work, we follow (Wu et al., 2017) to empirically
validate our AIS estimates with Bidirectional Monte Carlo
(BDMC, Grosse et al. (2015; 2016)). In addition to a lower
bound provided by AIS, BDMC runs AIS chains backward
from exact posterior samples to obtain an upper bound on
the marginal log-likelihood. It should be noted that BDMC
relies on the assumption that the distribution of the simulated
data from the model roughly matches that of the real data.
This is due to the backward chain initializes from exact
posterior samples (Grosse et al., 2015).
For the MNIST and Fashion datasets, BDMC gives a gap
within 0.1 nat for a linear schedule AIS with 104 intermediate distributions and 100 importance samples on 103
simulated datapoints. For 3-BIT CIFAR, the same AIS
setting gives a gap within 1 nat with the sigmoidial annealing schedule (Grosse et al., 2015) on 100 simulated datapoints. Loosely speaking, this should give us confidence
in how well our AIS lower bounds reflect the marginal loglikelihood computed on the real data.

4. Related Work
Much of the earlier work on variational inference focused
on optimizing the variational parameters locally for each
datapoint, e.g. the original Stochastic Variational Inference
scheme (SVI, Hoffman et al. (2013)). To scale inference to
large datasets, most related works utilize inference networks
to amortize the cost of inference over the entire dataset.

Inference Suboptimality in Variational Autoencoders

log p̂(x)
⇤
LVAE [qAF
]
⇤
LVAE [qF F G ]
LVAE [q]
Approximation
Amortization
Inference

MNIST
qF F G
qAF
-89.80
-88.94
-90.80
-90.38
-91.23
-113.54
-92.57
-91.79
1.43
1.44
1.34
1.41
2.77
2.85

Fashion-MNIST
qF F G
qAF
-97.47
-97.41
-98.92
-99.10
-100.53
-132.46
-104.75
-103.76
3.06
1.69
4.22
4.66
7.28
6.35

3-BIT CIFAR
qF F G
qAF
-816.9
-820.56
-820.19
-822.16
-831.65
-861.62
-869.12
-864.28
14.75
1.60
37.47
42.12
52.22
43.72

Table 2. Inference Gaps. The columns qF F G and qAF refer to the variational distribution used for training the model. These lower bounds
are computed on the training set and are in units of nats.

Our work analyses the error that these inference networks
introduce.
Most relevant to our work is the recent work of Krishnan
et al. (2017), which explicitly remarks on two sources of
error in variational learning with inference networks, and
proposes to optimize approximate inference locally from
an initialization output by the inference network. They
show improved training on high-dimensional, sparse data
with the hybrid method, claiming that local optimization
reduces the negative effects of random initialization in the
inference network early on in training. Thus their work
focuses on reducing the amortization gap early on in training.
Similar to this idea, Hoffman (2017) proposes to perform
approximate inference during model training with MCMC
at an initialization given by a variational distribution. Our
work provides a means of explaining these improvements
in terms of the sources of inference suboptimality that they
reduce.

5. Experimental Results
5.1. Intuition through Visualization
To begin, we would like to gain an intuitive visualization of
the gaps presented in Section 3.1. To this end, we trained
a VAE with a two-dimensional latent space on MNIST and
in Fig. 2 we show contour plots of various distributions
in the latent space. The first row contains contour plots of
the true posteriors p(z|x) for four different training datapoints (columns). We have selected these four examples
to highlight different inference phenomena. The amortized
fully-factorized Gaussian (FFG) row refers to the output
of the recognition net, in this case, a FFG approximation.
Optimal FFG is the FFG that best fits the posterior of the
datapoint. Optimal Flow is the optimal fit of a flexible distribution to the same posterior, where the flexible distribution
we use is described in Section 3.2.
Posterior A is an example of a distribution where a FFG can
fit relatively well. Posterior B is an example of a posterior
with dependence between dimensions, demonstrating the

limitation of having a factorized approximation. Posterior C
highlights a shortcoming of performing amortization with a
limited-capacity recognition network, where the amortized
FFG shares little support with the true posterior. Posterior
D is a bi-modal distribution which demonstrates the ability
of the flexible approximation to fit to complex distributions,
in contrast to the simple FFG approximation. These observations raise the following question: in more typical VAEs,
is the amortization of inference the leading cause of the
distribution mismatch, or is it the limited expressiveness of
the approximation?
5.2. Amortization vs Approximation Gap
In this section, we compare how much the approximation
and amortization gaps each contribute to the total inference
gap. Table 2 are results of inference on the training set of
MNIST, Fashion-MNIST and 3-BIT CIFAR (a binarized
version of CIFAR-10, see Section 7.0.3 for details). For
each dataset, we trained models with two different approximate posterior distributions: a fully-factorized Gaussian,
qF F G , and the flexible distribution, qAF . Due to the computational cost of optimizing the local parameters for each
datapoint, our evaluation is performed on a subset of 1000
datapoints for MNIST and Fashion-MNIST and a subset of
100 datapoints for 3-BIT CIFAR.
For MNIST, we see that the amortization and approximation
gaps each account for nearly half of the inference gap. On
the more difficult Fashion-MNIST dataset, the amortization
gap is larger than the approximation gap. For CIFAR, we
see that the amortization gap is much more significant compared to the approximation gap. Thus, for the three datasets
and model architectures that we consider, the amortization
gap is likely to be the more prominent cause of inference
suboptimality, especially when the dataset becomes more
challenging to model. This indicates that improvements in
inference will likely be a result of reducing amortization
error, rather than approximation errors.
With these results in mind, would simply increasing the
capacity of the encoder improve the amortization gap? We

Inference Suboptimality in Variational Autoencoders

log p̂(x)
⇤
LVAE [qAF
]
⇤
LVAE [qF F G ]
LVAE [q]
Approximation
Amortization
Inference

MNIST
qF F G
qAF
-89.61
-88.99
-90.65
-90.44
-91.07
-108.71
-92.18
-91.19
1.46
1.45
1.11
0.75
2.56
2.20

Fashion-MNIST
qF F G
qAF
-95.99
-96.18
-97.40
-97.91
-99.64
-129.70
-102.73
-101.67
3.65
1.73
3.09
3.76
6.74
5.49

MNIST
qF F G
qAF
-89.82
-89.52
-90.96
-90.45
-90.84
-92.25
-92.33
-91.75
1.02
0.93
1.49
1.30
2.51
2.23

Fashion-MNIST
qF F G
qAF
-102.56
-102.88
-103.73
-104.02
-103.85
-105.80
-106.90
-107.01
1.29
1.14
3.05
2.29
4.34
4.13

Table 3. Left: Larger Encoder. Right: Models trained without entropy annealing. The columns qF F G and qAF refer to the variational
distribution used for training the model. The lower bounds are computed on the training set and are in units of nats.

examined this by training the MNIST and Fashion-MNIST
models from above but with larger encoders. See Section
7.0.2 for implementation details. Table 3 (left) are the results
of this experiment. Comparing to Table 2, we see that, for
both datasets and both variational distributions, using a
larger encoder results in the inference gap decreasing and
the decrease is mainly due to a reduction in the amortization
gap.
5.3. Influence of Flows on the Amortization Gap
The common reasoning for increasing the expressiveness
of the approximate posterior is to minimize the difference
between the true and approximate distributions, i.e. reduce
the approximation gap. However, given that the expressive
approximation is often accompanied by many additional
parameters, we would like to know how much influence it
has on the amortization error.
To investigate this, we trained a VAE on MNIST, discarded
the encoder, then retrained encoders with different approximate distributions on the fixed decoder. We fixed the decoder so that the true posterior is constant for all the retrained encoders. The initial encoder was a two-layer MLP
with a factorized Gaussian distribution. In order to emphasize a large amortization gap, the retrained encoders
had no hidden layers (ie. just linear transformations). For
the retraiend encoders, we tested three approximate distributions: fully factorized Gaussian (qF F G ), auxiliary flow
(qAV ), and Flow (qF low ). See Section 3.2 for the details of
these distributions.
The inference gaps of the retrained encoders on the training
set are shown in Table 4. As expected, we observe that the
small encoder with qF F G has a very large amortization gap.
However, when we use qAF or qF low as the approximate
distribution, we see the approximation gap decrease, but
more importantly, there is a significant decrease in the amortization gap. This indicates that the parameters used for
increasing the complexity of the approximation also play a
large role in diminishing the amortization error.

Variational Family
log p̂(x)
LVAE [q ⇤ ]
LVAE [q]
Approximation
Amortization
Inference

qF F G
-84.70
-86.61
-129.83
1.91
43.22
45.13

qAF
-84.70
-85.48
-98.58
0.78
13.10
13.88

qF low
-84.70
-85.13
-97.99
0.43
12.86
13.29

Table 4. Influence of expressive approximations on the amortization gap. The parameters used to increase the flexibility of the
approximate distribution also reduce the amortization gap.

These results are expected given that the parameterization of
the Flow distribution can be interpreted as an instance of the
RevNet (Gomez et al., 2017) which has demonstrated that
Real-NVP transformations (Dinh et al., 2017) can model
complex functions similar to typical MLPs. Thus the flow
transformations we employ should also be expected to increase the expressiveness while also increasing the capacity
of the encoder. The implication of this observation is that
models which improve the flexibility of their variational
approximation, and attribute their improved results to the
increased expressiveness, may have actually been due to the
reduction in amortization error.
5.4. Influence of Approximate Posterior on True
Posterior
To what extent does the posterior approximation affect the
learned model? Turner & Sahani (2011) studied the biases in
parameter learning induced by the variational approximation
when learning via variational Expectation-Maximization.
Similarly, we ask whether a factorized Gaussian approximation causes the true posterior to be more like a factorized
Gaussian? Burda et al. (2016) visually demonstrate that
when trained with an importance-weighted approximate posterior, the resulting true posterior is more complex than those
trained with factorized Gaussian approximations. Just as it
is hard to evaluate a generative model by visually inspecting
samples, it is hard to judge how “Gaussian” the true posterior is by visual inspection. We can quantitatively determine

Inference Suboptimality in Variational Autoencoders

Figure 3. Inference gaps over epochs trained on binarized Fashion-MNIST. Blue is the approximation gap. Orange is the amortization gap.
Standard is a VAE with FFG approximation. Flow is a VAE with a Flow approximation.

how close the posterior is to a fully-factorized Gaussian
(FFG) by comparing the marginal log-likelihood estimate
log p̂(x) and the Optimal FFG bound LVAE [qF⇤ F G ]. This is
equivalent to estimating the KL divergence between the optimal Gaussian and the true posterior, KL (q ⇤ (z|x)||p(z|x)).
In Table 2 on MNIST, for the FFG trained model,
KL (q ⇤ (z|x)||p(z|x)) is nearly the same for both qF⇤ F G
⇤
and qAF
. In contrast, on the model trained with qAF ,
⇤
⇤
KL (q (z|x)||p(z|x)) is much larger for qF⇤ F G than qAF
.
This suggests that the true posterior of a FFG-trained model
is closer to FFG than the true posterior of the Flow-trained
model. The same observation can be made on the FashionMNIST dataset. This implies that the decoder can learn to
have a true posterior that fits better to the approximation.
These observations justify our results of Section 5.2. which
showed that the amortization error is often the main cause
of inference suboptimality. One reason for this is that the
generator accommodates the choice of approximation, thus
reducing the approximation error.

the training set. Specifically, we trained decoders with 0, 2,
and 4 hidden layers on MNIST. See Table 5 for the results.
We see that as the capacity of the decoder increases, the approximation gap decreases. This result implies that the more
flexible the generator is, the less flexible the approximate
distribution needs to be to ensure accurate inference.
5.5. Inference Generalization
How well does amortized inference generalize at test time?
We address this question by visualizing the gaps on training
and validation datapoints across the training epochs. In
Fig. 3, the models are trained on 50000 binarized FashionMNIST datapoints and the gaps are computed on a subset
of a 100 training and validation datapoints. The top and
bottom boundaries of the blue region represent log p̂(x) and
L[q ⇤ ]. The bottom boundary of the orange region represents
L[q]. In other words, the blue region is the approximation
gap and the orange is the amortization gap.

Table 5. Increased decoder capacity reduces the approximation
gap.

In Fig. 3, the Standard model (top left) refers to a VAE of
latent size 20 trained with a factorized Gaussian approximate posterior. In this case, the encoder and decoder both
have two hidden layers each consisting of 200 hidden units.
The Flow model (top right) augments the Standard model
with a qF low variational distribution. Larger Decoder and
Larger Encoder models have factorized Gaussian distributions and increase the number of hidden layers to three and
the number of units in each layer to 500.

Given that we have seen that the generator can accommodate
the choice of approximation, our next question is whether a
generator with more capacity increases its ability to fit to the
approximation. To this end, we trained VAEs with decoders
of different sizes and measured the approximation gaps on

Firstly, we observe that for all models, the approximation
gap on the training and validation sets are roughly equivalent.
This indicates that the true posteriors of the held-out data are
similar to that of the training data. Secondly, we note that
for all models, the encoder overfits more than the decoder.

Generator Hidden Layers
log p̂(x)
LVAE [qF⇤ F G ]
Approximation Gap

0
-100.52
-104.42
3.90

2
-84.78
-86.61
1.83

4
-82.19
-83.82
1.63

Inference Suboptimality in Variational Autoencoders

These observations resonate with the encoder overfitting
findings by Wu et al. (2017).
How does increasing decoder capacity affect inference on
held-out data? We know from Section 5.4 that increasing generator capacity results in a posterior that better fits
the approximation making posterior inference easier. Furthermore, the Larger Decoder plot of Fig. 3 shows that
increasing generator capacity causes the model to be more
prone to overfitting. Thus, there is a tradeoff between ease
of inference and decoder overfitting.
5.5.1. E NCODER C APACITY AND A PPROXIMATION
E XPRESSIVENESS
We have seen in Sections 5.2 and 5.3 that expressive approximations as well as increasing encoder capacity can lead to
a reduction in the amortization gap. This leads us to the following question: when should we increase encoder capacity
versus increasing the expressiveness of the approximation?
We answer this question in terms of how well each model
can generalize its efficient inference (recognition network
and variational distribution) to held-out data.
In Fig. 3, we see that the Flow model and the Larger Encoder
model achieve similar log p̂(x) on the validation set at the
end of training. However, we see that the L[q] bound of the
Larger Encoder model is significantly lower than the L[q]
bound of the Flow model due to the encoder overfitting to
the training data. Although they both model the data nearly
equally well, the recognition net of the Larger Encoder
model is no longer suitable to perform inference on the
held-out data due to overfitting. Thus a potential rational
for utilizing expressive approximations is that they improve
generalization to held-out data in comparison to increasing
the encoder capacity.
We highlight that, in many scenarios, efficient test time inference is not required and consequently, encoder overfitting
is not an issue, since we can use non-efficient encoderindependent methods to estimate log p(x), such as AIS,
IWAE with local optimization, or potentially retraining the
encoder on the held-out data. In contrast, when efficient test
time inference is required, encoder generalization is important and expressive approximations are likely advantageous.
5.6. Annealing the Entropy
Typical warm-up (Bowman et al., 2015; Sønderby et al.,
2016) refers to annealing the KL (q(z|x)||p(z)) term during
training. This can also be interpreted as performing maximum likelihood estimation (MLE) early on during training.
This optimization technique is known to help prevent the
latent variable from degrading to the prior (Burda et al.,
2016; Sønderby et al., 2016). We employ a similar annealing scheme during training by annealing the entropy of the

approximate distribution:
Ez⇠q(z|x) [log p(x, z)

log q(z|x)] ,

where is annealed from 0 to 1 over training. This can be
interpreted as maximum a posteriori (MAP) in the initial
phase of training.
We find that warm-up techniques, such as annealing the
entropy, are important for allowing the true posterior to be
more complex. Table 3 (right) are results from a model
trained without the entropy annealing schedule. Comparing
these results to Table 2, we observe that the difference be⇤
tween LVAE [qF⇤ F G ] and LVAE [qAF
] is significantly smaller
without entropy annealing. This indicates that the true posterior is more Gaussian when entropy annealing is not used.
This suggests that, in addition to preventing the latent variable from degrading to the prior, entropy annealing allows
the true posterior to better utilize the flexibility of the expressive approximation.

6. Conclusion
In this paper, we investigated how encoder capacity, approximation choice, decoder capacity, and model optimization
influence inference suboptimality in terms of the approximation and amortization gaps. We discovered that the amortization gap can be a leading source to inference suboptimality
and that the generator can reduce the approximation gap by
learning a true posterior that fits to the choice of approximation. We showed that the parameters used to increase the
expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity
of the approximation. We confirmed that increasing the
capacity of the encoder reduces the amortization error. Additionally, we demonstrated that optimization techniques,
such as entropy annealing, help the generative model to
better utilize the flexibility of expressive variational distributions. Analyzing these gaps can be useful for guiding
improvements in VAEs. Future work includes evaluating
other types of expressive approximations, more complex
likelihood functions, and datasets.

References
Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., and Bengio, S. Generating Sentences from a
Continuous Space. ArXiv e-prints, November 2015.
Burda, Y., Grosse, R., and Salakhutdinov, R. Importance
weighted autoencoders. In ICLR, 2016.
Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast
and accurate deep network learning by exponential linear
units (elus). arXiv preprint arXiv:1511.07289, 2015.

Inference Suboptimality in Variational Autoencoders

Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using Real NVP. ICLR, 2017.
Geweke, J. Bayesian inference in econometric models using
monte carlo integration. Econometrica: Journal of the
Econometric Society, pp. 1317–1339, 1989.
Glorot, X. and Bengio, Y. Understanding the difficulty
of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics, pp. 249–256, 2010.
Gomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The
reversible residual network: Backpropagation without
storing activations. In Advances in Neural Information
Processing Systems, pp. 2211–2221, 2017.
Grosse, R., Ghahramani, Z., and Adams, R. P. Sandwiching
the marginal likelihood using bidirectional monte carlo.
arXiv preprint arXiv:1511.02543, 2015.
Grosse, R. B., Ancha, S., and Roy, D. M. Measuring the
reliability of mcmc inference with bidirectional monte
carlo. In Advances in Neural Information Processing
Systems, pp. 2451–2459, 2016.
Hoffman, M. D. Learning deep latent gaussian models with
markov chain monte carlo. In International Conference
on Machine Learning, pp. 1510–1519, 2017.
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J.
Stochastic variational inference. The Journal of Machine
Learning Research, 14(1):1303–1347, 2013.
Jarzynski, C. Nonequilibrium equality for free energy differences. Physical Review Letters, 78(14):2690, 1997.
Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
Kingma, D. and Welling, M. Auto-Encoding Variational
Bayes. In ICLR, 2014.
Kingma, D., Salimans, T., Jozefowicz, R., Chen, X.,
Sutskever, I., and Welling, M. Improving Variational
Inference with Inverse Autoregressive Flow. NIPS, 2016.
Krishnan, R. G., Liang, D., and Hoffman, M. On the challenges of learning with inference networks on sparse,
high-dimensional data. ArXiv e-prints, October 2017.
Krizhevsky, A. and Hinton, G. Learning multiple layers of
features from tiny images. University of Toronto, 2009.
Larochelle, H. and Bengio, Y. Classification using discriminative restricted boltzmann machines. In Proceedings of
the 25th international conference on Machine learning,
pp. 536–543. ACM, 2008.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 1998.
Maaløe, L., Sønderby, C., Sønderby, S., and Winther, O.
Auxiliary Deep Generative Models. ICML, 2016.
Neal, R. Annealed importance sampling. Statistics and
Computing, 2001.
Ranganath, R., Tran, D., and Blei, D. M. Hierarchical
Variational Models. ICML, 2016.
Rezende, D. and Mohamed, S. Variational Inference with
Normalizing Flows. In ICML, 2015.
Rezende, D., Mohamed, S., and Wierstra, D. Stochastic
Backpropagation and Approximate Inference in Deep
Generative Models. ICML, 2014.
Salakhutdinov, R. and Murray, I. On the quantitative analysis of deep belief networks. In Proceedings of the 25th
international conference on Machine learning, pp. 872–
879. ACM, 2008.
Salimans, T., Kingma, D., and Welling, M. Markov chain
monte carlo and variational inference: Bridging the gap.
In ICML, 2015.
Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K.,
and Winther, O. Ladder variational autoencoders. In
Advances in Neural Information Processing Systems, pp.
3738–3746, 2016.
Tomczak, J. M. and Welling, M. Improving Variational
Auto-Encoders using Householder Flow. ArXiv e-prints,
November 2016.
Tomczak, J. M. and Welling, M. Improving Variational
Auto-Encoders using convex combination linear Inverse
Autoregressive Flow. ArXiv e-prints, June 2017.
Turner, R. and Sahani, M. Two problems with variational expectation maximisation for time-series models. inference
and learning in dynamic models. Cambridge University
Press, pp. 104–123, 2011.
Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. On
the Quantitative Analysis of Decoder-Based Generative
Models. ICLR, 2017.
Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a
novel image dataset for benchmarking machine learning
algorithms. github.com/zalandoresearch/fashion-mnist,
2017.

