The Power of Factorization Mechanisms in Local and Central
Differential Privacy
Alexander Edmonds

Aleksandar Nikolov

Jonathan Ullman

University of Toronto
Toronto, Canada
edmonds@cs.toronto.edu

University of Toronto
Toronto, Canada
anikolov@cs.toronto.edu

Northeastern University
Boston, USA
jullman@ccs.neu.edu

ABSTRACT
We give new characterizations of the sample complexity of answering linear queries (statistical queries) in the local and central
models of differential privacy: (1) In the non-interactive local model,
we give the first approximate characterization of the sample complexity. Informally our bounds are tight to within polylogarithmic
factors in the number of queries and desired accuracy. Our characterization extends to agnostic learning in the local model. (2) In the
central model, we give a characterization of the sample complexity
in the high-accuracy regime that is analogous to that of Nikolov,
Talwar, and Zhang (STOC 2013), but is both quantitatively tighter
and has a dramatically simpler proof.
Our lower bounds apply equally to the empirical and population
estimation problems. In both cases, our characterizations show that
a particular factorization mechanism is approximately optimal, and
the optimal sample complexity is bounded from above and below
by well studied factorization norms of a matrix associated with the
queries.

CCS CONCEPTS
â€¢ Theory of computation â†’ Design and analysis of algorithms.

KEYWORDS

Can we characterize the amount of error required
to estimate a given workload of linear queries
subject to differential privacy in terms of natural properties of the workload, and can we achieve
this error via computationally efficient algorithms?

Differential privacy, local differential privacy, matrix factorization,
matrix mechanism, factorization mechanism, statistical queries,
PAC learning.
ACM Reference Format:
Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman. 2020. The
Power of Factorization Mechanisms in Local and Central Differential Privacy.
In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing (STOC â€™20), June 22â€“26, 2020, Chicago, IL, USA. ACM, New York,
NY, USA, 14 pages. https://doi.org/10.1145/3357713.3384297

1

[App17], Google [EPK14, BEM+ 17, WZL+ 19], Uber [JNS18], and
the US Census Bureau [DLS+ 17].
To compute statistics of the data with differential privacyâ€”or
any notion of privacyâ€”we have to inject noise into the computation
of these statistics [DN03]. The amount of noise is highly dependent
on the particular statistic, and thus a central problem in differential
privacy is to determine how much error is necessary to compute a
given statistic.
In this work we consider the class of linear queries (also called
statistical queries [Kea93]). The simplest example of a linear query
is â€œWhat fraction of individuals in the data have property ğ‘ƒ?â€ Workloads of linear queries capture a variety of statistical tasks: computing histograms and PDFs, answering range queries and computing
CDFs, estimating the mean, computing correlations and higherorder marginals, and estimating the risk of a classifier.
The power of differentially private algorithms for answering a
worst-case workload of linear queries is well understood [BUV14],
and known bounds are essentially tight as a function of the dataset
size, the data domain, and the size of the workload. However, many
workloads, such as those corresponding to computing PDFs or CDFs,
have additional structure that makes it possible to answer them
with less error than these worst-case workloads. Thus, a central
question is

In the central model, there has been dramatic progress on this
question [HT10, BDKT12, NTZ16, Nik15, BBNS19], giving approximate characterizations for every workload of linear queries. We
extend this line of work in two ways:
(1) We give the first approximate characterization for noninteractive local differential privacy [DMNS06, KLN+ 08]. This
result is also much sharper than analogous results for the
central model of differential privacy.
(2) We give a new approximate characterization for the central
model of differential privacy in the high-accuracy regime
(equivalently, in the large-dataset regime). This characterization is analogous to a result of [NTZ16], but it is quantitatively tighter and its proof is dramatically simpler. For â„“22
error, our characterization is tight up to a constant factor.

INTRODUCTION

Differential privacy [DMNS06] is a rigorous mathematical framework for protecting individual privacy that is well suited to statistical data analysis. In addition to a rich academic literature, differential privacy is now being deployed on a large scale by Apple
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6979-4/20/06. . . $15.00
https://doi.org/10.1145/3357713.3384297

In particular, our results show that a natural and well studied
type of factorization mechanism is approximately optimal in these
settings. Factorization mechanisms capture a number of specialpurpose mechanisms from the theory literature [BCD+ 07, DNPR10,

425

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman

particular, the sample complexity of this mechanism is
!
p
âˆ¥ğ‘Š âˆ¥1â†’2 log(1/ğ›¿) log ğ‘˜
ğ‘‚
.
ğœ€ğ›¼

CSS11, TUV12, CTUW14], were involved in previous characterizations, and also roughly capture the matrix mechanisms [LHR+ 10,
MMHM18] from the databases literature, which have been developed into practical algorithms for US Census Data.1
Our characterization in the local model extends to agnostic PAC
learning, and shows that the optimal learner for any family of
queries is to use the optimal factorization mechanism to estimate
the error of every concept. Our characterization is sharper than
the previous characterization of [KLN+ 08], which loses polynomial
factors in the SQ dimension [BFJ+ 94].

1.1

where âˆ¥ğ‘Š âˆ¥1â†’2 denotes the largest â„“2 norm of any column of ğ‘Š ,
which is the â„“2 -sensitivity.
One can try to improve this mechanism by replacing ğ‘Š with
a simpler workload of queries ğ´, and then attempting to reconstruct the answer to ğ‘Š by applying a linear transform ğ‘… such that
ğ‘Š = ğ‘…ğ´. One can show that the overall mechanism has error
âˆ¥ğ‘…âˆ¥2â†’âˆ âˆ¥ğ´âˆ¥1â†’2 , where âˆ¥ğ‘…âˆ¥2â†’âˆ denotes the maximum â„“2 norm
of any row of ğ‘…. This quantity can be dramatically smaller than
âˆ¥ğ‘Š âˆ¥1â†’âˆ , for example if ğ‘Š contains many copies of the same query.
The factorization mechanism chooses the optimal factorization
ğ‘Š = ğ‘…ğ´, giving error proportional to the factorization norm

Background: Linear Queries and
Factorization Mechanisms

We start by briefly introducing the relevant concepts and definitions necessary to state our results. See Section 2 for a more
thorough treatment of the necessary background.

ğ›¾ 2 (ğ‘Š ) = min{âˆ¥ğ‘…âˆ¥2â†’âˆ âˆ¥ğ´âˆ¥1â†’2 : ğ‘Š = ğ‘…ğ´}.

Linear Queries. Suppose we are given a dataset ğ‘‹ = (ğ‘¥ 1, . . . , ğ‘¥ğ‘› ) âˆˆ
Xğ‘› , where each entry ğ‘¥ğ‘– is the data of one individual and X is
some data universe. We will treat the size of the dataset ğ‘› as public
information. A linear query is specified by a bounded function ğ‘ :
Ã
X â†’ R and (abusing notation) its answer is ğ‘(ğ‘‹ ) = ğ‘›1 ğ‘›ğ‘–=1 ğ‘(ğ‘¥ğ‘– ).
A workload is a set of linear queries ğ‘„ = {ğ‘ 1, . . . , ğ‘ğ‘˜ }, and we use
ğ‘„ (ğ‘‹ ) = (ğ‘ 1 (ğ‘‹ ), . . . , ğ‘ğ‘˜ (ğ‘‹ )) to denote the answers.
Given a workload of queries, we can associate a workload matrix
ğ‘Š âˆˆ Rğ‘„Ã—X , defined by ğ‘Šğ‘,ğ‘¥ = ğ‘(ğ‘¥). The convention of calling
the above queries â€œlinearâ€ stems from the fact that they can be
written as the product of the workload matrix with the histogram
vector of the dataset. As such, we will sometimes use ğ‘„ and ğ‘Š
interchangeably.

The sample complexity of this mechanism is thus
!
p
ğ›¾ 2 (ğ‘Š ) log(1/ğ›¿) log |ğ‘„ |
â„“âˆ
sc (Mğ›¾2 , ğ‘„, ğ›¼) = ğ‘‚
.
ğœ€ğ›¼
We note that that the factorization norm ğ›¾ 2 (ğ‘Š ) and an optimal
factorization ğ‘Š = ğ‘…ğ´ can be computed in time polynomial in the
size of ğ‘Š via semidefinite programming [LS09].
Finally, we can try to further improve the mechanism using an
approximate factorization mechanism that approximates the worke that is entrywise close to ğ‘Š ,
load ğ‘Š with a simpler workload ğ‘Š
e . The error of this
and applying the factorization mechanism to ğ‘Š
mechanism is proportional to the approximate factorization norm
e ) : âˆ¥ğ‘Š âˆ’ ğ‘Š
e âˆ¥1â†’âˆ â‰¤ ğ›¼/2},
ğ›¾ 2 (ğ‘Š , ğ›¼) = min{ğ›¾ 2 (ğ‘Š

Error and Sample Complexity. Our goal is to design an (ğœ€, ğ›¿)differentially private mechanism M that takes a dataset ğ‘‹ and
accurately estimates ğ‘„ (ğ‘‹ ) for an appropriate measure of accuracy.
In this work we primarily consider accuracy in the â„“âˆ norm, and
define

e âˆ¥1â†’âˆ is the maximum absolute difference between
where âˆ¥ğ‘Š âˆ’ ğ‘Š
e . The sample complexity of this mechanism is
entries of ğ‘Š and ğ‘Š
thus
!
p
ğ›¾ 2 (ğ‘Š , ğ›¼/2) log(1/ğ›¿) log |ğ‘„ |
â„“âˆ
sc (Mğ›¾2 ,ğ›¼ , ğ‘„, ğ›¼) = ğ‘‚
.
ğœ€ğ›¼

errâ„“âˆ (M, ğ‘„, ğ‘›) = maxğ‘› E [âˆ¥M (ğ‘‹ ) âˆ’ ğ‘„ (ğ‘‹ )âˆ¥âˆ ],
ğ‘‹ âˆˆX M
â„“âˆ
errğœ€,ğ›¿ (ğ‘„, ğ‘›) =
min
errâ„“âˆ (M, ğ‘„, ğ‘›).
(ğœ€, ğ›¿)-DP M

The Local Model. Although we have discussed the factorization
mechanism in the context of central differential privacy, these ideas
can all be adapted to (non-interactive) local differential privacy. In
this model, each user will apply a separate (ğœ€, ğ›¿)-differentially private mechanism M1, . . . , Mğ‘› to their own data, and the output
can then be postprocessed using an arbitrary algorithm A, so the
mechanism can be expressed as

Privacy becomes easier to achieve as the dataset size ğ‘› grows. We
are interested in the sample complexity, which is the smallest size
of dataset on which it is possible to achieve a specified error ğ›¼ for
given privacy parameters ğœ€ and ğ›¿:
n
o
â„“âˆ
â„“âˆ
scğœ€,ğ›¿
(ğ‘„, ğ›¼) = min ğ‘› : errğœ€,ğ›¿
(ğ‘„, ğ‘›) â‰¤ ğ›¼ .

M (ğ‘‹ ) = A (M1 (ğ‘‹ 1 ), . . . , Mğ‘› (ğ‘‹ğ‘› ))
The Approximate Factorization Mechanisms. One of the most
basic tools in the central-model of differential privacy is the Gaussian mechanism (see e.g. [DR14]). This mechanism computes the
vector of answers to the queries ğ‘„ (ğ‘‹ ) and perturbs it with spherical Gaussian noise scaled to the â„“2 -sensitivity of the workload. In

â„“âˆ ,loc
â„“âˆ ,loc
We define errğœ€,ğ›¿
, and scğœ€,ğ›¿
analogously to the central model,

but with the minimum taken over mechanisms that are (ğœ€, ğ›¿)-DP
in the local model.
Since the queries are linear, we can simply have each user apply
the approximate factorization mechanism to their own data and average the results. One can show that randomizing each individualâ€™s
data independently increases the variance of the noise by a factor of
âˆš
ğ‘› compared to the central model version of the mechanism. One
can also achieve (ğœ€, 0)-differential privacy by replacing Gaussian

1 In a nutshell, the matrix mechanism is a particular factorization mechanism designed

for the special case of â„“22 error, and combined with various optimizations and postprocessing techniques to improve computational efficiency and utility. Usually the
matrix mechanism is presented in the special case of pure differential privacy.

426

The Power of Factorization Mechanisms in Local and Central Differential Privacy

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

(2) Parity queries, which capture the covariance and higherorder moments of the data.
(3) Marginal queries, also known as conjunctions, which capture
the marginal distribution on subsets of the attributes.

noise with a different subgaussian noise distribution. Putting it
together, the resulting sample complexity becomes


ğ›¾ 2 (ğ‘Š , ğ›¼/2) 2 log |ğ‘„ |
scâ„“âˆ (Mğ›¾loc
,
ğ‘„,
ğ›¼)
=
ğ‘‚
.
(1)
2 ,ğ›¼
ğœ€ 2ğ›¼ 2

cdf

1.2.1 Linear Queries in the Local Model. Our main result in the
local model shows that the approximate factorization mechanism
described above is approximately optimal among all non-interactive
locally differentially private mechanisms.

Corollary 4 (Thresholds / CDFs). Let ğ‘„ğ‘‡ be the family of
statistical queries over the domain X = [ğ‘‡ ] that, for every 1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ ,
contains the statistical query ğ‘ğ‘¡ (ğ‘¥) = I{ğ‘¥ â‰¤ ğ‘¡ }. Then for every ğ‘‡ âˆˆ N
and ğœ€, ğ›¼ smaller than an absolute constant,


â„“âˆ ,loc cdf
scğœ€,0
(ğ‘„ğ‘‡ , ğ›¼) = Î© log2 ğ‘‡ .

Theorem 1 (Informal). Let ğ›¼, ğœ€, ğ›¿ > 0 be smaller than some
absolute constants and let ğ‘„ be a workload of linear queries with
workload matrix ğ‘Š . Then, for some ğ›¼ â€² = Î©(ğ›¼/log(1/ğ›¼)),


ğ›¾ 2 (ğ‘Š , ğ›¼/2) 2
â„“âˆ ,loc
scğœ€,0
(ğ‘„, ğ›¼ â€² ) = Î©
.
ğœ€ 2ğ›¼ 2

We obtain this corollary by combining Theorem 1 with results
from [FSSS03]. Corollary 4 should be compared to the upper bound
of ğ‘‚ (log3 ğ‘‡ ) that can be obtained from the local analogue of the
binary tree mechanism [DNPR10, CSS11]. Ours is the first lower
bound to go beyond the easy Î©(logğ‘‡ ) lower bound for this problem,
which follows easily via a so-called packing argument.

1.2

Our Results

parity

To interpret the theorem, it helps to start by imagining that
ğ›¾ 2 (ğ‘Š , ğ›¼ â€² /2) = ğ›¾ 2 (ğ‘Š , ğ›¼/2), in which case the theorem would show
that the sample complexity of answering queries up to error ğ›¼ â€² is


ğ›¾ 2 (ğ‘Š , ğ›¼ â€² /2) 2
Î©
,
ğœ€ 2ğ›¼ 2

Corollary 5 (Parities). Let ğ‘„ğ‘‘,ğ‘¤

be the family of statistical

queries over the domain X = {Â±1}ğ‘‘ that, for every ğ‘† âŠ† [ğ‘‘], |ğ‘† | â‰¤ ğ‘¤,
Ã
contains the statistical query ğ‘ğ‘† (ğ‘¥) = ğ‘— âˆˆğ‘† ğ‘¥ ğ‘— . Then for every ğ‘˜ â‰¤
ğ‘‘ âˆˆ N and ğœ€, ğ›¼ smaller than an absolute constant,
parity

â„“âˆ ,loc
scğœ€,0
(ğ‘„ğ‘‘,ğ‘¤ , ğ›¼) = Î©((ğ‘‘/ğ‘¤) ğ‘¤ ).

which differs from the sample complexity of the local approximate factorization mechanism, given in (1), by a factor of just
ğ‘‚ (log(1/ğ›¼ â€² ) 2 log |ğ‘„ |). The fact that we take ğ›¼ â€² < ğ›¼ means that
ğ›¾ 2 (ğ‘Š , ğ›¼/2) can be much smaller than ğ›¾ 2 (ğ‘Š , ğ›¼ â€² /2).2 Nevertheless,
for many natural families of queries and choices of ğ›¼, ğ›¾ 2 (ğ‘Š , ğ›¼/2)
will be relatively stable to small changes in ğ›¼, in which case our
lower bound will be tight up to this ğ‘‚ (log(1/ğ›¼) 2 log |ğ‘„ |) factor. In
contrast, existing characterizations for the central model [HT10,
BDKT12, NTZ16, Nik15, BBNS19] lose a poly(1/ğ›¼) factor, or else
they lose a polylog|X| factor that is typically large.

Corollary 5 says that adding independent Gaussian noise to
each query is optimal up to a ğ‘‚ (ğ‘¤ log(ğ‘‘/ğ‘¤)) factor. Using similar
techniques, one can also obtain a direct proof that gives a tight
lower bound up to constant factors, even for the simpler problem
of finding the subset ğ‘† of size at most ğ‘¤ that maximizes ğ‘ğ‘† (ğ‘‹ ).
marginal

Corollary 6 (Marginals). Let ğ‘„ğ‘‘,ğ‘¤

be the family of statisti-

cal queries over the domain X = {0, 1}ğ‘‘ that, for every ğ‘† âŠ† [ğ‘‘], |ğ‘† | â‰¤
Ã
ğ‘¤, contains the statistical query ğ‘ğ‘† (ğ‘¥) = ğ‘— âˆˆğ‘† ğ‘¥ ğ‘— . Then for every
ğ‘˜ â‰¤ ğ‘‘ âˆˆ N and ğœ€, ğ›¼ smaller than an absolute constant,

Remark 2. Our proof of Theorem 1, in fact, shows that the lower
bound holds in the distributional setting where ğ‘‹ is sampled i.i.d. from
an unknown distribution ğœ‡, and the goal is to estimate the quantity
ğ‘(ğœ‡) = E [ğ‘(ğ‘¥)] for every query ğ‘ âˆˆ ğ‘„ up to error at most ğ›¼.

marginal

â„“âˆ ,loc
scğœ€,0
(ğ‘„ğ‘‘,ğ‘¤

âˆš

, ğ›¼) = (ğ‘‘/ğ‘¤) Î© ( ğ‘¤) .

Marginal queries have been extremely well studied in differential privacy [BCD+ 07, KRSU10, GHRU11, HRS12, TUV12, CTUW14,
DNT15]. Corollary 6 shows that a natural local analogue of the algorithm of [TUV12] is optimal for answering marginal queries up
to the hidden constant factor in the exponent.

ğ‘¥âˆ¼ğœ‡

Remark 3. Theorem 1 crucially assumes that the error is bounded
in the â„“âˆ metric. If we consider the less stringent â„“22 error metric
(appropriately scaled to reflect the error per query), then one can
achieve sample complexity ğ‘‚ (log |X|/ğœ€ 2 ğ›¼ 4 ) for any workload of
queries [BBNS19], which can be exponentially smaller than the lower
bound we prove for â„“âˆ error. In many applications, such as releasing the PDF, CDF, or marginals of the data, the â„“âˆ error metric is
standard in the literature on these problems, and is more practical,
since, for natural datasets, the weaker â„“22 guarantee can be achieved
by mechanisms that ignore the data.

1.2.2 Agnostic Learning in the Local Model. Theorem 1 extends to
characterizing agnostic PAC learning [KSS94] in the local model. In
agnostic PAC learning, the dataset consists of labeled examples ğ‘‹ =
((ğ‘¥ 1, ğ‘¦1 ), . . . , (ğ‘¥ğ‘› , ğ‘¦ğ‘› )), where ğ‘¥ğ‘– âˆˆ X, and ğ‘¦ğ‘– âˆˆ {Â±1}, and each pair
(ğ‘¥ğ‘– , ğ‘¦ğ‘– ) is sampled independently from an unknown distribution ğœ‡.
The goal is to find a concept ğ‘ : X â†’ {Â±1} in a concept class C
that approximately maximizes E (ğ‘¥,ğ‘¦)âˆ¼ğœ‡ [ğ‘ (ğ‘¥)ğ‘¦].
The correlation of each concept ğ‘ with the labels in the data is a
linear query, and one natural approach to agnostic PAC learning
is to estimate all these linear queries, and output the concept that
corresponds to the largest query value. Thus, we can apply the local
approximate factorization mechanism to the family of queries C
to obtain the same sample complexity upper bound in (1). Interestingly, the proof of our lower bound in Theorem 1 shows that the
same lower bound also applies to this a priori easier problem of

Using Theorem 1, we obtain new lower bounds for three well studied families of queries:
(1) Threshold queries, which are also known as range queries,
and equivalent to computing the CDF of the data.
2 For example, if every entry of ğ‘Š is at most ğ›¼ in absolute value, then ğ›¾ (ğ‘Š , ğ›¼) = 0
2
whereas ğ›¾ 2 (ğ‘Š , ğ›¼ â€² ) can be arbitrarily large for ğ›¼ â€² < ğ›¼ , but this behavior typically
does not happen for â€œnon-trivialâ€ values of ğ›¼ .

427

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman

agnostic PAC learning, showing that the local approximate factorization mechanism gives an approximately optimal way to learn
any concept class C.
Prior results of Kasisiviswanathan et al. [KLN+ 08] connecting
learning algorithms in the local model with the SQ model, together
with characterizations of sample complexity in the SQ model [BFJ+ 94,
SzÃ¶09], give upper and lower bounds on sample complexity of
learning in the local model in terms of SQ dimension. These results, however, are only tight up to polynomial factors in the SQ
dimensionâ€”which can be polynomial in |C|â€”whereas our results
are sharper. We remark that, technically, the results are not comparable, since the the characterization via the SQ model holds for
sequentially interactive, rather than non-interactive, mechanisms.
1.2.3 Linear Queries in the Central Model. Our second set of results
quantitatively strengthensâ€”and simplifies the proof ofâ€”the central
model characterization of [NTZ16]. In contrast to the local model,
the sample complexity of answering many natural workloads of
linear queries exhibits two distinct regimes, depending on the desired accuracy. For example, for a worst-case workload of linear
queries, the sample complexity is at most
)
(
log1/2 |X| log |ğ‘„ | |ğ‘„ | 1/2
.
min
,
ğœ€ğ›¼
ğœ€ğ›¼ 2
Thus, the sample complexity behaves very differently when ğ›¼ goes
below some critical value. Our results concern this high-accuracy
regime where ğ›¼ is quite small. In these results, we consider the â„“22
error (scaled to be directly comparable to the â„“âˆ error), which is
h
i 1/2
2
errâ„“2 (M, ğ‘„, ğ‘›) = maxğ‘› E |ğ‘„1 | âˆ¥M (ğ‘‹ ) âˆ’ ğ‘„ (ğ‘‹ )âˆ¥22

In addition to being sharper, our proof of Theorem 7 is dramatically simpler than the lower bounds in [NTZ16, NT15].
Remark 8. By a trivial reduction, Theorem 1, in fact, gives lower
bounds for the distributional setting where ğ‘‹ is sampled i.i.d. from
an unknown distribution ğœ‡, and the goal is to estimate the quantity
ğ‘(ğœ‡) = E [ğ‘(ğ‘¥)] for every query ğ‘ âˆˆ ğ‘„ up to error at most ğ›¼.
ğ‘¥âˆ¼ğœ‡

Data-Independent Mechanisms. Along the way, we prove a simple result that this sample complexity bound holds for every choice
of ğ›¼, provided we restrict attention to data-independent mechanisms.
These mechanisms can be written in the form M (ğ‘‹ ) = ğ‘„ (ğ‘‹ ) +ğ‘ /ğ‘›
for some fixed random variable ğ‘ that depends only on ğ‘„ and not
on the data.
For such mechanisms we show that the sample complexity is
always Î©(ğ›¾ ğ¹ (ğ‘Š )/ğœ€ğ›¼), regardless of ğ›¼.3
Data-independent mechanisms are interesting on their own,
since the fact that we add noise from a known distribution makes
them simpler to implement, and also means that we can give precise
confidence intervals on the error of the mechanism. One application of our lower bound for data-independent mechanisms is an
Î©(logğ‘‡ ) lower bound on the sample complexity of any mechanism
for answering threshold queries over [ğ‘‡ ] in â„“22 error, which matches
the data-independent binary tree mechanism.

1.3

Techniques

Below we give a brief overview of the techniques used to prove
Theorems 1 and 7.

ğ‘‹ âˆˆX M

with the related quantities defined analogously. Notice that we have
2
scaled the â„“22 error so that errâ„“2 (M, ğ‘„, ğ‘›) â‰¤ errâ„“âˆ (M, ğ‘„, ğ‘›). For â„“22
error, the natural factorization norm that describes the error of the
factorization mechanisms is
o
n
ğ›¾ ğ¹ (ğ‘Š ) = |ğ‘„1| 1/2 âˆ¥ğ‘…âˆ¥ğ¹ âˆ¥ğ´âˆ¥1â†’2 : ğ‘Š = ğ‘…ğ´ ,
qÃ
2
where âˆ¥ğ‘…âˆ¥ğ¹ =
ğ‘–,ğ‘— ğ‘…ğ‘–,ğ‘— is the Frobenius norm of ğ‘….
In this high-accuracy regime, a combination of [NTZ16] and [NT15]
(see also the thesis [Nik14]) shows that, for every workload of linear
queries, there is some ğ›¼ âˆ— such that
ğ›¾ ğ¹ (ğ‘Š )
â„“22
â‰¤ scğœ€,ğ›¿
(ğ‘„, ğ›¼)
ğœ€ğ›¼
ğ›¾ ğ¹ (ğ‘Š )
â‰¤ ğ‘‚ (1) Â·
Â· log(1/ğ›¿).
ğœ€ğ›¼
Note that the upper and lower bound differ by a factor of ğ‘‚ (log |ğ‘„ | Â·
log(1/ğ›¿)). The upper bound above is precisely what is given by
the factorization mechanism. Our next theorem closes the gap
between the upper and lower bounds in terms of |ğ‘„ |, and thus gives
a characterization up to ğ‘‚ (log(1/ğ›¿)) for â„“22 .
âˆ€ğ›¼ â‰¤ ğ›¼ âˆ— Î©(logâˆ’1 |ğ‘„ |) Â·

Theorem 7. Let ğœ€, ğ›¿ > 0 be smaller than some absolute constants
and let ğ‘„ be a workload of linear queries with workload matrix ğ‘Š .
There exists some ğ›¼ âˆ— > 0 such that for every ğ›¼ â‰¤ ğ›¼ âˆ— ,


ğ›¾ ğ¹ (ğ‘Š )
â„“22
scğœ€,ğ›¿
(ğ‘„, ğ›¼) = Î©
.
ğœ€ğ›¼

Lower bound in the local model. As mentioned above, Theorem 1 is proved in the distributional setting, where the dataset ğ‘‹
consists of ğ‘› i.i.d. samples from some distribution ğœ‡, and the goal
is to estimate the expectation of each query ğ‘ âˆˆ ğ‘„ on ğœ‡. Our approach is to design two families of hard distributions {ğœ†1, . . . , ğœ†ğ‘˜ }
and {ğœ‡ 1, . . . , ğœ‡ğ‘˜ } with the following properties: first, any locally
differentially private mechanism requires many samples to distinguish these two families; second, the two families give very different
answers to the queries.
To show that the distributions are hard to distinguish, we prove
an upper bound on the KL-divergence between: (1) the transcript
of a private mechanism in the local model when run on ğ‘› samples
from a random distribution in {ğœ†1, . . . , ğœ†ğ‘˜ }, and (2) the same, but
for a random distribution in {ğœ‡ 1, . . . , ğœ‡ğ‘˜ }. Intuitively, the bound
shows that the KL-divergence between transcripts is small when
no bounded test function can simultaneously distinguish between
ğœ†ğ‘£ and ğœ‡ ğ‘£ on average over a random choice of ğ‘£ âˆˆ [ğ‘˜]. This bound
is a slight extension of a similar bound from [DJW18]. In particular,
the upper bound on the KL-divergence is in terms of the âˆ â†’ 2
operator norm of a matrix ğ‘€ derived from the two families of
distributions.
Thus, what remains is to find families distributions {ğœ†1, . . . , ğœ†ğ‘˜ }
and {ğœ‡1, . . . , ğœ‡ğ‘˜ }, for which the âˆ â†’ 2 operator norm of ğ‘€ is small,
but the expectations of the queries in ğ‘„ are sufficiently different
on the two families. Recall that our goal is to prove a lower bound
3 Technically, we require ğ›¼

âˆ¥ğ‘Š âˆ¥1â†’âˆ = 1.

428

â‰¤ âˆ¥ğ‘Š âˆ¥1â†’âˆ , but in nearly all applications of interest

The Power of Factorization Mechanisms in Local and Central Differential Privacy

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

in terms of the approximate norm ğ›¾ 2 (ğ‘Š , ğ›¼), where ğ‘Š is the workload matrix. Since ğ›¾ 2 (ğ‘Š , ğ›¼) is the value of a convex minimization
problem, it admits a dual characterization, showing that ğ›¾ 2 (ğ‘Š , ğ›¼) is
equal to the value of a maximization problem over matrices ğ‘ˆ . We
take an optimal dual solution ğ‘ˆ , and use it to derive distributions
{ğœ†1, . . . , ğœ†ğ‘˜ } and {ğœ‡ 1, . . . , ğœ‡ğ‘˜ }. The objective function of the dual
problem guarantees that these distributions are such that the expectation of any query ğ‘ âˆˆ ğ‘„ on any ğœ†ğ‘£ is small, yet the expectation of
the query ğ‘ ğ‘£ on ğœ‡ ğ‘£ is large. Moreover, the dual objective, together
with classical arguments in functional analysis, also guarantees an
upper bound on the âˆ â†’ 2 norm of the appropriate matrix ğ‘€,
giving us both ingredients for our lower bound.

largest entries of ğ‘€, âˆ¥ğ‘€ âˆ¥1â†’2 , which corresponds to the maximum
â„“2 -norm of a column of ğ‘€, and âˆ¥ğ‘€ âˆ¥2â†’âˆ , which corresponds to the
maximum â„“2 -norm of a row of ğ‘€.
â€²
The inner product of two matrices ğ‘€ and ğ‘ in R SÃ—S is defined
Ã
âŠ¤
by ğ‘€ â€¢ ğ‘ = Tr(ğ‘€ ğ‘ ) = ğ‘¢ âˆˆS,ğ‘£ âˆˆSâ€² ğ‘šğ‘¢,ğ‘£ ğ‘›ğ‘¢,ğ‘£ . The Frobenius norm
âˆš
â€²
of ğ‘€ âˆˆ R SÃ—S is given by âˆ¥ğ‘€ âˆ¥ğ¹ = ğ‘€ â€¢ ğ‘€.
Lastly, the factorization norms ğ›¾ ğ¹ and ğ›¾ 2 central to this work are
â€²
given for ğ‘€ âˆˆ R SÃ—S by
o
n
ğ›¾ ğ¹ (ğ‘€) = min |S1| 1/2 âˆ¥ğ‘…âˆ¥ğ¹ âˆ¥ğ´âˆ¥1â†’2 : ğ‘…ğ´ = ğ‘€ ,

Lower bound in the central model. The main ingredient of the
proof of Theorem 7 is a lower bound of Î©(ğ›¾ ğ¹ (ğ‘Š )/ğœ€ğ›¼) on the sample
complexity of data-independent mechanisms. Recall that a mechanism M is data-independent if M (ğ‘‹ ) = ğ‘„ (ğ‘‹ ) + ğ‘›1 ğ‘ for a random
variable ğ‘ âˆˆ Rğ‘„ . Our key observation is that, if Î£ is the covariance
matrix of ğ‘ , then the mechanism
ğ‘‚ (log(1/ğ›¿))
ğ‘„ (ğ‘‹ ) +
Â· N (0, Î£)
ğ‘›
that uses Gaussian noise in place of ğ‘ is also (ğœ€, ğ›¿)-differentially
private. Moreover, the â„“22 error of M is equal to Tr(Î£)/|ğ‘„ | 1/2 , so,
up to a factor of ğ‘‚ (log(1/ğ›¿)), the optimal data-independent mechanism with respect to â„“22 -error can be assumed to use correlated
Gaussian noise. It is easy to see that the class of all such mechanism is equivalent to the class of all factorization mechanisms, and,
hence, the optimal achievable â„“22 -error is ğ‘‚ (ğ›¾ ğ¹ (ğ‘Š )/ğœ€ğ‘›).
To give a lower bound for arbitrary mechanisms in the highaccuracy regime, we use a clever transformation from [BDKT12]
that turns a data-dependent mechanisms that is accurate for large
datasets into a data-independent mechanism.

2.2

ğ›¾ 2 (ğ‘€) = min{âˆ¥ğ‘…âˆ¥2â†’âˆ âˆ¥ğ´âˆ¥1â†’2 : ğ‘…ğ´ = ğ‘€ }.

Differential Privacy

Let X denote the data universe. A generic element from X will be
denoted by ğ‘¥. We consider datasets of the form ğ‘‹ = (ğ‘¥ 1, . . . , ğ‘¥ğ‘› ) âˆˆ
Xğ‘› , each of which is identified with its histogram â„ âˆˆ Z X
â‰¥0 where,
for every ğ‘¥ âˆˆ X, â„ğ‘¥ = |{ğ‘– : ğ‘¥ğ‘– = ğ‘¥ }|, so that âˆ¥â„âˆ¥1 = ğ‘›. To refer
to a dataset, we use ğ‘‹ and â„ interchangeably. A pair of datasets
ğ‘‹ = (ğ‘¥ 1, . . . , ğ‘¥ğ‘– , . . . , ğ‘¥ğ‘› ) and ğ‘‹ â€² = (ğ‘¥ 1, . . . , ğ‘¥ğ‘–â€², . . . , ğ‘¥ğ‘› ) are called
adjacent if ğ‘‹ â€² is obtained from ğ‘‹ by replacing an element ğ‘¥ğ‘– of ğ‘‹
with a new universe element ğ‘¥ğ‘–â€² .
For parameters ğœ€, ğ›¿ > 0, an (ğœ€, ğ›¿)-differentially private mechanism [DMNS06] (or (ğœ€, ğ›¿) âˆ’ ğ·ğ‘ƒ for short) is a randomized function
M : Xğ‘› â†’ Î© which, for all adjacent datasets ğ‘‹ and ğ‘‹ â€² , for all
outcomes ğ‘† âŠ† Î©, satisfies
Pr [M (ğ‘‹ ) âˆˆ ğ‘†] â‰¤ ğ‘’ ğœ€ Pr [M (ğ‘‹ â€² ) âˆˆ ğ‘†] + ğ›¿.
M

M

For a set S, the â„“1 , â„“2 and â„“âˆ norms on R S are given respectively

A mechanism which is (ğœ€, 0)-differentially private will be referred
to as being simply ğœ€-differentially private (or ğœ€-DP for short).
Of special interest are (ğœ€, ğ›¿)-differentially private mechanisms
Mğ‘– : X â†’ Î©Ì„ which take a singleton dataset ğ‘‹ = {ğ‘¥ } as input.
These are referred to as local randomizers. A sequence of (ğœ€, ğ›¿)differentially private local randomizers M1, . . . , Mğ‘› together with
a post-processing function A : Î©Ì„ğ‘› â†’ Î© specify a (non-interactive)
locally (ğœ€, ğ›¿)-differentially private mechanism M : Xğ‘› â†’ Î© [EGS03,
DMNS06, KLN+ 08]. In short, we say that such mechanisms are
(ğœ€, ğ›¿)-LDP, or ğœ€-LDP when ğ›¿ = 0. When the local mechanism M is
applied to a dataset ğ‘‹ , we refer to

sÃ•

TM (ğ‘‹ ) = (M1 (ğ‘¥ 1 ), . . . , Mğ‘› (ğ‘¥ğ‘› ))

2

PRELIMINARIES

In this section we recount basic notation and definitions used
throughout the paper.

2.1

Norms

by
âˆ¥ğ‘âˆ¥1 =

Ã•

|ğ‘ ğ‘£ |,

âˆ¥ğ‘âˆ¥2 =

ğ‘£ âˆˆS

(ğ‘ ğ‘£ ) 2,

âˆ¥ğ‘âˆ¥âˆ = max |ğ‘ ğ‘£ |.
ğ‘£ âˆˆS

ğ‘£ âˆˆS

as the transcript of the mechanism. Then the output of the mechanism is given by M (ğ‘‹ ) = A (TM (ğ‘‹ )).

Given a probability distribution ğœ‹ on S, we consider the norms
âˆ¥ Â· âˆ¥ğ¿1 (ğœ‹ ) and âˆ¥ Â· âˆ¥ğ¿2 (ğœ‹ ) on R S , given by
sÃ•
Ã•
âˆ¥ğ‘âˆ¥ğ¿1 (ğœ‹ ) =
ğœ‹ (ğ‘£)|ğ‘ ğ‘£ |, âˆ¥ğ‘âˆ¥ğ¿2 (ğœ‹ ) =
ğœ‹ (ğ‘£)(ğ‘ ğ‘£ ) 2 .

2.3

ğ‘£ âˆˆS

ğ‘£ âˆˆS

We also take advantage of a number of matrix norms. For norms
â€²
âˆ¥ Â· âˆ¥ğœ and âˆ¥ Â· âˆ¥ğœ‰ on R S and R S respectively, we consider the matrix
â€²
operator norm of ğ‘€ âˆˆ R SÃ—S given by
âˆ¥ğ‘€ âˆ¥ğœ â†’ğœ‰ =

ğ‘¥âˆ¼ğœ‡

workload is a set of linear queries ğ‘„ = {ğ‘ 1, . . . , ğ‘ğ‘˜ }, and ğ‘„ (ğ‘‹ ) =
(ğ‘ 1 (ğ‘‹ ), . . . , ğ‘ğ‘˜ (ğ‘‹ )) is used to denote their answers. The answers
on a distribution ğœ‡ on X are denoted by ğ‘„ (ğœ‡) = (ğ‘ 1 (ğœ‡), . . . , ğ‘ğ‘˜ (ğœ‡)).
We will often represent ğ‘„ by its workload matrix ğ‘Š âˆˆ Rğ‘„Ã—X with
entries ğ‘¤ğ‘,ğ‘¥ = ğ‘(ğ‘¥). In this notation, the answers to the queries are
given by ğ‘›1 ğ‘Š â„. We will often use ğ‘„ and ğ‘Š interchangeably.

âˆ¥ğ‘€ğ‘¥ âˆ¥ğœ‰
max
ğ‘¥ âˆˆRS \{0}

âˆ¥ğ‘¥ âˆ¥ğœ

Linear Queries

A linear query is specified by a bounded function ğ‘ : X â†’ R.
Abusing notation slightly, its answer on a dataset ğ‘‹ is given by
Ã
ğ‘(ğ‘‹ ) = ğ‘›1 ğ‘›ğ‘–=1 ğ‘(ğ‘¥ğ‘– ). We also extend this notation to distributions:
if ğœ‡ is a distribution on X, then we write ğ‘(ğœ‡) for E [ğ‘(ğ‘¥)]. A

.

For the special case of âˆ¥ğ‘€ âˆ¥â„“ğ‘  â†’â„“ğ‘¡ , we will simply write âˆ¥ğ‘€ âˆ¥ğ‘ â†’ğ‘¡ .
Of particular importance are âˆ¥ğ‘€ âˆ¥1â†’âˆ which corresponds to the

429

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

2.4

Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman

Error and Sample Complexity

Analogously, we can show that

The â„“âˆ and â„“22 -error of a mechanism M, which takes a dataset
of size ğ‘›, on the query workload ğ‘„ are given by

!
p
|ğ‘„ | âˆ’1/2 âˆ¥ğ‘…âˆ¥ğ¹ âˆ¥ğ´âˆ¥1â†’2 log(1/ğ›¿)
err (Mğ‘…,ğ´ , ğ‘„, ğ‘›) = ğ‘‚
.
ğœ€ğ‘›
â„“22

errâ„“âˆ (M, ğ‘„, ğ‘›) = maxğ‘› E [âˆ¥M (ğ‘‹ ) âˆ’ ğ‘„ (ğ‘‹ )âˆ¥âˆ ],
ğ‘‹ âˆˆX M

â„“22

err

h

(M, ğ‘„, ğ‘›) = maxğ‘› E |ğ‘„1 | âˆ¥M (ğ‘‹ ) âˆ’ ğ‘„ (ğ‘‹ )âˆ¥22
ğ‘‹ âˆˆX M

Optimizing this error bound over the choice of ğ‘… and ğ´ gives error
proportional to the factorization norm

i 1/2
.

ğ›¾ ğ¹ (ğ‘Š ) = min{|ğ‘„ | âˆ’1/2 âˆ¥ğ‘…âˆ¥ğ¹ âˆ¥ğ´âˆ¥1â†’2 : ğ‘Š = ğ‘…ğ´},

We can then define the sample complexity of a mechanism M for a
given â„“âˆ error ğ›¼ by

and the mechanism Mğ›¾ğ¹ that runs Mğ‘…,ğ´ with the ğ‘… and ğ´ achieving ğ›¾ ğ¹ (ğ‘Š ) has sample complexity
!
p
ğ›¾ ğ¹ (ğ‘Š ) log(1/ğ›¿)
â„“22
scğœ€,ğ›¿ (ğ‘„, ğ›¼) = ğ‘‚
.
ğ›¼

â„“âˆ
scğœ€,ğ›¿
(M, ğ‘„, ğ›¼) = min{ğ‘› : errâ„“âˆ (M, ğ‘„, ğ‘›) â‰¤ ğ›¼ }.
â„“2

2
The sample complexity with respect to â„“22 error scğœ€,ğ›¿
(ğ‘„, ğ›¼) is defined
analogously.
Having defined error and sample complexity for a fixed mechanism, we can define the optimal error and sample complexity
by

â„“âˆ
errğœ€,ğ›¿
(ğ‘„, ğ‘›) =

min

This factorization mechanism is equivalent to the Gaussian noise
matrix mechanism in [LHR+ 10].

3

errâ„“âˆ (M, ğ‘„, ğ‘›),

M is (ğœ€, ğ›¿)-DP

NON-INTERACTIVE LOCAL DP: LINEAR
QUERIES

2
2
The analogous quantities errğœ€,ğ›¿
(ğ‘„, ğ‘›) and scğœ€,ğ›¿
(ğ‘„, ğ›¼) for â„“22 -error
are defined similarly. The optimal error and sample complexity for
â„“âˆ ,loc
â„“âˆ ,loc
the local model are denoted errğœ€,ğ›¿
(ğ‘„, ğ‘›) and scğœ€,ğ›¿
(ğ‘„, ğ›¼), and
are defined in the same way but with the minimum taken over
(ğœ€, ğ›¿)-LDP mechanisms.

In this section we give details about our results for answering
linear queries in the local model. We first present the local approximate factorization mechanism. Then we give an information
theoretic lemma that bounds the KL-divergence between the transcripts of mechanisms in the local model on inputs drawn from
mixtures of product distributions. We then use a dual formulation
of the approximate ğ›¾ 2 norm to construct distributions to use with
the information theoretic lemma in order to prove the lower bound
in Theorem 1.

2.5

3.1

â„“âˆ
scğœ€,ğ›¿
(ğ‘„, ğ›¼) =

min

scâ„“âˆ (M, ğ‘„, ğ‘›).

M is (ğœ€, ğ›¿)-DP
â„“2

â„“2

Factorization Mechanisms

The Gaussian mechanism [DN03, DN04, DMNS06] is defined as
!


ğœğœ€,ğ›¿ âˆ¥ğ‘Š âˆ¥1â†’2 2
1
Â·ğ¼ ,
MGauss (ğ‘Š , â„) = ğ‘Š â„ + ğ‘, ğ‘ âˆ¼ N 0,
ğ‘›
ğ‘›
p
where ğœğœ€,ğ›¿ = ğ‘‚ ( log(1/ğ›¿)/ğœ€) depends only on the privacy parameters. Given a factorization ğ‘Š = ğ‘…ğ´, we consider the mechanism

e ) : âˆ¥ğ‘Š âˆ’ ğ‘Š
e âˆ¥1â†’âˆ â‰¤ ğ›¼/2},
ğ›¾ 2 (ğ‘Š , ğ›¼) = min{ğ›¾ 2 (ğ‘Š
e ) = min{âˆ¥ğ‘…âˆ¥2â†’âˆ âˆ¥ğ´âˆ¥1â†’2 : ğ‘Š = ğ‘…ğ´}. Matrices ğ‘Š
e , ğ‘…,
where ğ›¾ 2 (ğ‘Š
and ğ´ achieving the minimum to any degree of accuracy can be
computed in polynomial time via semidefinite programming, as
shown in [LS09]. Our main positive result shows that the sample
complexity of the corresponding approximate factorization mechanism is bounded above by the approximate ğ›¾ 2 norm. As sketched
in the introduction, this can be achieved via a local version of the
Gaussian noise mechanism, which can then be transformed into a
purely private mechanism using the results of [BNS18]. This gives,
however, a slightly suboptimal bound, and, instead, we use the local
randomizer from [BBNS19], which is a variant of a local randomizer
from [DJW18]. The relevant properties of this local randomizer are
captured by the next lemma. We recall that a random variable ğ‘
over R is ğœ-subgaussian if E exp(ğ‘ 2 /ğœ 2 ) â‰¤ 2, and a random variable ğ‘ over Rğ‘‘ is ğœ-subgaussian if ğœƒ âŠ¤ğ‘ is ğœ-subgaussian for every
vector ğœƒ such that âˆ¥ğœƒ âˆ¥2 = 1.

Mğ‘…,ğ´ (â„) = ğ‘… MGauss (ğ‘Š , â„)
1
= ğ‘Š â„ + ğ‘,
ğ‘›


ğ‘ âˆ¼ N 0,

Approximate Factorization

Here we give details of the approximate factorization mechanism,
which was sketched in the introduction. Recall that the approximate
ğ›¾ 2 norm is defined by

!

ğœğœ€,ğ›¿ âˆ¥ğ´âˆ¥1â†’2 2
Â· ğ‘…ğ‘… âŠ¤ ,
ğ‘›

and, utilizing Gaussian tail bounds, one can show that the error is
!
p
âˆ¥ğ‘…âˆ¥2â†’âˆ âˆ¥ğ´âˆ¥1â†’2 log(1/ğ›¿) log |ğ‘„ |
â„“âˆ
err (Mğ‘…,ğ´ , ğ‘„, ğ‘›) = ğ‘‚
.
ğœ€ğ‘›
We define the factorization mechanism Mğ›¾2 to be the mechanism
that chooses ğ‘…, ğ´ to minimize this expression, and its error is proportional to the factorization norm
ğ›¾ 2 (ğ‘Š ) = min{âˆ¥ğ‘…âˆ¥2â†’âˆ âˆ¥ğ´âˆ¥1â†’2 : ğ‘Š = ğ‘…ğ´}.
The sample complexity of this mechanism is thus
!
p
ğ›¾ 2 (ğ‘Š ) log(1/ğ›¿) log |ğ‘„ |
â„“âˆ
sc (Mğ›¾2 , ğ‘„, ğ›¼) = ğ‘‚
.
ğ›¼

Lemma 9 ([BBNS19]). There exists an ğœ€-DP mechanism M which
takes as input a single datapoint ğ‘¥ âˆˆ Rğ‘‘ such that âˆ¥ğ‘¥ âˆ¥2 â‰¤ 1, and
outputs a random ğ‘Œğ‘¥ := M (ğ‘¥) âˆˆ Rğ‘‘ such that
(1) ğ‘Œğ‘¥ can be sampled in time polynomial in ğ‘‘ on input ğ‘¥,
(2) E[ğ‘Œğ‘¥ ] = ğ‘¥,

This mechanism is implicit in [NTZ16], and is stated in this form
in [Nik14].

430

The Power of Factorization Mechanisms in Local and Central Differential Privacy

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

(3) ğ‘Œğ‘¥ âˆ’ ğ‘¥ is ğœ-subgaussian with ğœ = ğ‘‚ (ğœ€ âˆ’1 ).

Hence, our goal will be to define our distributions so that that
âˆ¥ğ‘€ âˆ¥â„“2 â†’ğ¿ (ğœ‹ ) is small while still meeting the requirement that
âˆ
2
estimating the queries ğ‘„ allows us to distinguish between ğœ†ğ‘›ğœ‹ and
ğœ‡ğ‘›ğœ‹ .
It is worth noting that Lemma 11 is not known to hold when
the protocol is allowed to be sequentially interactive. Indeed, this
is the bottleneck to generalizing our lower bound to the case of
sequentially interactive local privacy. See the proof of Lemma 11
for further discussion.

Based on this local randomizer, and the approximate factorizations, we prove the following upper bound in Appendix A.
Theorem 10 (Approximate Factorization Mechanism). There
exists an ğœ€-LDP mechanism Mğ›¾loc
such that, for any ğ‘˜ statistical
2 ,ğ›¼
queries ğ‘„ with workload matrix ğ‘Š , we have


ğ›¾ 2 (ğ‘Š , ğ›¼/2) 2 log ğ‘˜
scâ„“âˆ (Mğ›¾loc
,
ğ‘„,
ğ›¼)
=
ğ‘‚
,
2 ,ğ›¼
ğœ€ 2ğ›¼ 2

3.3

and the mechanism runs in time polynomial in ğ‘›, ğ‘˜, and |X|.

3.2

Bounding KL-Divergence

Our lower bound will rely on the construction, based on a workload ğ‘„, of families {ğœ†1, . . . , ğœ†ğ‘˜ } and {ğœ‡ 1, . . . , ğœ‡ğ‘˜ } of distributions on
X. Together with these, we consider a distribution ğœ‹ over [ğ‘˜]. For
any ğ‘£ âˆˆ [ğ‘˜], let ğœ†ğ‘›ğ‘£ be the product distribution induced by sampling ğ‘› times independently from ğœ†ğ‘£ , and let ğœ†ğ‘›ğœ‹ be the mixture
Ãğ‘˜
ğ‘›
ğ‘›
ğ‘›
ğ‘›
ğ‘›
ğ‘£=1 ğœ‹ (ğ‘£)ğœ†ğ‘£ . Define ğœ‡ ğ‘£ and ğœ‡ğœ‹ analogously. Note that ğœ†ğœ‹ and ğœ‡ğœ‹
are not product distributions, but mixtures of such distributions.
For a mechanism M in the local model, and a probability distribution ğœˆ on Xğ‘› , we use TM (ğœˆ) to denote the distribution on random
transcripts TM (ğ‘‹ ) when ğ‘‹ is sampled from ğœˆ. Similarly, if ğœˆ is a
distribution on X, we use the notation Mğ‘– (ğœˆ) for the distribution
of Mğ‘– (ğ‘¥), when ğ‘¥ is sampled from ğœˆ.
We approach the task of showing that ğœ†1, . . . , ğœ†ğ‘˜ and ğœ‡ 1, . . . , ğœ‡ğ‘˜
are â€œhardâ€ distributions on which to evaluate ğ‘„ in two steps. On
the one hand, we wish to argue that being able to estimate ğ‘„ on
the distributions ğœ†1, . . . , ğœ†ğ‘˜ and ğœ‡1, . . . , ğœ‡ğ‘˜ enables us to distinguish
between ğœ†ğ‘›ğœ‹ and ğœ‡ğ‘›ğœ‹ . On the other hand, we show a lower bound on
the number of samples required for a locally private mechanism to
distinguish between ğœ†ğ‘›ğœ‹ and ğœ‡ğ‘›ğœ‹ . The second of these objectives will
be met by way of the following bound on KL-divergence. Similar
bounds were proved in [DJW18, DR18] when only one of the two
distributions is a mixture of products, and our proof is similar to
the proof of Theorem 2 in [DR18]. Our proof is in Appendix B.

Lemma 12. For any ğ‘˜ Ã— ğ‘‡ matrix ğ‘Š and ğ›¼,


ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼ âˆ¥ğ‘ˆ âˆ¥1
ğ‘˜Ã—ğ‘‡
ğ›¾ 2 (ğ‘Š , ğ›¼) = max
: ğ‘ˆ âˆˆR
, ğ‘ˆ â‰ 0 ,
ğ›¾ 2âˆ— (ğ‘ˆ )
where ğ›¾ 2âˆ— is the dual norm to ğ›¾ 2 given by
ğ›¾ 2âˆ— (ğ‘ˆ ) = max{ğ‘ˆ â€¢ ğ‘‰ : ğ‘‰ âˆˆ Rğ‘˜Ã—ğ‘‡ , ğ›¾ 2 (ğ‘‰ ) â‰¤ 1}

where ğ‘ 1, . . . , ğ‘ğ‘˜ and ğ‘ 1, . . . , ğ‘ğ‘‡ range over vectors with unit â„“2 norm
in Rğ‘˜+ğ‘‡ .
The expression
ğ›¾ 2âˆ— (ğ‘ˆ ) = max

DKL (TM (ğœ†ğ‘›ğœ‹ ) âˆ¥TM (ğœ‡ğ‘›ğœ‹ ))

ğ‘˜ Ã•
ğ‘‡
Ã•

ğ‘¢ğ‘–,ğ‘— ğ‘ğ‘–âŠ¤ğ‘ ğ‘— ,

ğ‘–=1 ğ‘—=1

"
max

ğ‘˜ Ã•
ğ‘‡
Ã•

ğ‘¢ğ‘–,ğ‘— ğ‘ğ‘–âŠ¤ğ‘ ğ‘— ,
ğ‘ 1 ,...,ğ‘ğ‘˜
ğ‘ 1 ,...,ğ‘ğ‘‡ ğ‘–=1 ğ‘—=1

= max

Lemma 11. Let ğœ€ âˆˆ (0, 1], and let M be an ğœ€-DP mechanism in
the local model. Then, for families {ğœ†1, . . . , ğœ†ğ‘˜ } and {ğœ‡ 1, . . . , ğœ‡ğ‘˜ } of
distributions on X, together with a distribution ğœ‹ over [ğ‘˜],

â‰¤ ğ‘‚ (ğ‘›ğœ€ 2 ) Â·

Duality for ğ›¾ 2 (ğ‘Š , ğ›¼) and the Dual Norm

Recall that our goal is to prove a lower bound on the sample
complexity of mechanisms in the local model in terms of the approximate ğ›¾ 2 norm. We will do so via Lemma 11, and the distributions
{ğœ†1, . . . , ğœ†ğ‘˜ } and {ğœ‡ 1, . . . , ğœ‡ğ‘˜ } will serve as a certificate of a lower
bound on the sample complexity. On the other hand, convex duality
can certify a lower bound on the approximate ğ›¾ 2 norm. In the proof
of our lower bounds, we will show that these dual certificates for
which the approximate ğ›¾ 2 norm is large can be turned into hard
families of distributions to use in Lemma 11.
The key duality statement follows. This dual formulation for
the ğ›¾ 2 (ğ‘Š , ğ›¼) was also given in [LS09] for the special case when ğ‘Š
has entries in {âˆ’1, +1}.4 For completeness, here we rederive it in
Appendix C by directly applying the hyperplane separator theorem.

E [ğ‘“ğ‘¥ ] âˆ’ E [ğ‘“ğ‘¥ ]

E

ğ‘“ âˆˆRX : âˆ¥ğ‘“ âˆ¥âˆ â‰¤1 ğ‘‰ âˆ¼ğœ‹

with the max over unit vectors ğ‘ 1, . . . ğ‘ğ‘˜ and ğ‘ 1, . . . , ğ‘ğ‘‡ can be easily
formulated as a semidefinite program, and, in fact, is exactly the
semidefinite program that appears in Grothendieckâ€™s inequality
(see, e.g., [KN12, Pis12]). It is straightforward to check (just take
all the ğ‘ğ‘– and ğ‘ ğ‘— co-linear) that

2#

ğ‘¥âˆ¼ğœ†ğ‘‰

.

ğ‘¥âˆ¼ğœ‡ğ‘‰

In matrix notation, define the matrix ğ‘€ âˆˆ R [ğ¾ ]Ã—X by ğ‘š ğ‘£,ğ‘¥ =
(ğœ†ğ‘£ (ğ‘¥) âˆ’ ğœ‡ ğ‘£ (ğ‘¥)). Then

ğ›¾ 2âˆ— (ğ‘ˆ ) â‰¥ max{ğ‘¦ âŠ¤ğ‘ˆ ğ‘§ : ğ‘¦ âˆˆ {âˆ’1, 1}ğ‘š , ğ‘§ âˆˆ {âˆ’1, 1}ğ‘ } = âˆ¥ğ‘ˆ âˆ¥âˆâ†’1 .
(2)
Moreover, Grothendieck showed that this inequality is always tight
up to a universal constant [Gro53], although this fact will not be
used here. Instead, we will need the following lemma, which can
be derived from SDP duality, and is also due to Grothendieck. For a
proof using the Hahn-Banach theorem, see [Pis12].

DKL (TM (ğœ†ğ‘›ğœ‹ )âˆ¥TM (ğœ‡ğ‘›ğœ‹ )) â‰¤ ğ‘‚ (ğ‘›ğœ€ 2 ) Â· âˆ¥ğ‘€ âˆ¥â„“2 â†’ğ¿ (ğœ‹ ) .
âˆ

2

Being able to distinguish between TM (ğœ†ğ‘›ğœ‹ ) and TM (ğœ‡ğ‘›ğœ‹ ) with
constant probability implies, by Pinskerâ€™s inequality, that
DKL (TM (ğœ†ğ‘›ğœ‹ )âˆ¥TM (ğœ‡ğ‘›ğœ‹ )) â‰¥ Î©(1).
Together with Lemma 11, this would imply
!
1
ğ‘›=Î©
.
ğœ€ 2 Â· âˆ¥ğ‘€ âˆ¥â„“2 â†’ğ¿ (ğœ‹ )
âˆ

4 Note that in [LS09], Linial and Shraibman use the notation ğ›¾ ğ›¼ (ğ‘Š ) = inf {ğ›¾ (ğ‘Š
2 f) :
2
1â‰¤ğ‘¤
eğ‘– ğ‘— ğ‘¤ğ‘– ğ‘— â‰¤ ğ›¼ âˆ€ğ‘–, ğ‘— }. For sign matrices ğ‘Š this is equal to ğ›¼2+1 ğ›¾ 2 (ğ‘Š , (ğ›¼ âˆ’ 1)/(ğ›¼ +
1)) in our notation.

2

431

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman

Lemma 13 ([Gro53]). For any ğ‘˜ Ã— ğ‘‡ matrix ğ‘ˆ , ğ›¾ 2âˆ— (ğ‘ˆ ) â‰¤ ğ‘¡ if and
only if there exist diagonal matrices ğ‘ƒ âˆˆ Rğ‘˜Ã—ğ‘˜ and ğ‘„ âˆˆ Rğ‘‡ Ã—ğ‘‡ , and
a matrix ğ‘ˆe âˆˆ Rğ‘˜Ã—ğ‘‡ such that Tr(ğ‘ƒ 2 ) = Tr(ğ‘„ 2 ) = 1, ğ‘ˆ = ğ‘ƒ ğ‘ˆeğ‘„, and
âˆ¥ğ‘ˆe âˆ¥2â†’2 â‰¤ ğ‘¡.

Lemma 18. Let ğ›¼ > 0 and letğ‘Š âˆˆ Rğ‘„Ã—X be a symmetric workload
matrix with X + and ğ‘Š + as given by Definition 16. Then it holds that
ğ›¾ 2 (ğ‘Š ) = ğ›¾ 2 (ğ‘Š + ) and ğ›¾ 2 (ğ‘Š , ğ›¼) = ğ›¾ 2 (ğ‘Š +, ğ›¼). Moreover, if, for some
+
ğ‘ˆ + âˆˆ Rğ‘„Ã—X ,

By (2), the ğ›¾ 2âˆ— (Â·) norm is an upper bound on the âˆ¥ Â· âˆ¥âˆâ†’1 norm.
We use Lemma 13 to show a similar upper bound on the âˆ¥ Â· âˆ¥âˆâ†’2 ,
which allows projecting out some of the rows of the matrix, but is
quantitatively stronger. The reason we are interested in the âˆ¥ Â· âˆ¥âˆâ†’2
norm is that this is the norm that appears in the statement of
Lemma 11.

ğ›¾ 2 (ğ‘Š +, ğ›¼) =
then

ğ›¾ 2 (ğ‘Š , ğ›¼) =

q

ğ‘˜ âˆ¥Î  ğ‘ˆ âˆ¥
âˆ—
âˆâ†’2 â‰¤ ğ›¾ 2 (ğ‘ˆ ), where Î ğ‘† is the
ğ‘†
2
projection onto the subspace Rğ‘† .

3.5

Lower Bound Based on Dual Solutions

In this section we put together the different tools we have already
set up â€“ the KL-divergence lower bound, and the duality of the
approximate ğ›¾ 2 norm â€“ in order to prove our main lower bound
result Theorem 1.
For this section, it is convenient to consider the enumeration
ğ‘ 1, . . . , ğ‘ğ‘˜ of the queries of a symmetric workload ğ‘„ with workload
matrix ğ‘Š âˆˆ R [ğ‘˜ ]Ã—X . Let ğ‘ˆ be the dual witness to the lower bound
on ğ›¾ 2 (ğ‘Š , ğ›¼), as given by Lemma 12, so that

The next lemma slightly strengthens Lemma 14 to allow for
weights on the rows of the matrix. This is the key fact about the ğ›¾ 2âˆ—
norm that we need for our lower bounds.
Lemma 15. Let ğ‘ˆ and ğ‘€ be ğ‘˜Ã—ğ‘‡ matrices, and let ğœ‹ be a probability
distribution on [ğ‘˜] where, for any ğ‘– âˆˆ [ğ‘˜], ğ‘— âˆˆ [ğ‘‡ ], we have ğ‘¢ğ‘–,ğ‘— =
ğœ‹ (ğ‘–)ğ‘šğ‘–,ğ‘— . Then there exists a probability distribution ğœ‹b on [ğ‘˜], with
support contained in the support of ğœ‹, such that âˆ¥ğ‘€ âˆ¥â„“âˆ â†’ğ¿2 ( ğœ‹b) â‰¤
4ğ›¾ 2âˆ— (ğ‘ˆ ).

ğ›¾ 2 (ğ‘Š , ğ›¼) =

Lemmas 14 and 15 are proved in Appendix C.

3.4

ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼ âˆ¥ğ‘ˆ âˆ¥1
,
ğ›¾ 2âˆ— (ğ‘ˆ )

where ğ‘ˆ = 12 (ğ‘ˆ +, ğ‘ˆ âˆ’ ) is a matrix in Rğ‘„Ã—X such that the submatrix
âˆ’
+ for all ğ‘¥ âˆˆ X +
ğ‘ˆ âˆ’ is indexed by X âˆ’ and has entries ğ‘¢ğ‘,âˆ’ğ‘¥
= âˆ’ğ‘¢ğ‘,ğ‘¥
and ğ‘ âˆˆ ğ‘„.

Lemma 14. For any matrix ğ‘ˆ âˆˆ Rğ‘˜Ã—ğ‘‡ , there exists a set ğ‘† âŠ† [ğ‘˜]
of size |ğ‘† | â‰¥ ğ‘˜2 such that

ğ‘Š + â€¢ ğ‘ˆ + âˆ’ ğ›¼ âˆ¥ğ‘ˆ + âˆ¥1
,
ğ›¾ 2âˆ— (ğ‘ˆ + )

ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼ âˆ¥ğ‘ˆ âˆ¥1
.
ğ›¾ 2âˆ— (ğ‘ˆ )

(3)

By Lemma 18, we may assume without loss of generality that ğ‘ˆ is
of the form (ğ‘ˆ +, ğ‘ˆ âˆ’ ) where each entry of ğ‘ˆ âˆ’ is the additive inverse
of the corresponding entry of ğ‘ˆ + . Furthermore, by dividing each
entry of ğ‘ˆ by âˆ¥ğ‘ˆ âˆ¥1 if necessary, then we may assume without loss
of generality that âˆ¥ğ‘ˆ âˆ¥1 = 1. In this case,

Symmetrization

For our lower bound, it will be convenient to narrow our attention to the following restricted class of â€˜symmetricâ€™ query workloads.
Definition 16. Let ğ‘„ be a workload of statistical queries with
workload matrix ğ‘Š âˆˆ Rğ‘„Ã—X . Suppose there exists a partition of X
into sets X + and X âˆ’ , |X + | = |X âˆ’ |, where each element ğ‘¥ of X + is
identified with a distinct element of X âˆ’ , denoted âˆ’ğ‘¥, such that, for
all ğ‘ âˆˆ ğ‘„, for all ğ‘¥ âˆˆ X, ğ‘(âˆ’ğ‘¥) = âˆ’ğ‘(ğ‘¥). In other words, ğ‘Š can be
+
âˆ’
expressed as (ğ‘Š +,ğ‘Š âˆ’ ), where ğ‘Š + âˆˆ Rğ‘„Ã—X and ğ‘Š âˆ’ âˆˆ Rğ‘„Ã—X are
the restrictions of ğ‘Š to ğ‘„ Ã— X + and ğ‘„ Ã— X âˆ’ respectively, with each
+ of ğ‘Š + and the corresponding entry ğ‘¤ âˆ’
âˆ’
entry ğ‘¤ğ‘,ğ‘¥
ğ‘,âˆ’ğ‘¥ of ğ‘Š satisfying
âˆ’ = âˆ’ğ‘¤ + . Also write ğ‘„ + to denote the collection of queries with
ğ‘¤ğ‘,ğ‘¥
ğ‘,âˆ’ğ‘¥
workload matrix ğ‘Š + so that the queries ğ‘ + : X + â†’ R of ğ‘„ + are
obtained by restricting queries ğ‘ : X â†’ R of ğ‘„ to the input space X + ;
define ğ‘„ âˆ’ analogously. Then ğ‘„, and also ğ‘Š , are called symmetric.

ğ›¾ 2 (ğ‘Š , ğ›¼) =

ğ‘Š â€¢ğ‘ˆ âˆ’ğ›¼
.
ğ›¾ 2âˆ— (ğ‘ˆ )

Let us make a first attempt at constructing our collection of â€œhardâ€
distributions ğœ†1, . . . , ğœ†ğ‘˜ and ğœ‡ 1, . . . , ğœ‡ğ‘˜ for ğ‘„. Since âˆ¥ğ‘ˆ âˆ¥1 = 1, then
Ã•
ğœ‹ (ğ‘£) =
|ğ‘¢ ğ‘£,ğ‘¥ |
(4)
ğ‘¥ âˆˆX

defines a valid probability distribution over [ğ‘˜]. For each ğ‘£ âˆˆ [ğ‘˜],
we then define a pair of distributions ğœ†ğ‘£ and ğœ‡ ğ‘£ given by
âˆ€ğ‘¥ âˆˆ X + : ğœ†ğ‘£ (ğ‘¥) = ğœ†ğ‘£ (âˆ’ğ‘¥) = |ğ‘¢ ğ‘£,ğ‘¥ |/ğœ‹ (ğ‘£)
(
2|ğ‘¢ ğ‘£,ğ‘¥ |/ğœ‹ (ğ‘£) if ğ‘¢ ğ‘£,ğ‘¥ â‰¥ 0
+
âˆ€ğ‘¥ âˆˆ X : ğœ‡ ğ‘£ (ğ‘¥) =
0
if ğ‘¢ ğ‘£,ğ‘¥ < 0
(
0
if ğ‘¢ ğ‘£,ğ‘¥ â‰¥ 0
ğœ‡ ğ‘£ (âˆ’ğ‘¥) =
2|ğ‘¢ ğ‘£,ğ‘¥ |/ğœ‹ (ğ‘£) if ğ‘¢ ğ‘£,ğ‘¥ < 0

The following result will allow us to translate our lower bound
for the symmetric query workloads into a lower bound for general
query workloads. Its proof is given in Appendix D.
Lemma 17. Let ğ›¼, ğœ– > 0. Let ğ‘„ be a symmetric workload of statistical queries and take ğ‘„ + as given by Definition 16. Suppose there
exists a non-interactive locally ğœ€-LDP mechanism M + which takes
ğ‘› samples as input and achieves errâ„“âˆ (M +, ğ‘„ +, ğ‘›) â‰¤ ğ›¼. Then there
exists a local 3ğœ€-LDP mechanism M which takes ğ‘› â€² = max{ğ‘›, ğœ€ 21ğ›¼ 2 }
samples as input and achieves errâ„“âˆ (M, ğ‘„, ğ‘› â€² ) â‰¤ 4ğ›¼.

(5)
(6)

(7)

Then, for all ğ‘–, ğ‘£ âˆˆ [ğ‘˜], the symmetry of ğœ†ğ‘£ implies ğ‘ğ‘– (ğœ†ğ‘£ ) = 0.
By contrast, it holds for all ğ‘£ âˆˆ [ğ‘˜] that
Ã•
ğ‘ ğ‘£ (ğœ‡ ğ‘£ ) =
ğ‘ ğ‘£ (ğ‘¥)ğœ‡ ğ‘£ (ğ‘¥)
ğ‘¥ âˆˆX

=

Ã•
ğ‘¥ âˆˆX +
+

Lemma 18 allows us to relate ğ›¾ 2 (ğ‘Š ) and ğ›¾ 2 (ğ‘Š + ) and their wit-

ğ‘ ğ‘£ (ğ‘¥)(ğœ‡ ğ‘£ (ğ‘¥) âˆ’ ğœ‡ ğ‘£ (âˆ’ğ‘¥))

= 2ğ‘Š â€¢ ğ‘ˆ + = ğ‘Š â€¢ ğ‘ˆ .

nesses. Its proof is also given in Appendix D.

432

The Power of Factorization Mechanisms in Local and Central Differential Privacy

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

e âˆˆ R [ğ‘˜ ]Ã—X with entries ğ‘š
Consider now the matrix ğ‘€
eğ‘£,ğ‘¥ =
e
e
e of Lemma 19
ğœ†ğ‘£ (ğ‘¥) âˆ’ e
ğœ‡ ğ‘£ (ğ‘¥). Since ğ‘€ is obtained from the matrix ğ‘ˆ
e by 1 , it follows that
by scaling each row ğ‘£ of ğ‘ˆ
2ğœ‹ (ğ‘£)

Hence,

E


max ğ‘ğ‘– (ğœ‡ğ‘‰ ) â‰¥ E [ğ‘ğ‘‰ (ğœ‡ğ‘‰ )] = ğ‘Š â€¢ ğ‘ˆ .

ğ‘‰ âˆ¼ğœ‹ ğ‘– âˆˆ [ğ‘˜ ]

ğ‘‰ âˆ¼ğœ‹

Since ğ‘Š â€¢ ğ‘ˆ = ğ›¾ 2âˆ— (ğ‘ˆ )ğ›¾ 2 (ğ‘Š , ğ›¼) + ğ›¼ â‰¥ ğ›¼ by Lemma 12, then

e âˆ¥â„“ â†’ğ¿ ( ğœ‹e) =
âˆ¥ğ‘€
âˆ
1

Eğ‘‰ âˆ¼ğœ‹ [ max ğ‘ğ‘– (ğœ‡ğ‘‰ )] â‰¥ ğ›¼ .
ğ‘– âˆˆ [ğ‘˜ ]

This is not quite the quantity

If we could guarantee that ğ‘ğ‘‰ (ğœ‡ğ‘‰ ) was close to its expectation when
ğ‘‰ âˆ¼ ğœ‹, then estimating each of the queries ğ‘ğ‘– of ğ‘„ with error less
than ğ›¼ would allow us to distinguish the distributions ğœ†1, . . . , ğœ†ğ‘˜
from the distributions ğœ‡ 1, . . . , ğœ‡ğ‘˜ . The following result modifies our
distributions in a way that resolves this issue.

eâˆ¥2
âˆ¥ğ‘€
â„“ â†’ğ¿ ( ğœ‹
e) =
âˆ

âˆ

E

ğ‘“ âˆˆRX : âˆ¥ğ‘“ âˆ¥âˆ â‰¤1 ğ‘‰ âˆ¼ğœ‹

Eğ‘¥âˆ¼ğœ†e [ğ‘“ğ‘¥ ] âˆ’ Eğ‘¥âˆ¼e
ğœ‡ğ‘‰ [ğ‘“ğ‘¥ ]

2

ğ‘‰

1

eğ‘‰
ğ‘“ âˆˆRX : âˆ¥ğ‘“ âˆ¥âˆ â‰¤1 ğ‘‰ âˆ¼ğœ‹ ğ‘¥âˆ¼ğœ†

ğ‘¥âˆ¼e
ğœ‡ğ‘‰

Since the trivial case of Holderâ€™s inequality implies that the ğ¿1 (e
ğœ‹ )norm is always bounded above by the ğ¿2 (e
ğœ‹ )-norm, it holds that
e âˆ¥â„“ â†’ğ¿ ( ğœ‹e) â‰¤ âˆ¥ ğ‘€
e âˆ¥â„“ â†’ğ¿ ( ğœ‹e) . However, this inequality goes in
âˆ¥ğ‘€
âˆ
1
âˆ
2
the wrong direction for our requirements. This issue is remedied
by taking advantage of Lemma 15.

ğ‘Š â€¢ğ‘ˆ âˆ’ğ›¼/4

(2) for all ğ‘£ in the support of ğœ‹e, ğ‘ ğ‘£ (e
ğœ‡ ğ‘£ ) â‰¥ ğ‘‚ (log(1/ğ›¼)) ;
[ğ‘„
]Ã—X
e âˆˆ R
(3) the matrix ğ‘ˆ
with entries ğ‘¢eğ‘£,ğ‘¥ = ğœ‹e(ğ‘£)(ğœ†eğ‘£ (ğ‘¥) âˆ’
âˆ—
e
e
ğœ‡ ğ‘£ (ğ‘¥)) satisfies ğ›¾ (ğ‘ˆ ) â‰¤ ğ›¾ âˆ— (ğ‘ˆ ).

Lemma 21. Let ğ‘„ be a collection of symmetric queries with workload matrix ğ‘Š âˆˆ R [ğ‘˜ ]Ã—X . Let ğ‘ˆ âˆˆ R [ğ‘˜ ]Ã—X be the dual witness so that
(3) is satisfied. Then there exist probability distributions ğœ†e1, . . . , ğœ†eğ‘˜
and e
ğœ‡1, . . . , e
ğœ‡ğ‘˜ over X, and a distribution ğœ‹b over [ğ‘˜] such that:
e
(1) ğœ†1, . . . , ğœ†eğ‘˜ , e
ğœ‡1, . . . , e
ğœ‡ğ‘˜ and ğœ‹b satisfy criteria 1. and 2. of Lemma 19;
e with entries ğ‘š
(2) the matrix ğ‘€
eğ‘£,ğ‘¥ = ğœ†eğ‘£ (ğ‘¥) âˆ’ e
ğœ‡ ğ‘£ (ğ‘¥) satisfies

2

The proof of Lemma 19 will take advantage of the following
exponential binning lemma. A proof is given in Appendix E.
Lemma 20. Suppose that ğ‘ 1, . . . , ğ‘ğ‘˜ âˆˆ [0, 1] and that ğœ‹ is a probability distribution over [ğ‘˜]. Then for any ğ›½ âˆˆ (0, 1], there exists a set
Ãğ‘˜

2


max

which Lemma 11 would have us bound. For comparison, note
"
#
e
âˆ¥ ğ‘€ âˆ¥â„“ â†’ğ¿ ( ğœ‹e) =
max
E
E [ğ‘“ğ‘¥ ]] âˆ’ E [ğ‘“ğ‘¥ ] .

Lemma 19. Let ğ‘„ be a collection of symmetric queries with workload matrix ğ‘Š âˆˆ R [ğ‘˜ ]Ã—X . Let ğ‘ˆ âˆˆ R [ğ‘˜ ]Ã—X be the dual witness so that
(3) is satisfied. Then there exist probability distributions ğœ†e1, . . . , ğœ†eğ‘˜
and e
ğœ‡1, . . . , e
ğœ‡ğ‘˜ over X, and a distribution ğœ‹e over [ğ‘˜] such that:
e
(1) ğ‘ğ‘– (ğœ†ğ‘£ ) = 0 for all ğ‘–, ğ‘£ âˆˆ [ğ‘˜];

2

1 e
e) â‰¤ ğ›¾ âˆ— (ğ‘ˆ ) = ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼ .
âˆ¥ğ‘ˆ âˆ¥âˆâ†’1 â‰¤ ğ›¾ 2âˆ— (ğ‘ˆ
2
2
ğ›¾ 2 (ğ‘Š , ğ›¼)

ğœ‹ (ğ‘£)ğ‘ âˆ’ğ›½

e âˆ¥â„“ â†’ğ¿ ( ğœ‹b) â‰¤ 4ğ›¾ âˆ— (ğ‘ˆ ) =
âˆ¥ğ‘€
2
âˆ
2

ğ‘£
.
ğ‘† âŠ† [ğ‘˜] such that ğœ‹ (ğ‘†) Â· minğ‘£ âˆˆğ‘† ğ‘ ğ‘£ â‰¥ ğ‘‚ğ‘£=1(log(1/ğ›½))

Proof of Lemma 19. Let ğœ†1, . . . , ğœ†ğ‘˜ , ğœ‡ 1, . . . , ğœ‡ğ‘˜ , and ğœ‹ be as given
by equations (4) - (7). Since ğ‘ ğ‘£ (ğœ‡ ğ‘£ ) > 0 for all ğ‘£, we may apply
Lemma 20 with ğ‘ ğ‘£ = ğ‘ ğ‘£ (ğœ‡ ğ‘£ ) and ğ›½ = ğ›¼/4 to obtain a subset ğ‘† âŠ† [ğ‘˜]
for which
Eğ‘‰ âˆ¼ğœ‹ ğ‘ ğ‘£ (ğœ‡ ğ‘£ ) âˆ’ ğ›¼/4 ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼/4
ğœ‹ (ğ‘†) Â· min ğ‘ ğ‘£ (ğœ‡ ğ‘£ ) â‰¥
=
.
ğ‘‚ (log(1/ğ›¼))
ğ‘‚ (log(1/ğ›¼))
ğ‘£ âˆˆğ‘†

4(ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼)
ğ›¾ 2 (ğ‘Š , ğ›¼)

ğœ‡1, . . . , e
ğœ‡ğ‘˜ and ğœ‹e be the distributions guarProof. Let ğœ†e1, . . . , ğœ†eğ‘˜ , e
e âˆˆ R [ğ‘˜ ]Ã—X be the correspondanteed to exist by Lemma 19, and let ğ‘ˆ
ing matrix with entries ğ‘¢eğ‘£,ğ‘¥ = ğœ‹e(ğ‘£)(ğœ†eğ‘£ (ğ‘¥) âˆ’ e
ğœ‡ ğ‘£ (ğ‘¥)). The entries of
e satisfy ğœ‹ (ğ‘£)ğ‘š
the matrix ğ‘€
eğ‘£,ğ‘¥ = ğ‘¢eğ‘£,ğ‘¥ , so we may apply Lemma 15
to obtain a distribution ğœ‹b such that
e âˆ¥â„“ â†’ğ¿ ( ğœ‹b) â‰¤ 4ğ›¾ âˆ— (ğ‘ˆ
e) â‰¤ 4ğ›¾ âˆ— (ğ‘ˆ ) = 4(ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼) .
âˆ¥ğ‘€
2
2
âˆ
2
ğ›¾ 2 (ğ‘Š , ğ›¼)

Now define ğœ‹e as ğœ‹ conditional on ğ‘†. In particular,
(
ğœ‹ (ğ‘£)/ğœ‹ (ğ‘†), if ğ‘£ âˆˆ ğ‘†
ğœ‹e(ğ‘£) =
0,
otherwise.
Then, for all ğ‘£ âˆˆ [ğ‘˜], define ğœ†eğ‘£ = ğœ†ğ‘£ and e
ğœ‡ ğ‘£ = ğœ‹ (ğ‘†)ğœ‡ ğ‘£ + (1 âˆ’ ğœ‹ (ğ‘†))ğœ†ğ‘£ .
This implies

Lemma 15 further guarantees that the support of ğœ‹b lies within the
Ëœ which together with the properties of the distributions
support of ğœ‹,
ğœ†e1, . . . , ğœ†eğ‘˜ , e
ğœ‡1, . . . , e
ğœ‡ğ‘˜ and ğœ‹e gives the first condition of our lemma.
â–¡

âˆ€ğ‘–, ğ‘£ âˆˆ [ğ‘˜] : ğ‘(ğœ†eğ‘£ ) = ğ‘(ğœ†ğ‘£ ) = 0,

At last, we have all the components needed to prove our lower
bounds for symmetric workloads.

âˆ€ğ‘£ âˆˆ [ğ‘˜] : ğ‘ ğ‘£ (e
ğœ‡ ğ‘£ ) = ğœ‹ (ğ‘†)ğ‘ ğ‘£ (ğœ‡ ğ‘£ ) â‰¥

ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼/4
,
ğ‘‚ (log(1/ğ›¼))

Theorem 22. Let ğ›¼, ğœ€ âˆˆ (0, 1]. Let ğ‘„ be a symmetric workload
of statistical queries with workload matrix ğ‘Š âˆˆ R [ğ‘˜ ]Ã—X . Then, for

âˆ€ğ‘£ âˆˆ [ğ‘˜] : e
ğœ‡ ğ‘£ âˆ’ ğœ†eğ‘£ = ğœ‹ (ğ‘†)(ğœ‡ ğ‘£ âˆ’ ğœ†ğ‘£ ).

some ğ›¼ â€² = Î©(ğ›¼/log(1/ğ›¼)), if
constant ğ¶, we have

By the last of these facts, together with the definition of ğœ‹e, it follows
e satisfy
that the entries ğ‘¢eğ‘£,ğ‘¥ = ğœ‹e(ğ‘£)(ğœ†eğ‘£ (ğ‘¥) âˆ’ e
ğœ‡ ğ‘£ (ğ‘¥)) of the matrix ğ‘ˆ
(
ğ‘¢ ğ‘£,ğ‘¥ , if ğ‘£ âˆˆ ğ‘†
ğ‘¢eğ‘£,ğ‘¥ =
0,
otherwise.

ğ¶ log 2ğ‘˜
ğ›¾ 2 (ğ‘Š ,ğ›¼) 2
â‰¥ (ğ›¼ â€² ) 2 for a large enough
ğœ€ 2ğ›¼ 2

â„“âˆ ,loc
scğœ€,0
(ğ‘„, ğ›¼ â€² ) = Î©




ğ›¾ 2 (ğ‘Š , ğ›¼) 2
.
ğœ€ 2ğ›¼ 2

Proof. Let ğ›¼ â€² = Î©(ğ›¼/log(1/ğ›¼)) be a value that will be decided shortly, and ğ¶ â€² be a sufficiently
large constant. If we
n
o run

In other words, ğ‘ˆe is obtained from ğ‘ˆ by replacing some of its rows
with the zero-vector. It is easy to see from the definition of ğ›¾ 2âˆ— that
this implies ğ›¾ 2âˆ— (ğ‘ˆe) â‰¤ ğ›¾ 2âˆ— (ğ‘ˆ ).
â–¡

ğ¶ â€² log 2ğ‘˜

a ğœ€-DP mechanism M on ğ‘› = max scâ„“âˆ (M, ğ‘„, ğ›¼ â€² ), (ğ›¼ â€² ) 2 samples drawn i.i.d. from some distribution ğœ‡ on X, then, by classical

433

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman



uniform convergence results, E ğ‘› âˆ¥ğ‘„ (ğ‘‹ ) âˆ’ ğ‘„ (ğœ‡)âˆ¥ âˆ â‰¤ ğ›¼ â€², where

and ğœ†bğ‘£ = ğœ†eğ‘£ , given for ğ‘£ âˆˆ [ğ‘˜]. We have

ğ‘‹ âˆ¼ğœ‡

DKL (TM (ğœ†bğ‘›ğœ‹b)âˆ¥TM (b
ğœ‡ğ‘›ğœ‹b))

ğ‘„ (ğœ‡) = (ğ‘ 1 (ğœ‡), . . . , ğ‘ğ‘˜ (ğœ‡)). Therefore, the mechanism will satisfy


E ğ‘› âˆ¥M (ğ‘‹ ) âˆ’ ğ‘„ (ğœ‡)âˆ¥ âˆ â‰¤ 2ğ›¼ â€² .

! 2ï£¹
ï£®
ï£¯
ï£º
ï£¯
â‰¤ ğ‘‚ (ğœ€ ğ‘›) Â·
max
E ï£¯ E [ğ‘“ğ‘¥ ] âˆ’ E [ğ‘“ğ‘¥ ] ï£ºï£º
bğ‘‰
ğœ‹ ï£¯ ğ‘¥âˆ¼ğœ†
ğ‘¥âˆ¼b
ğœ‡ğ‘‰
ğ‘“ âˆˆRX : âˆ¥ ğ‘“ âˆ¥âˆ â‰¤1 ğ‘‰ âˆ¼b
ï£º
ï£°
ï£»
! 2ï£¹
ï£®
ï£¯
ï£º
2
2
ï£¯
= ğ‘‚ (ğœ€ ğ‘›) Â· ğ›½
max
E ï£¯ E [ğ‘“ğ‘¥ ] âˆ’ E [ğ‘“ğ‘¥ ] ï£ºï£º
eğ‘‰
ğ‘¥âˆ¼e
ğœ‡ğ‘‰
ğœ‹ ï£¯ ğ‘¥âˆ¼ğœ†
ğ‘“ âˆˆRX : âˆ¥ğ‘“ âˆ¥âˆ â‰¤1 ğ‘‰ âˆ¼b
ï£º
ï£°
ï£»
2

ğ‘Š
â€¢
ğ‘ˆ
âˆ’
ğ›¼
.
â‰¤ ğ‘‚ (ğœ€ 2ğ‘›) Â· ğ›½ 2 Â·
ğ›¾ 2 (ğ‘Š , ğ›¼)
2

(8)

ğ‘‹ âˆ¼ğœ‡

We will show that for any ğœ€-LDP mechanism M such that (8) holds
for an arbitrary ğœ‡, we must have



ğ›¾ 2 (ğ‘Š , ğ›¼) 2
ğ‘›=Î©
.
ğœ€ 2ğ›¼ 2

(9)

n
o


ğ¶ â€² log 2ğ‘˜
ğ›¾ (ğ‘Š ,ğ›¼) 2
Therefore, we get that max scâ„“âˆ (M, ğ‘„, ğ›¼ â€² ), (ğ›¼ â€² ) 2
= Î© 2 ğœ€ 2ğ›¼ 2 ,
ğ›¾ (ğ‘Š ,ğ›¼) 2

which implies the theorem by the assumption on 2 ğœ€ 2 ğ›¼ 2 .
e âˆˆ
Let ğœ†e1, . . . , ğœ†eğ‘˜ , e
ğœ‡1, . . . , e
ğœ‡ğ‘˜ and ğœ‹b be the distributions, and ğ‘€
[ğ‘˜
]Ã—X
R
the matrix, guaranteed to exist by Lemma 21. The matrix
e has entries ğ‘š
ğ‘€
eğ‘£,ğ‘¥ = ğœ†eğ‘£ (ğ‘¥) âˆ’ e
ğœ‡ ğ‘£ (ğ‘¥) and satisfies

for all ğ‘– in the support of ğœ‹b. In particular, if we set
ğ›¼â€² =

Equivalently,
! 2ï£¹ 
ï£®

ï£¯
ï£º
4(ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼) 2
ï£¯
ï£º
max
E ï£¯ E [ğ‘“ğ‘¥ ] âˆ’ E [ğ‘“ğ‘¥ ] ï£º â‰¤
.
ğ›¾ 2 (ğ‘Š , ğ›¼)
eğ‘‰
ğ‘¥âˆ¼e
ğœ‡ğ‘‰
ğœ‹ ï£¯ ğ‘¥âˆ¼ğœ†
ğ‘“ âˆˆRX : âˆ¥ğ‘“ âˆ¥âˆ â‰¤1 ğ‘‰ âˆ¼b
ï£º
ï£°
ï£»

samples are required for privacy ğœ€ and accuracy ğ›¼ â€² . Indeed, by
taking ğ›½ = ğ‘ˆ â€¢ğ‘Š
ğ›¼ , we get that

 !
ğ›¾ 2 (ğ‘Š , ğ›¼) 2
ğ‘›=Î©
ğœ€ğ›¼

By Lemma 11, this implies

â€²
samples are required for
 privacy ğœ€ and accuracy ğ›¼ which satisfies
ğ›½ (ğ‘Š â€¢ğ‘ˆ âˆ’ğ›¼/4)

ğ›¼
ğ›¼ â€² â‰¥ ğ‘‚ (log(1/ğ›¼)) = Î© log(1/ğ›¼)
.


ğ›¾ 2 (ğ‘Š ,ğ›¼) 2
In both cases, ğ‘› = Î© ğœ€ 2 ğ›¼ 2
samples are required for privacy


ğ›¼
ğœ€ and accuracy ğ›¼ â€² , where ğ›¼ â€² = Î© log(1/ğ›¼)
â–¡

(10)

Lemma 21 guarantees further that ğ‘ğ‘– (ğœ†eğ‘£ ) = 0 for all ğ‘–, ğ‘£ âˆˆ [ğ‘˜],
ğ‘Š â€¢ğ‘ˆ âˆ’ğ›¼/4
while ğ‘ ğ‘£ (e
ğœ‡ ğ‘£ ) â‰¥ ğ‘‚ (log(1/ğ›¼)) for all ğ‘£ in the support of ğœ‹b. Let ğ›¼ â€² =

The symmetrization techniques of

1
ğœ‡ ğ‘£ ). Then a mechanism M satisfying (8) can distin8 minğ‘£ âˆˆ [ğ‘˜ ] ğ‘ ğ‘£ (e
guish between the distributions ğœ†eğ‘›ğœ‹b and e
ğœ‡ğ‘›ğœ‹b with constant probability,
and, by Pinskerâ€™s inequality, DKL (TM (ğœ†eğ‘›ğœ‹b)âˆ¥TM (e
ğœ‡ğ‘›ğœ‹b)) is bounded

Theorem 23 (Formal version of Theorem 1). Let ğ›¼, ğœ€ âˆˆ (0, 1].
Let ğ‘„ be a collection
of queries with workload matrix ğ‘Š . Then, for

ğ›¾ (ğ‘Š ,ğ›¼) 2

ğ›¼
some ğ›¼ â€² = Î© log(1/ğ›¼)
, if 2 ğœ€ 2 ğ›¼ 2
enough constant ğ¶, we have

from below by some constant ğ¶ > 0. By (10), this implies that
ğ‘›=Î©



2
ğ›¾ 2 (ğ‘Š , ğ›¼)
ğœ€ Â· (ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼)

ğ¶ log 2ğ‘˜

ğ¶
â‰¥ (ğ›¼ â€² ) 2 + ğœ€ 2 (ğ›¼
â€² ) 2 for a large



ğ›¾ 2 (ğ‘Š , ğ›¼)
â„“âˆ ,loc
scğœ€,0
(ğ‘„, ğ›¼ â€² ) = Î©
ğœ€ 2ğ›¼ 2

3.6

samples are required to obtain accuracy ğ›¼ â€² /4 and privacy ğœ€.
Case 1: ğ‘Š â€¢ ğ‘ˆ â‰¤ 2ğ›¼. Recall that ğ‘Š â€¢ğ‘ˆ â‰¥ ğ›¼. Hence, if ğ‘Š â€¢ğ‘ˆ â‰¤ 2ğ›¼,


ğ›¾ (ğ‘Š ,ğ›¼) 2
and furthermore
then ğ‘› = Î© 2 ğœ€ 2 ğ›¼ 2
ğ›¼â€² â‰¥

ğ›½ (ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼/4)
1
min ğ‘ ğ‘£ (b
ğœ‡ğ‘£ ) â‰¥
8 ğ‘£
ğ‘‚ (log(1/ğ›¼))

and (8) holds for M and this value of ğ›¼ â€² , then M can distinguish between ğœ†bğ‘›ğœ‹b and b
ğœ‡ğ‘›ğœ‹b. This implies DKL (TM (ğœ†bğ‘›ğœ‹b)âˆ¥TM (b
ğœ‡ğ‘›ğœ‹b)) is bounded
below by a constant, from which we obtain that


ğ›¾ 2 (ğ‘Š , ğ›¼) 2
ğ‘›=Î©
ğœ€ğ›½ Â· (ğ‘Š â€¢ ğ‘ˆ )

e âˆ¥â„“ â†’ğ¿ ( ğœ‹b) â‰¤ 4(ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼) .
âˆ¥ğ‘€
âˆ
2
ğ›¾ 2 (ğ‘Š , ğ›¼)



ğ‘Š â€¢ğ‘ˆ âˆ’ğ›¼ 2
DKL (TM (ğœ†eğ‘›ğœ‹b)âˆ¥TM (e
ğœ‡ğ‘›ğœ‹b)) â‰¤ ğ‘‚ (ğ‘›ğœ€ 2 ) Â·
ğ›¾ 2 (ğ‘Š , ğ›¼)

Also, ğ‘ğ‘– (ğœ†bğ‘£ ) = 0 for all ğ‘–, ğ‘£ âˆˆ [ğ‘˜], while


ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼/4
ğ‘ ğ‘£ (b
ğœ‡ ğ‘£ ) = ğ›½ Â· ğ‘ ğ‘£ (e
ğœ‡ğ‘£ ) â‰¥ ğ›½ Â·
ğ‘‚ (log(1/ğ›¼))

2

.

Applications of the Lower Bounds

In this subsection we apply Theorem 23 to several workloads
of interest, and, using known bounds on the approximate ğ›¾ 2 norm,
prove new lower bounds on the sample complexity of these workloads.
We start with the threshold queries ğ‘„ğ‘‡cdf . Identifying ğ‘ğ‘¡ with
ğ‘¡, we see that the corresponding workload matrix ğ‘Š is a lower
triangular matrix, with entries equal to 1 on and below the main
diagonal. Let us consider a different matrix ğ‘Š â€² = 2ğ‘Š âˆ’ ğ½ , where ğ½
is the all-ones ğ‘‡ Ã— ğ‘‡ matrix. Forster et al. [FSSS03] showed a lower



ğ‘Š â€¢ ğ‘ˆ âˆ’ ğ›¼/4
ğ›¼
=Î©
ğ‘‚ (log(1/ğ›¼))
log(1/ğ›¼)

Case 2: ğ‘Š â€¢ ğ‘ˆ > 2ğ›¼. However, if ğ‘Š â€¢ğ‘ˆ > 2ğ›¼, then, for ğ›½ âˆˆ [0, 1],
we may instead consider the distributions b
ğœ‡ ğ‘£ = (1 âˆ’ ğ›½) Â· ğœ†eğ‘£ + ğ›½ Â· e
ğœ‡ğ‘£

434

The Power of Factorization Mechanisms in Local and Central Differential Privacy

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

bound on the margin complexity of ğ‘Š â€² , which implies that for any
â€² â‰¥ 1 holds for all ğ‘¡, ğ‘¥ âˆˆ [ğ‘‡ ], we have
b such that ğ‘¤
ğ‘Š
bğ‘¡,ğ‘¥ ğ‘¤ğ‘¡,ğ‘¥
b ) = Î©(logğ‘‡ ).
ğ›¾ 2 (ğ‘Š

an algorithm and lower bound for probably approximately correct
learning in the local model.
A concept ğ‘ : X + â†’ {âˆ’1, +1} from a concept class C identifies
each sample ğ‘¥ of X + with a label ğ‘ (ğ‘¥). The labelled pair (ğ‘¥, ğ‘ (ğ‘¥)) =
(ğ‘¥, 1) may be identified with the sample ğ‘¥ of X + , while the labelled
pair (ğ‘¥, ğ‘ (ğ‘¥)) = (ğ‘¥, âˆ’1) may be identified with the sample âˆ’ğ‘¥ of
X âˆ’ . Let ğ‘ : X â†’ {âˆ’1, +1} be given by
(
ğ‘ (ğ‘¥),
if ğ‘¥ âˆˆ X +
ğ‘(ğ‘¥) =
âˆ’ğ‘ (âˆ’ğ‘¥), if ğ‘¥ âˆˆ X âˆ’

(11)

e satisfies âˆ¥ğ‘Š
e âˆ’ ğ‘Š â€² âˆ¥1â†’âˆ â‰¤ 1 , then we can take
Note that, if ğ‘Š
2
b = 2ğ‘Š
e , and (11) implies ğ›¾ 2 (ğ‘Š â€², 1/2) = Î©(logğ‘‡ ). Finally, homoğ‘Š
geneity and the triangle inequality for ğ›¾ 2 , and ğ›¾ 2 (ğ½ ) = 1 imply
that ğ›¾ 2 (ğ‘Š , 1/2) â‰¥ 12 ğ›¾ 2 (ğ‘Š â€², 1/2) âˆ’ 21 = Î©(logğ‘‡ ). Together with
Theorem 23, this gives Corollary 4.
parity
Next, we consider the parity queries ğ‘„ğ‘‘,ğ‘¤ . Note that the workğ‘‘
load matrix ğ‘Š of these queries is a submatrix consisting of ğ‘¤

ğ‘‘
rows of the 2ğ‘‘ Ã— 2ğ‘‘ Hadamard matrix. Let ğ‘  = 2ğ‘‘ ğ‘¤
be the number of entries in ğ‘Š . To prove a lower bound on ğ›¾ 2 (ğ‘Š , ğ›¼), we can
use Lemma 12 with ğ‘ˆ = ğ‘Š . The rows of a Hadamard matrix are
pairwise orthogonal and have â„“2 norm 2ğ‘‘/2 , and, so, Lemma 13,
used with ğ‘ƒ and ğ‘„ set to appropriately scaled copies of the identity
âˆš
matrices of the respective dimensions, implies that ğ›¾ 2âˆ— (ğ‘ˆ ) â‰¤ ğ‘ 2ğ‘‘ .
Moreover, ğ‘Š â€¢ ğ‘ˆ = âˆ¥ğ‘ˆ âˆ¥1 = ğ‘ , and, by Lemma 12, we have
  1/2 !
âˆš
ğ‘ 
ğ‘‘
ğ›¾ 2 (ğ‘Š , 1/2) â‰¥
=Î©
.
ğ‘¤
2 (ğ‘‘/2)+1

Then the loss of the concept ğ‘ on a dataset ğ‘‹ = ((ğ‘¥ 1, ğ‘¦1 ), . . . , (ğ‘¥ğ‘› , ğ‘¦ğ‘› )),
denoted Î›ğ‘‹ (ğ‘), is
ğ‘›

Î›ğ‘‹ (ğ‘) =
=

ğ‘‘ğ‘,ğ‘¥ = ğ‘ (ğ‘¥)
and takes advantage of the fact that the workload matrix ğ‘Š of
the corresponding query workload ğ‘„ is obtained by extending ğ·
to C Ã— X in the usual way with ğ‘¤ğ‘,ğ‘¥ = ğ‘‘ğ‘,ğ‘¥ and ğ‘¤ğ‘,âˆ’ğ‘¥ = âˆ’ğ‘‘ğ‘,ğ‘¥
for ğ‘ âˆˆ ğ‘„ and ğ‘¥ âˆˆ X + when ğ‘ is the concept that corresponds to
ğ‘. In particular, the queries ğ‘„ are symmetric, and, by Lemma 18,
ğ›¾ 2 (ğ·, ğ›¼) = ğ›¾ 2 (ğ‘Š , ğ›¼).
In order to state our results for agnostic learning, we need to
define notation for population loss, in addition to the empirical loss
defined above. For a distribution ğœ‡ over X + Ã— {âˆ’1, +1}, we will use
Î› ğœ‡ (ğ‘) to denote the loss of the concept ğ‘ on ğœ‡, given by

(2ğ‘‘) ğ‘¤

workload matrix ğ‘Š for ğ‘„ğ‘‘,ğ‘¤
. Let ğ‘  = 2ğ‘‘ ğ‘¤ ğ‘¤ be the number
of entries in ğ‘Š â€² . By Theorem 8.1. in [She11], we have that, for any
ğ›¼ â‰¤ 16 ,


  deg1/3 ( ğ‘“ )/2
1 e
e âˆ’ ğ‘Š â€² âˆ¥1â†’âˆ â‰¤ ğ›¼ = Î© ğ‘‘
min âˆš âˆ¥ğ‘Š
âˆ¥ğ‘¡ğ‘Ÿ : âˆ¥ğ‘Š
,
ğ‘¤
ğ‘ 

Î› ğœ‡ (ğ‘) =

Pr
(ğ‘¥,ğ‘¦)âˆ¼ğœ‡

[ğ‘ (ğ‘¥) â‰  ğ‘¦].

For ğ›¼, ğ›½ > 0 we will say that the mechanism M (ğ›¼,ğ›½)-learns C with
ğ‘› samples if, for all distributions ğœ‡ over X + Ã— {âˆ’1, +1}, given as
input a dataset ğ‘‹ = ((ğ‘¥ 1, ğ‘¦1 ), . . . , (ğ‘¥ğ‘› , ğ‘¦ğ‘› )) of ğ‘› samples drawn IID
from ğœ‡, M outputs a concept ğ‘ âˆˆ C and an estimate Î› such that

e âˆ¥ğ‘¡ğ‘Ÿ is the trace-norm, i.e., the sum of singular values of
where âˆ¥ğ‘Š
e
ğ‘Š , and deg1/3 (ğ‘“ ) is the (1/3)-approximate degree of ğ‘“ , which is
âˆš
e âˆ¥ğ‘¡ğ‘Ÿ is a lower bound on
known to be Î©( ğ‘¤) [NS94]. Since âˆš1 âˆ¥ğ‘Š

Pr [Î› ğœ‡ (ğ‘) â‰¤ min
Î› ğœ‡ (ğ‘ â€² ) + ğ›¼ and |Î› âˆ’ Î› ğœ‡ (ğ‘)| â‰¤ ğ›¼] â‰¥ 1 âˆ’ ğ›½.
â€²

ğ‘ 

e ) (see [LMSS07, Lemma 3.4]), this implies
ğ›¾ 2 (ğ‘Š
  Î© ( âˆšğ‘¤)
1 e
ğ‘‘
ğ›¾ 2 (ğ‘Š , 1/6) â‰¥ âˆš âˆ¥ğ‘Š âˆ¥ğ‘¡ğ‘Ÿ = Î©
,
ğ‘¤
ğ‘ 

M,ğ‘‹

ğ‘ âˆˆC

Typically, the learning problem does not require outputting an
estimate of the loss Î› ğœ‡ (ğ‘), since it is usually easy to compute such
an estimate with few additional samples, once a concept ğ‘ has been
computed. In the local model, however, this would require an additional round of interactivity. Since we focus on the non-interactive
local model, it is natural to make this additional requirement on
the learning algorithm.
Since we wish to bound population loss, it is necessary to assume that there are sufficiently many samples to guarantee uniform

giving us Corollary 6.

4

ğ‘›
ğ‘›
1
1 1
1 Ã•
1 Ã•
1
ğ‘“ (ğ‘¥ğ‘– )ğ‘¦ğ‘– = âˆ’
ğ‘(ğ‘¥ğ‘– Â· ğ‘¦ğ‘– ) = âˆ’ ğ‘(ğ‘‹ )
âˆ’
2 2ğ‘› ğ‘–=1
2 2ğ‘› ğ‘–=1
2 2

where ğ‘‹ is the dataset (ğ‘¥ 1 Â· ğ‘¦1, . . . , ğ‘¥ğ‘› Â· ğ‘¦ğ‘› ). In this way, estimating
Î›ğ‘‹ (ğ‘) given the dataset ğ‘‹ is equivalent to estimating ğ‘(ğ‘‹ ) given
the dataset ğ‘‹ . More generally, if we consider the query workload
ğ‘„ consisting of all such queries ğ‘ obtained from some concept ğ‘
of C in this way, then estimating ğ‘„ (ğ‘‹ ) is equivalent to estimating
Î›ğ‘‹ (C) = (Î›ğ‘‹ (ğ‘))ğ‘ âˆˆ C . This idea allows us to adapt the algorithm
of Theorem 10 for estimating linear queries to an algorithm for
learning. The result is stated in terms of the concept matrix ğ· âˆˆ
+
R CÃ—X of C with entries given by

This gives Corollary 5.
Finally, we treat marginal queries. Let us define these queries
slightly more generally than we did in the introduction, by allowing
marginal
for negation. We define ğ‘„ğ‘‘,ğ‘¤
to consist of the queries ğ‘ğ‘†,ğ‘¦ (ğ‘‹ ) =
1 Ãğ‘› Ã
I[ğ‘¥
=
ğ‘¦
],
with
ğ‘†
ranging over subsets of [ğ‘‘] of
ğ‘–,ğ‘—
ğ‘—
ğ‘— âˆˆğ‘†
ğ‘› ğ‘–=1
size at most ğ‘¤, and ğ‘¦ ranging over {0, 1}ğ‘‘ . These queries can be
expressed in terms of the ğ‘ğ‘† queries defined in the introduction by
doubling the dimension ğ‘‘.
marginal
To prove a lower bound for ğ‘„ğ‘‘,ğ‘¤
, we use the pattern matrix method of Sherstov [She11]. We will omit a full definition of
a pattern matrix here, and refer the reader to Sherstovâ€™s paper.
Instead, we remark that, denoting by ğ‘“ the AND function on ğ‘¤
(2ğ‘‘) ğ‘¤
bits, a (ğ‘‘, ğ‘¤, ğ‘“ )-pattern matrix ğ‘Š â€² is a ğ‘¤ ğ‘¤ Ã— 2ğ‘‘ submatrix of the
marginal

1Ã•
(1 âˆ’ I[ğ‘“ (ğ‘¥ğ‘– ) = ğ‘¦ğ‘– ])
ğ‘› ğ‘–=1

NON-INTERACTIVE LOCAL DP:
PAC LEARNING

It turns out that we are able to translate our algorithm and
lower bound for answering linear queries in the local model into

435

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman

5.1

convergence. It suffices to assume, for some constant ğ¶, that the
ğ¶ log 2| C |
number of samples is at least ğ‘› â‰¥
to guarantee
ğ›¼2
Pr [âˆ€ğ‘ âˆˆ C, |Î›ğ‘‹ (ğ‘) âˆ’ Î› ğœ‡ (ğ‘)| â‰¤ ğ›¼] â‰¥ 1 âˆ’
ğ‘‹

ğ›½
2

when ğ‘‹ consists of ğ‘› IID samples drawn from ğœ‡.
Theorem 24. Let ğ›¼, ğ›½ âˆˆ (0, 1), and let ğœ€ > 0. There exists an ğœ€LDP mechanism M such that, for any concept class C of size |C| = ğ‘˜
+
with corresponding concept matrix ğ· âˆˆ R CÃ—X , it suffices to have a
dataset ğ‘‹ = ((ğ‘¥ 1, ğ‘¦1 ), . . . , (ğ‘¥ğ‘› , ğ‘¦ğ‘› )) of
 
 

ğ›¾ 2 (ğ·, ğ›¼) 2 log ğ‘˜
log ğ‘˜
ğ‘› = max ğ‘‚
,
ğ‘‚
ğœ€ 2ğ›¼ 2
ğ›¼2

where ğ‘ is a random variable over Rğ‘„ whose distribution does not
depend on â„. Without loss of generality, we assume E[ğ‘ ] = 0. Let
Î£ = E ğ‘ğ‘ ğ‘‡ be the covariance matrix of ğ‘ . Then the â„“2 error of
such a mechanism is
v
u
#
t "
âˆ¥M (â„) âˆ’ ğ‘›1 ğ‘Š â„âˆ¥22
â„“22
err (M,ğ‘Š , ğ‘›) = max
E
|ğ‘„ |
â„: âˆ¥â„ âˆ¥1 =ğ‘›
v
u
s
"
#
t
âˆ¥ğ‘ âˆ¥22
Tr(Î£)
= E
=
ğ‘›|ğ‘„ |
ğ‘›|ğ‘„ |

samples to guarantee that M (ğ›¼, ğ›½)-learns C.
Applying the same ideas, we know that if we estimate the quantity minğ‘ âˆˆ C Î›ğ‘‹ (ğ‘), then we can estimate maxğ‘ âˆˆğ‘„ ğ‘(ğ‘‹ ). Similarly,
estimating minğ‘ âˆˆ C Î› ğœ‡ (ğ‘) is equivalent to estimating maxğ‘ âˆˆğ‘„ ğ‘(ğœ‡ â€² )
where ğœ‡ â€² is the distribution on X obtained from ğœ‡ by associating
samples of the forms (ğ‘¥, 1) and (ğ‘¥, âˆ’1) with ğ‘¥ and âˆ’ğ‘¥, respectively.
Since the matrix ğ‘Š âˆˆ R CÃ—X obtained from ğ· is symmetric, and
estimating maxğ‘ âˆˆğ‘„ ğ‘(ğœ‡ â€² ) is precisely what is required for the lower
bound of Theorem 22, we the following lower bound for agnostic
learning.

In this section, we will show that, if M is (ğœ€, ğ›¿)-differentially private (for ğœ€, ğ›¿ smaller than some absolute constants), then Tr(Î£) =
|ğ‘„ |ğ›¾ (ğ‘Š ) 2

Lemma 26 ([KRSU10]). For any single-query workload ğ‘¤ âˆˆ R |X | ,
and any data-independent mechanism M (â„) = ğ‘›1 ğ‘¤ âŠ¤â„ + ğ‘›1 ğ‘§ that is
(ğœ€, ğ›¿)-differentially
private for ğœ€, ğ›¿ smaller than some absolute con 
1 âˆ¥ğ‘¤ âˆ¥ for some absolute constant ğ¶ > 0.
stants, E ğ‘§ 2 â‰¥ ğ¶ğœ€
âˆ

ğ¶ log 2ğ‘˜
ğ›¾ (ğ‘Š ,ğ›¼)
For some ğ›¼ â€² = Î©
, if 2ğœ€ 2 ğ›¼ 2 â‰¥ ğ›¼ â€² for a large enough
constant ğ¶ > 0, then any ğœ€-LDP mechanism M which (ğ›¼ â€², ğ›½)-learns
ğ›¼
log(1/ğ›¼)

C requires


ğ›¾ 2 (ğ‘Š , ğ›¼) 2
ğœ€ 2ğ›¼ 2

|X |



Next, we define the sensitivity polytope ğ¾ = ğ‘Š ğµ 1 , where
|X |
ğµ1

= {â„ âˆˆ R |X | : âˆ¥â„âˆ¥1 â‰¤ 1}. With this definition, we have
that for any pair of neighboring datasets ğ‘‹, ğ‘‹ â€² with associated histograms â„, â„ â€² , we have ğ‘Š (â„ âˆ’ â„ â€² ) âˆˆ ğ¾. The next lemma says that
the covariance matrix Î£ defines an ellipsoid that contains at least a
constant multiple of the sensitivity polytope.

samples as input.

5

ğ›¾ (ğ‘Š )

ğ¹
Î©(
), and thus err(M,ğ‘Š , ğ‘›) = Î©( ğ¹ ğœ€ğ‘› ).
ğœ€2
We start with the following basic lemma about differential privacy, which says that the variance of any differentially private
algorithm for answering a single query ğ‘¤ must be proportional to
the sensitivity of the query.

Theorem 25. Let ğ›½ âˆˆ (0, 1) be a small enough constant, and let
+
ğœ€ > 0. Let C be a concept class with concept matrix ğ· âˆˆ R CÃ—X .

ğ‘›=Î©

Data-Independent Mechanisms

Let ğ‘„ be a workload of linear queries over data universe X and
let ğ‘Š âˆˆ Rğ‘„Ã—X be the matrix form of this workload. An instanceindependent mechanism M can be written (as a function of the
histogram of the dataset) as,
1
M (â„) = (ğ‘Š â„ + ğ‘ )
ğ‘›

CHARACTERIZING CENTRAL DP FOR
LARGE DATASETS

Lemma 27. Let ğ‘Š be a workload matrix such that the sensitivity
polytope ğ¾ is full dimensional. Let M be an (ğœ€, ğ›¿)-differentially private data-independent mechanism for ğ‘Š that has covariance matrix
Î£, for ğœ€, ğ›¿ smaller than some absolute constants. Then Î£ is invertible,
and
max âˆ¥Î£âˆ’1/2ğ‘¦ âˆ¥22 = max ğ‘¦ âŠ¤ Î£âˆ’1ğ‘¦ â‰¤ ğ¶ 2ğœ€ 2

The goal of this section is to show that the sample complexity of
releasing a given set of linear queries with workload matrix ğ‘Š is


ğ›¾ ğ¹ (ğ‘Š )
â„“22
sc (ğ‘Š , ğ›¼, ğœ€, ğ›¿) = Î˜
ğ›¼ğœ€
when ğ›¼ is sufficiently small (smaller than some ğ›¼ âˆ— (ğ‘„, ğœ€)). Or, equiv2
ğ›¾ (ğ‘Š )
alently, we show that errâ„“2 (ğ‘Š , ğ‘›, ğœ€, ğ›¿) = Î˜( ğ¹ ğœ€ğ‘› ), when ğ‘› is suffiâˆ—
ciently large (larger than some ğ‘› (ğ‘„, ğœ€)).
The proof consists of two steps. First, we argue that error

ğ‘¦ âˆˆğ¾

ğ‘¦ âˆˆğ¾

for some absolute constant ğ¶ > 0.
Proof. By post-processing, for any ğ‘¢ âˆˆ R |X | ,
1
1
ğ‘¢ âŠ¤ M (â„) = ğ‘¢ âŠ¤ğ‘Š â„ + ğ‘¢ âŠ¤ğ‘
ğ‘›
ğ‘›
is an (ğœ€, ğ›¿)-DP mechanism for the single query ğ‘¢ âŠ¤ğ‘Š . The sensitivity
polytope of the workload ğ‘¢ âŠ¤ğ‘Š is the line [âˆ’â„ğ¾ (ğ‘¢), â„ğ¾ (ğ‘¢)], where
â„ğ¾ (ğ‘¢) = maxğ‘¦ âˆˆğ¾ ğ‘¢ âŠ¤ğ‘¦ is the support function. By Lemma 26, if M
is an (ğœ€, ğ›¿)-differentially private mechanism, then for some constant
ğ¶,
âˆš
â„ (ğ‘¢)
âˆ¥Î£1/2ğ‘¢ âˆ¥2 = ğ‘¢ âŠ¤ Î£ğ‘¢ â‰¥ ğ¾
.
(12)
ğ¶ğœ€

ğ›¾ ğ¹ (ğ‘Š )
)
ğœ€ğ‘›
is necessary for every ğ‘› if we restrict attention only to mechanisms
that are data-independent. That is, mechanisms that perturb the output with noise from a fixed distribution independent of the dataset.
Then, we apply a lemma of Bhaskara et al. [BDKT12] that says,
when ğ‘› is sufficiently large, any instance-dependent mechanism
can be replaced with an instance-independent mechanism with the
same error and similar privacy parameters.
err(ğ‘Š , ğ‘›, ğœ€, ğ›¿) = Î˜(

436

The Power of Factorization Mechanisms in Local and Central Differential Privacy

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

If ğ¾ is full dimensional, then in particular we have â„ğ¾ (ğ‘’ğ‘– ) > 0 for
any standard basis vector ğ‘’ğ‘– , which implies that the matrix Î£ is
positive definite and invertible.
By change of variables we can write ğ‘£ = Î£1/2ğ‘¢ and rewrite (12)
as
1
1
1
âˆ¥ğ‘£ âˆ¥2 â‰¥
Â·â„ğ¾ (Î£âˆ’1/2 ğ‘£) =
Â·max (Î£âˆ’1/2 ğ‘£) âŠ¤ğ‘¦ =
Â·max ğ‘£ âŠ¤ Î£âˆ’1/2ğ‘¦
ğ¶ğœ€
ğ¶ğœ€ ğ‘¦ âˆˆğ¾
ğ¶ğœ€ ğ‘¦ âˆˆğ¾

Theorem 30. Let ğ‘„ be linear queries with symmetric workload
matrix ğ‘Š âˆˆ Rğ‘„Ã—X . Then for every ğœ€, ğ›¿ smaller than some absolute
constants, there exists ğ‘› âˆ— âˆˆ N such that
ğ›¾ ğ¹ (ğ‘Š )
â„“22
âˆ€ğ‘› â‰¤ ğ‘› âˆ— errğœ€,ğ›¿
(ğ‘„, ğ‘›) â‰¥
.
ğ¶ğœ€ğ‘›
By standard transformations (see e.g. [BUV14]), we can convert
this to the following sample complexity lower bound,

Since the above holds for any unit vector ğ‘£ âˆˆ S | X |âˆ’1 , we have

Corollary 31. Let ğ‘„ be linear queries with symmetric workload
matrix ğ‘Š âˆˆ Rğ‘„Ã—X . Then for every ğœ€, ğ›¿ smaller than some absolute
constants, there exists ğ›¼ âˆ— > 0 such that
ğ›¾ ğ¹ (ğ‘Š )
â„“22
âˆ€ğ›¼ â‰¤ ğ›¼ âˆ— scğœ€,ğ›¿
(ğ‘„, ğ›¼) â‰¥
ğ¶ğœ€ğ›¼
We remark that our lower bounds may be extended to case
of non-symmetric workloads by using the same technique which
we used to obtain Lemma 17. An advantage of performing this
reduction in the central model is that we may take advantage of
the central model
of the Laplace
which will
 version

 mechanism,


max âˆ¥Î£âˆ’1/2ğ‘¦ âˆ¥2 = max max ğ‘£ âŠ¤ Î£âˆ’1/2ğ‘¦
ğ‘¦ âˆˆğ¾

ğ‘¦ âˆˆğ¾ ğ‘£ âˆˆS |X|âˆ’1

= max max ğ‘£ âŠ¤ Î£âˆ’1/2ğ‘¦ â‰¤ ğ¶ğœ€
ğ‘£ âˆˆS |X|âˆ’1 ğ‘¦ âˆˆğ¾

where the first equality is the equality-case of Cauchy-Schwarz.

â–¡

Recall that for a matrix ğ‘Š âˆˆ Rğ‘„Ã—X ,
n
o
ğ›¾ ğ¹ (ğ‘Š ) = inf |ğ‘„1| 1/2 âˆ¥ğ‘…âˆ¥ğ¹ âˆ¥ğ´âˆ¥1â†’2 : ğ‘…ğ´ = ğ‘Š .
We now prove our main result, which shows that the error of dataindependent private mechanisms must be proportional to ğ›¾ ğ¹ (ğ‘Š ).

1 rather than ğ‘› = ğ‘‚
1
use only ğ‘› = ğ‘‚ ğ›¼ğœ€
samples. In this way,
ğ›¼ 2ğœ€ 2
Theorem 28, Theorem 30, and Corollary 31 may be obtained under
the additional assumption that ğ›¾ ğ¹ (ğ‘Š ) > ğ· for a sufficiently large
constant ğ· > 0.
We also note that Theorem 28, Theorem 30, and Corollary 31
2 -error, defined by
may be extended to â„“âˆ

2
2  1/2
errâ„“âˆ (M, ğ‘„, ğ‘›) = maxğ‘› E âˆ¥M (ğ‘‹ ) âˆ’ ğ‘„ (ğ‘‹ )âˆ¥âˆ
,

Theorem 28. Let ğ‘Š be a workload matrix. Let M is a (ğœ€, ğ›¿)differentially private data-independent mechanism for ğ‘Š with covariance matrix Î£, for ğœ€, ğ›¿ smaller than some absolute constants. Then


2
ğ›¾ ğ¹ (ğ‘Š )
errâ„“2 (M,ğ‘Š , ğ‘›) = Î©
.
ğ¶ğœ€ğ‘›

ğ‘‹ âˆˆX M

Proof. Let ğ‘¤ 1, . . . , ğ‘¤ | X | be the columns of the workload matrix
ğ‘Š . Let ğ´ = Î£âˆ’1/2ğ‘Š with columns ğ‘ 1, . . . , ğ‘ | X | and let ğ‘… = Î£1/2 so
that ğ‘…ğ´ = ğ‘Š . By Lemma 27, the matrix ğ´ is well defined, and for
every ğ‘–, âˆ¥ğ‘ğ‘– âˆ¥ = âˆ¥Î£âˆ’1/2ğ‘¤ğ‘– âˆ¥2 â‰¤ ğ¶ğœ€. Hence âˆ¥ğ´âˆ¥1â†’2 â‰¤ ğ¶ğœ€. We also
have

2
2
â„“âˆ
â„“âˆ
with errğœ€,ğ›¿
(ğ‘„, ğ‘›), and scğœ€,ğ›¿
(ğ‘„, ğ›¼) defined analogously, and ğ›¾ ğ¹ (ğ‘Š )
replaced by ğ›¾ 2 (ğ‘Š ) in the lower bounds.

ACKNOWLEDGMENTS

âˆ¥ğ‘…âˆ¥ğ¹ = Tr(ğ‘… âŠ¤ ğ‘…) 1/2 = Tr(Î£) 1/2 = |ğ‘„ | 1/2 Â· ğ‘› Â· errâ„“2 (M,ğ‘Š , ğ‘›).
2

Combining the inequalities, we get
2
1
âˆ¥ğ‘…âˆ¥ğ¹ âˆ¥ğ´âˆ¥1â†’2 â‰¤ ğ¶ğœ€ Â· ğ‘› Â· errâ„“2 (M,ğ‘Š , ğ‘›).
ğ›¾ ğ¹ (ğ‘Š ) â‰¤
|ğ‘„ | 1/2
The theorem follows from rearranging this inequality.

5.2

Part of this work was done while the authors were visiting the
Simons Institute for Theory of Computing. We are grateful to Toniann Pitassi for many helpful discussions about local differential
privacy. AE and AN were supported by an Ontario Early Researcher
Award, and an NSERC Discovery Grant. JU was supported by NSF
grants CNS-1718088, CCF-1750640 and CNS-1816028, and a Google
Faculty Research Award.

â–¡

From Data-Dependent to Data-Independent
Mechanisms

REFERENCES

In this section we describe a reduction of Bhaskara et al. [BDKT12]
showing, in the case of symmetric workloads, that any data-dependent
mechanism with small error for datasets of arbitrary size can be
converted into a data-independent mechanism with approximately
the same error.
Lemma 29 ([BDKT12]). Let ğ‘Š âˆˆ Rğ‘„Ã—X be a symmetric workload
matrix. For every (ğœ€, ğ›¿)-differentially private mechanism M, there
exists a (2ğœ€, 2ğ‘’ ğœ€ ğ›¿)-differentially private data-independent mechanism
M â€² such that
2
2
1
errâ„“2 (M â€²,ğ‘Š , ğ‘›) â‰¤ max (ğ‘š Â· errâ„“2 (M,ğ‘Š , ğ‘š))
ğ‘› ğ‘š âˆˆN
As an immediate, corollary, lower bounds for data-independent
mechanisms imply lower bounds for arbitrary data-dependent
mechanisms for some dataset size ğ‘› âˆ— . Thus we obtain the following
theorem by combining Theorem 28 with Lemma 29.

437

[App17] Apple Differential Privacy Team. Learning with privacy at scale. Apple
Machine Learning Journal, 2017.
[BBNS19] Jaroslaw Blasiok, Mark Bun, Aleksandar Nikolov, and Thomas Steinke.
Towards instance-optimal private query release. In SODA, pages 2480â€“
2497. SIAM, 2019.
[BCD+ 07] Boaz Barak, Kamalika Chaudhuri, Cynthia Dwork, Satyen Kale, Frank
McSherry, and Kunal Talwar. Privacy, accuracy, and consistency too:
a holistic solution to contingency table release. In Proceedings of the
26th ACM Symposium on Principles of Database Systems, PODS â€™07, pages
273â€“282. ACM, 2007.
[BDKT12] Aditya Bhaskara, Daniel Dadush, Ravishankar Krishnaswamy, and Kunal Talwar. Unconditional differentially private mechanisms for linear
queries. In Proceedings of the 44th Annual ACM Symposium on Theory of
Computing, STOC â€™12, pages 1269â€“1284, 2012.
[BEM+ 17] Andrea Bittau, Ãšlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth
Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes,
and Bernhard Seefeld. Prochlo: Strong privacy for analytics in the crowd.
In Proceedings of the 26th Symposium on Operating Systems Principles,
SOSP â€™17, pages 441â€“459. ACM, 2017.
[BFJ+ 94] Avrim Blum, Merrick L. Furst, Jeffrey C. Jackson, Michael J. Kearns,
Yishay Mansour, and Steven Rudich. Weakly learning DNF and characterizing statistical query learning using fourier analysis. In Proceedings

STOC â€™20, June 22â€“26, 2020, Chicago, IL, USA

Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman

of the Twenty-Sixth Annual ACM Symposium on Theory of Computing,
23-25 May 1994, MontrÃ©al, QuÃ©bec, Canada, pages 253â€“262, 1994.
[BNS18] Mark Bun, Jelani Nelson, and Uri Stemmer. Heavy hitters and the structure of local privacy. In Proceedings of the 37th ACM Symposium on
Principles of Database Systems, PODSâ€™18, pages 435â€“447. ACM, 2018.
[BUV14] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes
and the price of approximate differential privacy. In 46th Annual ACM
Symposium on the Theory of Computing, STOC â€™14, pages 1â€“10, New York,
NY, USA, 2014.
[CSS11] T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual
release of statistics. ACM Transactions on Information and System Security
(TISSEC), 14(3):26, 2011.
[CTUW14] Karthekeyan Chandrasekaran, Justin Thaler, Jonathan Ullman, and Andrew Wan. Faster private release of marginals on small databases. In
Proceedings of the 5th ACM Conference on Innovations in Theoretical Computer Science, ITCS â€™14, pages 287â€“402, Princeton, NJ, 2014. ACM.
[DJW18] John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Minimax
optimal procedures for locally private estimation. J. Amer. Statist. Assoc.,
113(521):182â€“201, 2018.
+
[DLS 17] Aref N. Dajani, Amy D. Lauger, Phyllis E. Singer, Daniel Kifer, Jerome P.
Reiter, Ashwin Machanavajjhala, Simson L. Garfinkel, Scot A. Dahl,
Matthew Graham, Vishesh Karwa, Hang Kim, Philip Lelerc, Ian M.
Schmutte, William N. Sexton, Lars Vilhuber, and John M. Abowd. The
modernization of statistical disclosure limitation at the U.S. census bureau, 2017. Presented at the September 2017 meeting of the Census
Scientific Advisory Committee.
[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of
the 3rd Conference on Theory of Cryptography, TCC â€™06, pages 265â€“284,
Berlin, Heidelberg, 2006. Springer.
[DN03] Irit Dinur and Kobbi Nissim. Revealing information while preserving
privacy. In Proceedings of the 22nd ACM Symposium on Principles of
Database Systems, PODS â€™03, pages 202â€“210. ACM, 2003.
[DN04] Cynthia Dwork and Kobbi Nissim. Privacy-preserving datamining on
vertically partitioned databases. In Annual International Cryptology
Conference, pages 528â€“544. Springer, 2004.
[DNPR10] Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum.
Differential privacy under continual observation. In Symposium on
Theory of Computing (STOC), pages 715â€“724. ACM, 2010.
[DNT15] Cynthia Dwork, Aleksandar Nikolov, and Kunal Talwar. Efficient algorithms for privately releasing marginals via convex relaxations. Discrete
& Computational Geometry, 53(3):650â€“673, 2015.
[DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science,
9(3â€“4):211â€“407, 2014.
[DR18] John C. Duchi and Feng Ruan. The right complexity measure in locally private estimation: It is not the fisher information. arXiv preprint
arXiv:1806.05756, 2018.
[EGS03] Alexandre V. Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant.
Limiting privacy breaches in privacy preserving data mining. In PODS,
pages 211â€“222. ACM, 2003.
[EPK14] Ãšlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM Conference on Computer and Communications Security, CCSâ€™14. ACM, 2014.
[FSSS03] JÃ¼rgen Forster, Niels Schmitt, Hans Ulrich Simon, and Thorsten Suttorp.
Estimating the optimal margins of embeddings in euclidean half spaces.
Machine Learning, 51(3):263â€“281, 2003.
[GHRU11] Anupam Gupta, Moritz Hardt, Aaron Roth, and Jonathan Ullman. Privately releasing conjunctions and the statistical query barrier. In Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC â€™11,
pages 803â€“812, San Jose, CA, 2011.
[Gro53] A. Grothendieck. RÃ©sumÃ© de la thÃ©orie mÃ©trique des produits tensoriels
topologiques. Bol. Soc. Mat. SÃ£o Paulo, 8:1â€“79, 1953.

[HRS12] Moritz Hardt, Guy N. Rothblum, and Rocco A. Servedio. Private data
release via learning thresholds. In Proceedings of the Twenty-Third Annual
ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 168â€“187,
2012.
[HT10] Moritz Hardt and Kunal Talwar. On the geometry of differential privacy.
In Proceedings of the 42nd ACM Symposium on Theory of Computing,
STOC, 2010.
[JNS18] Noah Johnson, Joseph P Near, and Dawn Song. Towards practical differential privacy for sql queries. Proceedings of the VLDB Endowment,
11(5):526â€“539, 2018.
[Kea93] Michael J. Kearns. Efficient noise-tolerant learning from statistical
queries. In STOC, pages 392â€“401. ACM, May 16-18 1993.
[KLN+ 08] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya
Raskhodnikova, and Adam Smith. What can we learn privately? In
FOCS, pages 531â€“540. IEEE, Oct 25â€“28 2008.
[KN12] Subhash Khot and Assaf Naor. Grothendieck-type inequalities in combinatorial optimization. Comm. Pure Appl. Math., 65(7):992â€“1035, 2012.
[KRSU10] Shiva Prasad Kasiviswanathan, Mark Rudelson, Adam Smith, and
Jonathan Ullman. The price of privately releasing contingency tables
and the spectra of random matrices with correlated rows. In Proceedings
of the 42nd ACM Symposium on Theory of Computing, STOC â€™10, pages
775â€“784. ACM, 2010.
[KSS94] Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward efficient
agnostic learning. Machine Learning, 17(2-3):115â€“141, 1994.
[LHR+ 10] Chao Li, Michael Hay, Vibhor Rastogi, Gerome Miklau, and Andrew
McGregor. Optimizing linear counting queries under differential privacy.
In Proceedings of the 29th ACM Symposium on Principles of Database
Systems, PODSâ€™10, pages 123â€“134. ACM, 2010.
[LMSS07] Nati Linial, Shahar Mendelson, Gideon Schechtman, and Adi Shraibman.
Complexity measures of sign matrices. Combinatorica, 27(4):439â€“463,
2007.
[LS09] Nati Linial and Adi Shraibman. Lower bounds in communication
complexity based on factorization norms. Random Struct. Algorithms,
34(3):368â€“394, 2009.
[MMHM18] Ryan McKenna, Gerome Miklau, Michael Hay, and Ashwin Machanavajjhala. Optimizing error of high-dimensional statistical queries under
differential privacy. Proceedings of the VLDB Endowment, 11(10):1206â€“
1219, 2018.
[Nik14] Aleksandar Nikolov. New Computational Aspects of Discrepancy Theory.
PhD thesis, Rutgers, The State University of New Jersey, 2014.
[Nik15] Aleksandar Nikolov. An improved private mechanism for small databases.
In Automata, Languages, and Programming - 42nd International Colloquium, ICALP, pages 1010â€“1021, 2015.
[NS94] Noam Nisan and Mario Szegedy. On the degree of boolean functions as
real polynomials. Computational Complexity, 4:301â€“313, 1994.
[NT15] Aleksandar Nikolov and Kunal Talwar. Approximating hereditary discrepancy via small width ellipsoids. In Proceedings of the 26th Annual
ACM-SIAM Symposium on Discrete Algorithms, SODAâ€™15, pages 324â€“336.
SIAM, 2015.
[NTZ16] Aleksandar Nikolov, Kunal Talwar, and Li Zhang. The geometry of
differential privacy: The small database and approximate cases. SIAM J.
Comput., 45(2):575â€“616, 2016.
[Pis12] Gilles Pisier. Grothendieckâ€™s theorem, past and present. Bull. Amer. Math.
Soc. (N.S.), 49(2):237â€“323, 2012.
[She11] Alexander A. Sherstov. The pattern matrix method. SIAM J. Comput.,
40(6):1969â€“2000, 2011.
[SzÃ¶09] BalÃ¡zs SzÃ¶rÃ©nyi. Characterizing statistical query learning: Simplified
notions and proofs. In ALT, volume 5809 of Lecture Notes in Computer
Science, pages 186â€“200. Springer, 2009.
[TUV12] Justin Thaler, Jonathan Ullman, and Salil P. Vadhan. Faster algorithms
for privately releasing marginals. In 39th International Colloquium on
Automata, Languages, and Programming -, ICALP â€™12, pages 810â€“821,
Warwick, UK, 2012. Springer.
+
[WZL 19] Royce J Wilson, Celia Yuxin Zhang, William Lam, Damien Desfontaines,
Daniel Simmons-Marengo, and Bryant Gipson. Differentially private sql
with bounded user contribution. arXiv preprint arXiv:1909.01917, 2019.

438

